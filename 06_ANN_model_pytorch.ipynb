{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden1_size)\n",
    "        # self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.batch1 = nn.BatchNorm1d(hidden1_size)\n",
    "\n",
    "        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        # self.dropout2 = nn.Dropout(p=0.001)\n",
    "        self.batch2 = nn.BatchNorm1d(hidden2_size)\n",
    "\n",
    "        # self.hidden3 = nn.Linear(hidden2_size, hidden3_size)\n",
    "        # self.batch3 = nn.BatchNorm1d(hidden3_size)\n",
    "\n",
    "        self.predict = nn.Linear(hidden2_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        result = self.hidden1(input)\n",
    "        # result = self.dropout1(result)\n",
    "        result = self.batch1(result)\n",
    "\n",
    "        result = F.leaky_relu(result)\n",
    "\n",
    "        result = self.hidden2(result)\n",
    "        # result = self.dropout2(result)\n",
    "        result = self.batch2(result)\n",
    "        result = F.leaky_relu(result)\n",
    "\n",
    "        # result = self.hidden3(result)\n",
    "        # result = self.batch3(result)\n",
    "        # result = F.leaky_relu(result)\n",
    "\n",
    "        result = self.predict(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_data, y_data, optimizer, loss, epochs, device):\n",
    "\n",
    "    final_R2_train = []\n",
    "    final_R2_test = []\n",
    "    final_rmse_train = []\n",
    "    final_rmse_test = []\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 64)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # for train_index, val_index in kf.split(x_train):\n",
    "\n",
    "    #     x_train_1, x_test_1 = x_train[train_index], x_train[val_index]\n",
    "    #     y_train_1, y_test_1 = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).cuda()\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).cuda()\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).cuda()\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).cuda()\n",
    "\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        mse_loss = []\n",
    "        # define batch: batch_sizee = 100\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()   # initialize gradient, gradient computed from batch 1 does not interfere batch2 and model update\n",
    "\n",
    "            output = model(batch_x)\n",
    "            l = loss(output, batch_y)\n",
    "\n",
    "            mse_loss.append(l.item())   # l.item() 返回tensor中的每一个值，节约内存\n",
    "\n",
    "            l.backward()     # back propagation\n",
    "            optimizer.step()    # update model\n",
    "        \n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                train_pred = model(batch_x.to(device)).cpu().numpy().flatten()\n",
    "                r2_train = r2_score(batch_y, train_pred)\n",
    "                rmse_train = np.sqrt(mean_squared_error(batch_y, train_pred))\n",
    "\n",
    "                final_R2_train.append(r2_train)\n",
    "                final_rmse_train.append(rmse_train)\n",
    "\n",
    "                # final_R2_test.append()\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {np.mean(mse_loss)}, R2 Score (Train): {r2_train}, RMSE (Train): {rmse_train}\")\n",
    "\n",
    "                test_loss = []\n",
    "                test_preds_list = []\n",
    "                test_target_list = []\n",
    "                for batch_x, batch_y in test_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    test_output = model(batch_x)\n",
    "                    l = loss(test_output, batch_y)\n",
    "                    test_loss.append(l.item())\n",
    "\n",
    "                    test_preds_list.append(test_output.cpu().numpy())\n",
    "                    test_target_list.append(batch_y.cpu().numpy())\n",
    "                    \n",
    "                test_preds = np.concatenate(test_preds_list).flatten()\n",
    "                test_target = np.concatenate(test_target_list).flatten()\n",
    "\n",
    "                r2_test = r2_score(test_target, test_preds)\n",
    "                rmse_test = np.sqrt(mean_squared_error(test_target, test_preds))\n",
    "\n",
    "                final_R2_test.append(r2_test)\n",
    "                final_rmse_test.append(rmse_test)\n",
    "\n",
    "                print(f\"Test Loss: {np.mean(test_loss)}, R2 Score (Test): {r2_test}, RMSE (Test): {rmse_test}\")\n",
    "\n",
    "    return final_R2_test, final_R2_train, final_rmse_test, final_rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = pd.read_csv('./Data/x_value_train.csv')\n",
    "x_data = x_data.values\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.read_csv('./Data/y_value_train.csv')\n",
    "y_data = y_data.values\n",
    "# y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000], Train Loss: 43.26290321350098, R2 Score (Train): -14.90602905694612, RMSE (Train): 6.758498191833496\n",
      "Test Loss: 43.19074630737305, R2 Score (Test): -16.713536486666026, RMSE (Test): 6.601667881011963\n",
      "Epoch [6/5000], Train Loss: 23.85483964284261, R2 Score (Train): -9.142537683401185, RMSE (Train): 5.4625244140625\n",
      "Test Loss: 30.371487617492676, R2 Score (Test): -11.503276300154239, RMSE (Test): 5.546422958374023\n",
      "Epoch [11/5000], Train Loss: 14.568433125813803, R2 Score (Train): -4.192687726750593, RMSE (Train): 3.7768473625183105\n",
      "Test Loss: 16.75106716156006, R2 Score (Test): -5.915871295533533, RMSE (Test): 4.12500524520874\n",
      "Epoch [16/5000], Train Loss: 8.73831049601237, R2 Score (Train): -2.04205989895645, RMSE (Train): 2.7948639392852783\n",
      "Test Loss: 11.321361064910889, R2 Score (Test): -3.6718778505304552, RMSE (Test): 3.3903677463531494\n",
      "Epoch [21/5000], Train Loss: 5.081542571385701, R2 Score (Train): -0.7886965657529086, RMSE (Train): 2.2135190963745117\n",
      "Test Loss: 7.811140298843384, R2 Score (Test): -2.235567955682048, RMSE (Test): 2.8214757442474365\n",
      "Epoch [26/5000], Train Loss: 2.9877294301986694, R2 Score (Train): -0.26788456294008456, RMSE (Train): 1.6193737983703613\n",
      "Test Loss: 5.404184579849243, R2 Score (Test): -1.2541653402127904, RMSE (Test): 2.3550164699554443\n",
      "Epoch [31/5000], Train Loss: 1.505885382493337, R2 Score (Train): 0.2908205518653937, RMSE (Train): 1.266700029373169\n",
      "Test Loss: 4.210371017456055, R2 Score (Test): -0.7579333384953453, RMSE (Test): 2.079707622528076\n",
      "Epoch [36/5000], Train Loss: 0.6805165509382883, R2 Score (Train): 0.7737953352601121, RMSE (Train): 0.8632475137710571\n",
      "Test Loss: 3.1201714277267456, R2 Score (Test): -0.30863370920946065, RMSE (Test): 1.7943620681762695\n",
      "Epoch [41/5000], Train Loss: 0.3325117826461792, R2 Score (Train): 0.8621609370796504, RMSE (Train): 0.6017563939094543\n",
      "Test Loss: 2.4453903436660767, R2 Score (Test): -0.031679955779978064, RMSE (Test): 1.5932118892669678\n",
      "Epoch [46/5000], Train Loss: 0.182953879237175, R2 Score (Train): 0.936698380949671, RMSE (Train): 0.3814622163772583\n",
      "Test Loss: 2.0656909942626953, R2 Score (Test): 0.1346576402679842, RMSE (Test): 1.459133505821228\n",
      "Epoch [51/5000], Train Loss: 0.18859333048264185, R2 Score (Train): 0.9575303849211046, RMSE (Train): 0.299710214138031\n",
      "Test Loss: 1.832228422164917, R2 Score (Test): 0.227457931680922, RMSE (Test): 1.3786755800247192\n",
      "Epoch [56/5000], Train Loss: 0.14352633183201155, R2 Score (Train): 0.9765160777024322, RMSE (Train): 0.2363600879907608\n",
      "Test Loss: 1.714698314666748, R2 Score (Test): 0.28265783132092714, RMSE (Test): 1.3285081386566162\n",
      "Epoch [61/5000], Train Loss: 0.08848532103002071, R2 Score (Train): 0.9883190332767144, RMSE (Train): 0.18618565797805786\n",
      "Test Loss: 1.431571364402771, R2 Score (Test): 0.39633992441476795, RMSE (Test): 1.2187013626098633\n",
      "Epoch [66/5000], Train Loss: 0.05722113698720932, R2 Score (Train): 0.9865980935696624, RMSE (Train): 0.18324117362499237\n",
      "Test Loss: 1.5236589908599854, R2 Score (Test): 0.3573715078522588, RMSE (Test): 1.2574219703674316\n",
      "Epoch [71/5000], Train Loss: 0.11275334966679414, R2 Score (Train): 0.9922011803207017, RMSE (Train): 0.15486718714237213\n",
      "Test Loss: 1.4855875372886658, R2 Score (Test): 0.3646218139851285, RMSE (Test): 1.2503085136413574\n",
      "Epoch [76/5000], Train Loss: 0.08392786731322606, R2 Score (Train): 0.9938876665127453, RMSE (Train): 0.12705878913402557\n",
      "Test Loss: 1.470798671245575, R2 Score (Test): 0.382679388143753, RMSE (Test): 1.2324135303497314\n",
      "Epoch [81/5000], Train Loss: 0.0755313312013944, R2 Score (Train): 0.9959734152918134, RMSE (Train): 0.11565720289945602\n",
      "Test Loss: 1.3717541694641113, R2 Score (Test): 0.41605063263315734, RMSE (Test): 1.1986396312713623\n",
      "Epoch [86/5000], Train Loss: 0.0519843657190601, R2 Score (Train): 0.9929850513843539, RMSE (Train): 0.1424909234046936\n",
      "Test Loss: 1.343643605709076, R2 Score (Test): 0.4319618255297557, RMSE (Test): 1.182196855545044\n",
      "Epoch [91/5000], Train Loss: 0.07470203097909689, R2 Score (Train): 0.9903097039831098, RMSE (Train): 0.16076266765594482\n",
      "Test Loss: 1.4119012951850891, R2 Score (Test): 0.39909544782334694, RMSE (Test): 1.215916633605957\n",
      "Epoch [96/5000], Train Loss: 0.0435612207899491, R2 Score (Train): 0.9961727662554211, RMSE (Train): 0.10764916241168976\n",
      "Test Loss: 1.3124711513519287, R2 Score (Test): 0.43781554133633493, RMSE (Test): 1.1760896444320679\n",
      "Epoch [101/5000], Train Loss: 0.05847160331904888, R2 Score (Train): 0.9932761930093568, RMSE (Train): 0.1426875740289688\n",
      "Test Loss: 1.3319495916366577, R2 Score (Test): 0.4322134426447204, RMSE (Test): 1.1819350719451904\n",
      "Epoch [106/5000], Train Loss: 0.054200672854979835, R2 Score (Train): 0.9902046077308316, RMSE (Train): 0.16502699255943298\n",
      "Test Loss: 1.1947523355484009, R2 Score (Test): 0.48351855743725836, RMSE (Test): 1.1272711753845215\n",
      "Epoch [111/5000], Train Loss: 0.04147731543829044, R2 Score (Train): 0.9935635558440055, RMSE (Train): 0.13989582657814026\n",
      "Test Loss: 1.3777085542678833, R2 Score (Test): 0.4147437574163104, RMSE (Test): 1.1999802589416504\n",
      "Epoch [116/5000], Train Loss: 0.05572448546687762, R2 Score (Train): 0.9937661856566256, RMSE (Train): 0.13130898773670197\n",
      "Test Loss: 1.2956584692001343, R2 Score (Test): 0.45081930277934157, RMSE (Test): 1.1624082326889038\n",
      "Epoch [121/5000], Train Loss: 0.04686618192742268, R2 Score (Train): 0.9932346303971294, RMSE (Train): 0.13413897156715393\n",
      "Test Loss: 1.3008032441139221, R2 Score (Test): 0.4497192936907364, RMSE (Test): 1.163571834564209\n",
      "Epoch [126/5000], Train Loss: 0.03338986014326414, R2 Score (Train): 0.9948387881613429, RMSE (Train): 0.11011143773794174\n",
      "Test Loss: 1.2903398871421814, R2 Score (Test): 0.44613185730859384, RMSE (Test): 1.1673583984375\n",
      "Epoch [131/5000], Train Loss: 0.07691284455358982, R2 Score (Train): 0.9927474019340773, RMSE (Train): 0.11965640634298325\n",
      "Test Loss: 1.2892329096794128, R2 Score (Test): 0.4461439900085038, RMSE (Test): 1.167345643043518\n",
      "Epoch [136/5000], Train Loss: 0.06818163519104321, R2 Score (Train): 0.9877170180573341, RMSE (Train): 0.1916334629058838\n",
      "Test Loss: 1.2352468967437744, R2 Score (Test): 0.4710217512752233, RMSE (Test): 1.1408274173736572\n",
      "Epoch [141/5000], Train Loss: 0.03391594315568606, R2 Score (Train): 0.99524232948188, RMSE (Train): 0.11171291023492813\n",
      "Test Loss: 1.216056227684021, R2 Score (Test): 0.4790687387665732, RMSE (Test): 1.1321167945861816\n",
      "Epoch [146/5000], Train Loss: 0.07915829370419185, R2 Score (Train): 0.9952539506821658, RMSE (Train): 0.11170525848865509\n",
      "Test Loss: 1.2346290946006775, R2 Score (Test): 0.475008788251598, RMSE (Test): 1.1365199089050293\n",
      "Epoch [151/5000], Train Loss: 0.03286330526073774, R2 Score (Train): 0.9947001444076475, RMSE (Train): 0.11694910377264023\n",
      "Test Loss: 1.1945581436157227, R2 Score (Test): 0.487665593027635, RMSE (Test): 1.1227364540100098\n",
      "Epoch [156/5000], Train Loss: 0.10510855757941802, R2 Score (Train): 0.9939111292032188, RMSE (Train): 0.14121584594249725\n",
      "Test Loss: 1.157214194536209, R2 Score (Test): 0.5054707113513013, RMSE (Test): 1.1030546426773071\n",
      "Epoch [161/5000], Train Loss: 0.06048381173362335, R2 Score (Train): 0.9925098243118213, RMSE (Train): 0.13187943398952484\n",
      "Test Loss: 1.1793526411056519, R2 Score (Test): 0.4944127213578111, RMSE (Test): 1.115319013595581\n",
      "Epoch [166/5000], Train Loss: 0.04694901561985413, R2 Score (Train): 0.9946175383433231, RMSE (Train): 0.10981518775224686\n",
      "Test Loss: 1.1521641612052917, R2 Score (Test): 0.5086240155193591, RMSE (Test): 1.0995323657989502\n",
      "Epoch [171/5000], Train Loss: 0.03261977216849724, R2 Score (Train): 0.9967960764022779, RMSE (Train): 0.10121393948793411\n",
      "Test Loss: 1.1234430372714996, R2 Score (Test): 0.5210762710825461, RMSE (Test): 1.0855109691619873\n",
      "Epoch [176/5000], Train Loss: 0.05100981270273527, R2 Score (Train): 0.9961806917140369, RMSE (Train): 0.11626040935516357\n",
      "Test Loss: 1.1484822928905487, R2 Score (Test): 0.5085086628536926, RMSE (Test): 1.0996613502502441\n",
      "Epoch [181/5000], Train Loss: 0.03657232535382112, R2 Score (Train): 0.9939030307391289, RMSE (Train): 0.1410807967185974\n",
      "Test Loss: 1.155507743358612, R2 Score (Test): 0.5089745323795136, RMSE (Test): 1.0991401672363281\n",
      "Epoch [186/5000], Train Loss: 0.043394447614749275, R2 Score (Train): 0.9928000357372092, RMSE (Train): 0.14280998706817627\n",
      "Test Loss: 1.1084064543247223, R2 Score (Test): 0.5294630911565426, RMSE (Test): 1.075964331626892\n",
      "Epoch [191/5000], Train Loss: 0.03586656600236893, R2 Score (Train): 0.9953344536893394, RMSE (Train): 0.11076512932777405\n",
      "Test Loss: 1.1261164247989655, R2 Score (Test): 0.5215435829825337, RMSE (Test): 1.0849812030792236\n",
      "Epoch [196/5000], Train Loss: 0.047047133247057595, R2 Score (Train): 0.9962054865246762, RMSE (Train): 0.10153236985206604\n",
      "Test Loss: 1.17683345079422, R2 Score (Test): 0.5032215834261724, RMSE (Test): 1.105560302734375\n",
      "Epoch [201/5000], Train Loss: 0.04874850995838642, R2 Score (Train): 0.9945151196857858, RMSE (Train): 0.1131918728351593\n",
      "Test Loss: 1.0943168699741364, R2 Score (Test): 0.5321159979525169, RMSE (Test): 1.0729268789291382\n",
      "Epoch [206/5000], Train Loss: 0.034909943118691444, R2 Score (Train): 0.9954016537719249, RMSE (Train): 0.11990486085414886\n",
      "Test Loss: 1.0656134486198425, R2 Score (Test): 0.5451016419731884, RMSE (Test): 1.0579332113265991\n",
      "Epoch [211/5000], Train Loss: 0.028479292057454586, R2 Score (Train): 0.9958969699649255, RMSE (Train): 0.10255374014377594\n",
      "Test Loss: 1.092630922794342, R2 Score (Test): 0.5315531955966561, RMSE (Test): 1.073572039604187\n",
      "Epoch [216/5000], Train Loss: 0.037752723321318626, R2 Score (Train): 0.9973092143531858, RMSE (Train): 0.08182840794324875\n",
      "Test Loss: 1.095703512430191, R2 Score (Test): 0.5355183018603686, RMSE (Test): 1.069018840789795\n",
      "Epoch [221/5000], Train Loss: 0.05562578203777472, R2 Score (Train): 0.9947766790732901, RMSE (Train): 0.11011995375156403\n",
      "Test Loss: 1.0452249646186829, R2 Score (Test): 0.5613795558024244, RMSE (Test): 1.038832426071167\n",
      "Epoch [226/5000], Train Loss: 0.04316114572187265, R2 Score (Train): 0.9951240739501048, RMSE (Train): 0.11442368477582932\n",
      "Test Loss: 1.084377646446228, R2 Score (Test): 0.5383947838910121, RMSE (Test): 1.0657035112380981\n",
      "Epoch [231/5000], Train Loss: 0.03157924891759952, R2 Score (Train): 0.9924883274356576, RMSE (Train): 0.1358841061592102\n",
      "Test Loss: 1.097861498594284, R2 Score (Test): 0.5334161864310418, RMSE (Test): 1.0714352130889893\n",
      "Epoch [236/5000], Train Loss: 0.03834673979630073, R2 Score (Train): 0.9939930024439226, RMSE (Train): 0.13516423106193542\n",
      "Test Loss: 1.081429123878479, R2 Score (Test): 0.5438564588434435, RMSE (Test): 1.059380054473877\n",
      "Epoch [241/5000], Train Loss: 0.033004615145425, R2 Score (Train): 0.9907931964291857, RMSE (Train): 0.1549811065196991\n",
      "Test Loss: 1.0317618548870087, R2 Score (Test): 0.5637806444517683, RMSE (Test): 1.0359851121902466\n",
      "Epoch [246/5000], Train Loss: 0.026851110781232517, R2 Score (Train): 0.9943893756387555, RMSE (Train): 0.12261634320020676\n",
      "Test Loss: 1.0570872128009796, R2 Score (Test): 0.5514123103951694, RMSE (Test): 1.0505692958831787\n",
      "Epoch [251/5000], Train Loss: 0.0226753040527304, R2 Score (Train): 0.9943462897026333, RMSE (Train): 0.10493331402540207\n",
      "Test Loss: 1.0225583910942078, R2 Score (Test): 0.5651270918538718, RMSE (Test): 1.0343849658966064\n",
      "Epoch [256/5000], Train Loss: 0.0354353291913867, R2 Score (Train): 0.996139724193902, RMSE (Train): 0.09958908706903458\n",
      "Test Loss: 1.0771377980709076, R2 Score (Test): 0.5449232682152383, RMSE (Test): 1.058140516281128\n",
      "Epoch [261/5000], Train Loss: 0.04073923919349909, R2 Score (Train): 0.9938703922792543, RMSE (Train): 0.11934894323348999\n",
      "Test Loss: 1.075184404850006, R2 Score (Test): 0.5466044773879284, RMSE (Test): 1.05618417263031\n",
      "Epoch [266/5000], Train Loss: 0.05243715768059095, R2 Score (Train): 0.9887927131385039, RMSE (Train): 0.17191235721111298\n",
      "Test Loss: 1.047247737646103, R2 Score (Test): 0.5589582972350176, RMSE (Test): 1.0416957139968872\n",
      "Epoch [271/5000], Train Loss: 0.02648619158814351, R2 Score (Train): 0.9964484092881217, RMSE (Train): 0.10896149277687073\n",
      "Test Loss: 1.0580913722515106, R2 Score (Test): 0.5535006838914984, RMSE (Test): 1.0481210947036743\n",
      "Epoch [276/5000], Train Loss: 0.031442904844880104, R2 Score (Train): 0.9957371311883186, RMSE (Train): 0.10739648342132568\n",
      "Test Loss: 1.0042370855808258, R2 Score (Test): 0.5731641213220757, RMSE (Test): 1.0247820615768433\n",
      "Epoch [281/5000], Train Loss: 0.035342008961985506, R2 Score (Train): 0.9962441843034656, RMSE (Train): 0.08872432261705399\n",
      "Test Loss: 1.0590968430042267, R2 Score (Test): 0.5527984252495577, RMSE (Test): 1.0489450693130493\n",
      "Epoch [286/5000], Train Loss: 0.02246486923346917, R2 Score (Train): 0.9953598683890991, RMSE (Train): 0.11600086838006973\n",
      "Test Loss: 1.0388101935386658, R2 Score (Test): 0.5580056824221284, RMSE (Test): 1.0428200960159302\n",
      "Epoch [291/5000], Train Loss: 0.0591035538042585, R2 Score (Train): 0.9945999044886267, RMSE (Train): 0.12234427034854889\n",
      "Test Loss: 1.1438973546028137, R2 Score (Test): 0.5230326717788893, RMSE (Test): 1.0832915306091309\n",
      "Epoch [296/5000], Train Loss: 0.04386200103908777, R2 Score (Train): 0.995586844133544, RMSE (Train): 0.13065199553966522\n",
      "Test Loss: 0.9726365208625793, R2 Score (Test): 0.5858557514560305, RMSE (Test): 1.0094314813613892\n",
      "Epoch [301/5000], Train Loss: 0.049015806366999946, R2 Score (Train): 0.994776143155775, RMSE (Train): 0.1111907958984375\n",
      "Test Loss: 1.0301639437675476, R2 Score (Test): 0.559209532129831, RMSE (Test): 1.0413990020751953\n",
      "Epoch [306/5000], Train Loss: 0.05370879825204611, R2 Score (Train): 0.9970948331529319, RMSE (Train): 0.09684424847364426\n",
      "Test Loss: 1.0178595185279846, R2 Score (Test): 0.5690020115954284, RMSE (Test): 1.029766321182251\n",
      "Epoch [311/5000], Train Loss: 0.020322442365189392, R2 Score (Train): 0.9956461951478113, RMSE (Train): 0.11024970561265945\n",
      "Test Loss: 0.9785700142383575, R2 Score (Test): 0.5856119534640307, RMSE (Test): 1.0097285509109497\n",
      "Epoch [316/5000], Train Loss: 0.02924525613586108, R2 Score (Train): 0.9950100734107032, RMSE (Train): 0.1142948642373085\n",
      "Test Loss: 0.9926711916923523, R2 Score (Test): 0.5782138087023267, RMSE (Test): 1.0187021493911743\n",
      "Epoch [321/5000], Train Loss: 0.03366628692795833, R2 Score (Train): 0.9949815068935438, RMSE (Train): 0.1156429797410965\n",
      "Test Loss: 1.0202783048152924, R2 Score (Test): 0.5662747874222712, RMSE (Test): 1.0330191850662231\n",
      "Epoch [326/5000], Train Loss: 0.03361517873903116, R2 Score (Train): 0.9953782314984386, RMSE (Train): 0.11351649463176727\n",
      "Test Loss: 1.031464844942093, R2 Score (Test): 0.560230865635439, RMSE (Test): 1.0401917695999146\n",
      "Epoch [331/5000], Train Loss: 0.017459809314459562, R2 Score (Train): 0.996896701758633, RMSE (Train): 0.09309500455856323\n",
      "Test Loss: 1.0177618563175201, R2 Score (Test): 0.5693796983454997, RMSE (Test): 1.0293149948120117\n",
      "Epoch [336/5000], Train Loss: 0.026796882040798664, R2 Score (Train): 0.9944604056879291, RMSE (Train): 0.10192306339740753\n",
      "Test Loss: 0.9794190526008606, R2 Score (Test): 0.5814167065455251, RMSE (Test): 1.0148268938064575\n",
      "Epoch [341/5000], Train Loss: 0.0323756675546368, R2 Score (Train): 0.9941840796217862, RMSE (Train): 0.11451350897550583\n",
      "Test Loss: 1.0224460661411285, R2 Score (Test): 0.5691194636412571, RMSE (Test): 1.0296260118484497\n",
      "Epoch [346/5000], Train Loss: 0.04634815144042174, R2 Score (Train): 0.9928187221110762, RMSE (Train): 0.1224202811717987\n",
      "Test Loss: 1.0539624094963074, R2 Score (Test): 0.5548981406283485, RMSE (Test): 1.0464794635772705\n",
      "Epoch [351/5000], Train Loss: 0.05761270597577095, R2 Score (Train): 0.9924986862878583, RMSE (Train): 0.13216623663902283\n",
      "Test Loss: 0.9895421862602234, R2 Score (Test): 0.5770290398953003, RMSE (Test): 1.0201318264007568\n",
      "Epoch [356/5000], Train Loss: 0.028200070994595688, R2 Score (Train): 0.9968947723624586, RMSE (Train): 0.09777882695198059\n",
      "Test Loss: 0.9999180734157562, R2 Score (Test): 0.57365321696125, RMSE (Test): 1.0241948366165161\n",
      "Epoch [361/5000], Train Loss: 0.032185196255644165, R2 Score (Train): 0.9959098511638655, RMSE (Train): 0.10216809064149857\n",
      "Test Loss: 0.996976226568222, R2 Score (Test): 0.5765709803271983, RMSE (Test): 1.0206841230392456\n",
      "Epoch [366/5000], Train Loss: 0.033632805260519184, R2 Score (Train): 0.9932478284363289, RMSE (Train): 0.13169889152050018\n",
      "Test Loss: 0.9455764889717102, R2 Score (Test): 0.5933032131614393, RMSE (Test): 1.0003142356872559\n",
      "Epoch [371/5000], Train Loss: 0.04190474065641562, R2 Score (Train): 0.9953271773648893, RMSE (Train): 0.12417086213827133\n",
      "Test Loss: 1.0007428228855133, R2 Score (Test): 0.5739586523882292, RMSE (Test): 1.0238277912139893\n",
      "Epoch [376/5000], Train Loss: 0.028592918689052265, R2 Score (Train): 0.994876286724826, RMSE (Train): 0.12070880085229874\n",
      "Test Loss: 0.9556317925453186, R2 Score (Test): 0.5897424328502463, RMSE (Test): 1.0046837329864502\n",
      "Epoch [381/5000], Train Loss: 0.023735240412255127, R2 Score (Train): 0.9967599170134488, RMSE (Train): 0.09403803199529648\n",
      "Test Loss: 0.9396295547485352, R2 Score (Test): 0.5934662943798725, RMSE (Test): 1.000113606452942\n",
      "Epoch [386/5000], Train Loss: 0.01549827866256237, R2 Score (Train): 0.9973092960086386, RMSE (Train): 0.09181652218103409\n",
      "Test Loss: 0.9387295544147491, R2 Score (Test): 0.5972375489842054, RMSE (Test): 0.9954639077186584\n",
      "Epoch [391/5000], Train Loss: 0.017652994797875483, R2 Score (Train): 0.9972381171786394, RMSE (Train): 0.08863574266433716\n",
      "Test Loss: 0.9481684863567352, R2 Score (Test): 0.5941785294423922, RMSE (Test): 0.9992371201515198\n",
      "Epoch [396/5000], Train Loss: 0.027027301335086424, R2 Score (Train): 0.994316868798195, RMSE (Train): 0.0998050719499588\n",
      "Test Loss: 0.978798896074295, R2 Score (Test): 0.5800801998885257, RMSE (Test): 1.0164457559585571\n",
      "Epoch [401/5000], Train Loss: 0.025790673835823934, R2 Score (Train): 0.9958620586081331, RMSE (Train): 0.104465551674366\n",
      "Test Loss: 0.9354702234268188, R2 Score (Test): 0.6014947440771719, RMSE (Test): 0.9901889562606812\n",
      "Epoch [406/5000], Train Loss: 0.04442931075269977, R2 Score (Train): 0.9957259928972475, RMSE (Train): 0.10610656440258026\n",
      "Test Loss: 0.9266716539859772, R2 Score (Test): 0.6018069300542064, RMSE (Test): 0.9898009896278381\n",
      "Epoch [411/5000], Train Loss: 0.01987865660339594, R2 Score (Train): 0.9962612597051372, RMSE (Train): 0.10085026174783707\n",
      "Test Loss: 0.9208934307098389, R2 Score (Test): 0.6039504076273963, RMSE (Test): 0.9871333837509155\n",
      "Epoch [416/5000], Train Loss: 0.02369615000983079, R2 Score (Train): 0.9948354355473337, RMSE (Train): 0.10828952491283417\n",
      "Test Loss: 0.9630200862884521, R2 Score (Test): 0.589753157867936, RMSE (Test): 1.0046706199645996\n",
      "Epoch [421/5000], Train Loss: 0.03416702368607124, R2 Score (Train): 0.9971872686524499, RMSE (Train): 0.10793212801218033\n",
      "Test Loss: 0.9682266116142273, R2 Score (Test): 0.5829319444452021, RMSE (Test): 1.0129884481430054\n",
      "Epoch [426/5000], Train Loss: 0.033401923875013985, R2 Score (Train): 0.9947985837007696, RMSE (Train): 0.12843537330627441\n",
      "Test Loss: 0.913565069437027, R2 Score (Test): 0.6068340778654269, RMSE (Test): 0.9835330843925476\n",
      "Epoch [431/5000], Train Loss: 0.025445507218440373, R2 Score (Train): 0.9951923679077185, RMSE (Train): 0.11109704524278641\n",
      "Test Loss: 0.8973699510097504, R2 Score (Test): 0.6132462842969184, RMSE (Test): 0.975479781627655\n",
      "Epoch [436/5000], Train Loss: 0.038819730592270695, R2 Score (Train): 0.9959079290266196, RMSE (Train): 0.10414572060108185\n",
      "Test Loss: 0.8795439302921295, R2 Score (Test): 0.6199835789956842, RMSE (Test): 0.966946005821228\n",
      "Epoch [441/5000], Train Loss: 0.027543658080200355, R2 Score (Train): 0.9951324941279606, RMSE (Train): 0.11192334443330765\n",
      "Test Loss: 0.9272554516792297, R2 Score (Test): 0.6008099195106641, RMSE (Test): 0.9910393953323364\n",
      "Epoch [446/5000], Train Loss: 0.016532723947117727, R2 Score (Train): 0.995829157894782, RMSE (Train): 0.1077570989727974\n",
      "Test Loss: 0.8832474648952484, R2 Score (Test): 0.6185255106565555, RMSE (Test): 0.9687991738319397\n",
      "Epoch [451/5000], Train Loss: 0.019784869936605293, R2 Score (Train): 0.9965007566116397, RMSE (Train): 0.09408702701330185\n",
      "Test Loss: 0.9038695693016052, R2 Score (Test): 0.6087731388513896, RMSE (Test): 0.9811047911643982\n",
      "Epoch [456/5000], Train Loss: 0.01836759379754464, R2 Score (Train): 0.9971674179736513, RMSE (Train): 0.10123201459646225\n",
      "Test Loss: 0.9266618192195892, R2 Score (Test): 0.597412666802079, RMSE (Test): 0.9952475428581238\n",
      "Epoch [461/5000], Train Loss: 0.03178420243784785, R2 Score (Train): 0.9940343232890076, RMSE (Train): 0.1191583201289177\n",
      "Test Loss: 0.9124828279018402, R2 Score (Test): 0.6062099002249337, RMSE (Test): 0.984313428401947\n",
      "Epoch [466/5000], Train Loss: 0.021389313352604706, R2 Score (Train): 0.9949040838812985, RMSE (Train): 0.13241620361804962\n",
      "Test Loss: 0.8932840824127197, R2 Score (Test): 0.6173696425900023, RMSE (Test): 0.9702659249305725\n",
      "Epoch [471/5000], Train Loss: 0.038820475805550814, R2 Score (Train): 0.9947101908144893, RMSE (Train): 0.13383029401302338\n",
      "Test Loss: 0.8831004798412323, R2 Score (Test): 0.6185348876720944, RMSE (Test): 0.9687873125076294\n",
      "Epoch [476/5000], Train Loss: 0.023773560921351116, R2 Score (Train): 0.9955920153620483, RMSE (Train): 0.09299978613853455\n",
      "Test Loss: 0.903414249420166, R2 Score (Test): 0.6102477841963294, RMSE (Test): 0.9792540073394775\n",
      "Epoch [481/5000], Train Loss: 0.021005199290812016, R2 Score (Train): 0.9940201172916149, RMSE (Train): 0.12025374174118042\n",
      "Test Loss: 0.9078847765922546, R2 Score (Test): 0.609742467054411, RMSE (Test): 0.9798885583877563\n",
      "Epoch [486/5000], Train Loss: 0.018676046282052994, R2 Score (Train): 0.9963315504441772, RMSE (Train): 0.09283945709466934\n",
      "Test Loss: 0.886170506477356, R2 Score (Test): 0.6165779360133143, RMSE (Test): 0.9712691903114319\n",
      "Epoch [491/5000], Train Loss: 0.03480039474864801, R2 Score (Train): 0.9958439011951878, RMSE (Train): 0.1084698811173439\n",
      "Test Loss: 0.8456821143627167, R2 Score (Test): 0.6402648009793172, RMSE (Test): 0.9407896399497986\n",
      "Epoch [496/5000], Train Loss: 0.029533132910728455, R2 Score (Train): 0.9963762169307869, RMSE (Train): 0.10798826813697815\n",
      "Test Loss: 0.857904314994812, R2 Score (Test): 0.6308380613895795, RMSE (Test): 0.9530364274978638\n",
      "Epoch [501/5000], Train Loss: 0.025793576302627724, R2 Score (Train): 0.9964227068566391, RMSE (Train): 0.09983428567647934\n",
      "Test Loss: 0.9068928956985474, R2 Score (Test): 0.6096955707630045, RMSE (Test): 0.9799473881721497\n",
      "Epoch [506/5000], Train Loss: 0.022019538562744856, R2 Score (Train): 0.9955891508627532, RMSE (Train): 0.11435756832361221\n",
      "Test Loss: 0.8568597435951233, R2 Score (Test): 0.6315257518767795, RMSE (Test): 0.9521483182907104\n",
      "Epoch [511/5000], Train Loss: 0.023880545205126207, R2 Score (Train): 0.9962280407797519, RMSE (Train): 0.09663774818181992\n",
      "Test Loss: 0.8878547251224518, R2 Score (Test): 0.6215070895454311, RMSE (Test): 0.9650058150291443\n",
      "Epoch [516/5000], Train Loss: 0.019701058665911358, R2 Score (Train): 0.9958223438604288, RMSE (Train): 0.11065591126680374\n",
      "Test Loss: 0.8715206384658813, R2 Score (Test): 0.6253747331013075, RMSE (Test): 0.9600626826286316\n",
      "Epoch [521/5000], Train Loss: 0.020037900035579998, R2 Score (Train): 0.9953137433750068, RMSE (Train): 0.10344640165567398\n",
      "Test Loss: 0.8690064251422882, R2 Score (Test): 0.6247027416276121, RMSE (Test): 0.9609233140945435\n",
      "Epoch [526/5000], Train Loss: 0.02653727090607087, R2 Score (Train): 0.9971852374147229, RMSE (Train): 0.09772709012031555\n",
      "Test Loss: 0.8498086035251617, R2 Score (Test): 0.636867311783824, RMSE (Test): 0.9452218413352966\n",
      "Epoch [531/5000], Train Loss: 0.024831966341783602, R2 Score (Train): 0.9940990363909455, RMSE (Train): 0.13837146759033203\n",
      "Test Loss: 0.8291572034358978, R2 Score (Test): 0.639182031746671, RMSE (Test): 0.9422044157981873\n",
      "Epoch [536/5000], Train Loss: 0.01974822860211134, R2 Score (Train): 0.9956675765326718, RMSE (Train): 0.10469284653663635\n",
      "Test Loss: 0.9033462703227997, R2 Score (Test): 0.6127593956920916, RMSE (Test): 0.9760936498641968\n",
      "Epoch [541/5000], Train Loss: 0.03311282179007927, R2 Score (Train): 0.9911165282712974, RMSE (Train): 0.15511877834796906\n",
      "Test Loss: 0.8803547918796539, R2 Score (Test): 0.6215275469904424, RMSE (Test): 0.9649797081947327\n",
      "Epoch [546/5000], Train Loss: 0.021462627376119297, R2 Score (Train): 0.997583378219137, RMSE (Train): 0.08989052474498749\n",
      "Test Loss: 0.9106019139289856, R2 Score (Test): 0.6129940160902982, RMSE (Test): 0.9757978916168213\n",
      "Epoch [551/5000], Train Loss: 0.01981864341845115, R2 Score (Train): 0.9935396776267845, RMSE (Train): 0.12393291294574738\n",
      "Test Loss: 0.8778621256351471, R2 Score (Test): 0.6233308930209545, RMSE (Test): 0.9626779556274414\n",
      "Epoch [556/5000], Train Loss: 0.019231450588752825, R2 Score (Train): 0.9965360551711107, RMSE (Train): 0.10908111929893494\n",
      "Test Loss: 0.9007600545883179, R2 Score (Test): 0.6151830418716987, RMSE (Test): 0.9730343222618103\n",
      "Epoch [561/5000], Train Loss: 0.01616639519731204, R2 Score (Train): 0.9947964628400745, RMSE (Train): 0.10913195461034775\n",
      "Test Loss: 0.8477106392383575, R2 Score (Test): 0.6354468983635022, RMSE (Test): 0.9470686316490173\n",
      "Epoch [566/5000], Train Loss: 0.022195925160000723, R2 Score (Train): 0.9967540315156279, RMSE (Train): 0.09814246743917465\n",
      "Test Loss: 0.8883283734321594, R2 Score (Test): 0.6168871239473228, RMSE (Test): 0.9708774089813232\n",
      "Epoch [571/5000], Train Loss: 0.026010423433035612, R2 Score (Train): 0.9909844590860231, RMSE (Train): 0.1508488804101944\n",
      "Test Loss: 0.8509997725486755, R2 Score (Test): 0.6352230079589167, RMSE (Test): 0.9473593235015869\n",
      "Epoch [576/5000], Train Loss: 0.02129314374178648, R2 Score (Train): 0.9976678207490266, RMSE (Train): 0.09457932412624359\n",
      "Test Loss: 0.864482045173645, R2 Score (Test): 0.6313807945161228, RMSE (Test): 0.9523355960845947\n",
      "Epoch [581/5000], Train Loss: 0.02039133400345842, R2 Score (Train): 0.9972088826562029, RMSE (Train): 0.08940386027097702\n",
      "Test Loss: 0.8325761556625366, R2 Score (Test): 0.6422329570604454, RMSE (Test): 0.938212513923645\n",
      "Epoch [586/5000], Train Loss: 0.01559124638636907, R2 Score (Train): 0.9964795997011733, RMSE (Train): 0.1044171154499054\n",
      "Test Loss: 0.8260781764984131, R2 Score (Test): 0.6439011036923077, RMSE (Test): 0.9360227584838867\n",
      "Epoch [591/5000], Train Loss: 0.017591586181273062, R2 Score (Train): 0.9968734169005088, RMSE (Train): 0.09283711761236191\n",
      "Test Loss: 0.794588029384613, R2 Score (Test): 0.6629424793967, RMSE (Test): 0.910653293132782\n",
      "Epoch [596/5000], Train Loss: 0.019787329869965713, R2 Score (Train): 0.9950425584815219, RMSE (Train): 0.11812364310026169\n",
      "Test Loss: 0.8037331700325012, R2 Score (Test): 0.654336892736175, RMSE (Test): 0.9222051501274109\n",
      "Epoch [601/5000], Train Loss: 0.014582612551748753, R2 Score (Train): 0.9967039565416101, RMSE (Train): 0.09009627997875214\n",
      "Test Loss: 0.831106960773468, R2 Score (Test): 0.6462380300720416, RMSE (Test): 0.9329462647438049\n",
      "Epoch [606/5000], Train Loss: 0.020670035233100254, R2 Score (Train): 0.9953637028127108, RMSE (Train): 0.12360292673110962\n",
      "Test Loss: 0.8404771387577057, R2 Score (Test): 0.6462492172526302, RMSE (Test): 0.9329314827919006\n",
      "Epoch [611/5000], Train Loss: 0.031144992758830387, R2 Score (Train): 0.996022417079212, RMSE (Train): 0.10256758332252502\n",
      "Test Loss: 0.8031798601150513, R2 Score (Test): 0.6536504629483724, RMSE (Test): 0.9231203198432922\n",
      "Epoch [616/5000], Train Loss: 0.01988769043236971, R2 Score (Train): 0.9972631463620975, RMSE (Train): 0.08412868529558182\n",
      "Test Loss: 0.8045101761817932, R2 Score (Test): 0.6573272879146241, RMSE (Test): 0.9182074666023254\n",
      "Epoch [621/5000], Train Loss: 0.01381884515285492, R2 Score (Train): 0.9963363017852505, RMSE (Train): 0.10707897692918777\n",
      "Test Loss: 0.8714649975299835, R2 Score (Test): 0.6322494946529806, RMSE (Test): 0.9512127637863159\n",
      "Epoch [626/5000], Train Loss: 0.020407474289337795, R2 Score (Train): 0.9980655793285618, RMSE (Train): 0.08034554123878479\n",
      "Test Loss: 0.8254776298999786, R2 Score (Test): 0.6495430496573615, RMSE (Test): 0.9285779595375061\n",
      "Epoch [631/5000], Train Loss: 0.021861227539678414, R2 Score (Train): 0.9962019569590401, RMSE (Train): 0.09085262566804886\n",
      "Test Loss: 0.8808549046516418, R2 Score (Test): 0.625202467393217, RMSE (Test): 0.9602833390235901\n",
      "Epoch [636/5000], Train Loss: 0.01767957645157973, R2 Score (Train): 0.9960713078294758, RMSE (Train): 0.11155515164136887\n",
      "Test Loss: 0.8463360369205475, R2 Score (Test): 0.6420933231343285, RMSE (Test): 0.9383955597877502\n",
      "Epoch [641/5000], Train Loss: 0.019785425625741482, R2 Score (Train): 0.9938729315966482, RMSE (Train): 0.12307018786668777\n",
      "Test Loss: 0.8710079789161682, R2 Score (Test): 0.6363537769091852, RMSE (Test): 0.9458898305892944\n",
      "Epoch [646/5000], Train Loss: 0.022073337187369663, R2 Score (Train): 0.9949193384201507, RMSE (Train): 0.11943873018026352\n",
      "Test Loss: 0.829925149679184, R2 Score (Test): 0.649678479732014, RMSE (Test): 0.9283985495567322\n",
      "Epoch [651/5000], Train Loss: 0.01755911282574137, R2 Score (Train): 0.9962563261301252, RMSE (Train): 0.10210291296243668\n",
      "Test Loss: 0.8354987502098083, R2 Score (Test): 0.6489585073002408, RMSE (Test): 0.9293520450592041\n",
      "Epoch [656/5000], Train Loss: 0.024369532242417336, R2 Score (Train): 0.9903473720529237, RMSE (Train): 0.13551059365272522\n",
      "Test Loss: 0.7814975380897522, R2 Score (Test): 0.6704474788104541, RMSE (Test): 0.9004577994346619\n",
      "Epoch [661/5000], Train Loss: 0.026034006538490456, R2 Score (Train): 0.9960405744543257, RMSE (Train): 0.09572891891002655\n",
      "Test Loss: 0.8446401357650757, R2 Score (Test): 0.6435458334390702, RMSE (Test): 0.936489462852478\n",
      "Epoch [666/5000], Train Loss: 0.025863540979723137, R2 Score (Train): 0.9923272393776642, RMSE (Train): 0.13869331777095795\n",
      "Test Loss: 0.8787433207035065, R2 Score (Test): 0.6313284745471686, RMSE (Test): 0.95240318775177\n",
      "Epoch [671/5000], Train Loss: 0.025331728470822174, R2 Score (Train): 0.9947342505682459, RMSE (Train): 0.11650865525007248\n",
      "Test Loss: 0.8528156280517578, R2 Score (Test): 0.641193558913252, RMSE (Test): 0.9395743608474731\n",
      "Epoch [676/5000], Train Loss: 0.032654715391496815, R2 Score (Train): 0.9952405839011765, RMSE (Train): 0.12605635821819305\n",
      "Test Loss: 0.8217140138149261, R2 Score (Test): 0.6546469648531903, RMSE (Test): 0.9217914938926697\n",
      "Epoch [681/5000], Train Loss: 0.027217521021763485, R2 Score (Train): 0.9960643012292932, RMSE (Train): 0.10770037770271301\n",
      "Test Loss: 0.8267703950405121, R2 Score (Test): 0.6526558955712443, RMSE (Test): 0.924444854259491\n",
      "Epoch [686/5000], Train Loss: 0.019585149673124153, R2 Score (Train): 0.9961207332145214, RMSE (Train): 0.10433031618595123\n",
      "Test Loss: 0.8397424817085266, R2 Score (Test): 0.6443760860251806, RMSE (Test): 0.9353981614112854\n",
      "Epoch [691/5000], Train Loss: 0.02296289475634694, R2 Score (Train): 0.9946483032080715, RMSE (Train): 0.12437471002340317\n",
      "Test Loss: 0.8875689804553986, R2 Score (Test): 0.6222227199688755, RMSE (Test): 0.9640930891036987\n",
      "Epoch [696/5000], Train Loss: 0.03590584391107162, R2 Score (Train): 0.9956221537318809, RMSE (Train): 0.09941113740205765\n",
      "Test Loss: 0.8337072432041168, R2 Score (Test): 0.6468404057072639, RMSE (Test): 0.9321514964103699\n",
      "Epoch [701/5000], Train Loss: 0.027584300842136145, R2 Score (Train): 0.9951992461084194, RMSE (Train): 0.10536159574985504\n",
      "Test Loss: 0.8768545389175415, R2 Score (Test): 0.6306710935626906, RMSE (Test): 0.9532518982887268\n",
      "Epoch [706/5000], Train Loss: 0.019143411424010992, R2 Score (Train): 0.9960215067828987, RMSE (Train): 0.10073408484458923\n",
      "Test Loss: 0.8256797194480896, R2 Score (Test): 0.6539424071888988, RMSE (Test): 0.9227312803268433\n",
      "Epoch [711/5000], Train Loss: 0.01660195815687378, R2 Score (Train): 0.9973475088210308, RMSE (Train): 0.09057315438985825\n",
      "Test Loss: 0.7934548556804657, R2 Score (Test): 0.6628662388576242, RMSE (Test): 0.9107562303543091\n",
      "Epoch [716/5000], Train Loss: 0.024412787053734064, R2 Score (Train): 0.9964769095005319, RMSE (Train): 0.10912492871284485\n",
      "Test Loss: 0.8484771847724915, R2 Score (Test): 0.6412864828058666, RMSE (Test): 0.9394526481628418\n",
      "Epoch [721/5000], Train Loss: 0.021844705566763878, R2 Score (Train): 0.9970285428476329, RMSE (Train): 0.10036154836416245\n",
      "Test Loss: 0.8039744794368744, R2 Score (Test): 0.6637988555759929, RMSE (Test): 0.9094956517219543\n",
      "Epoch [726/5000], Train Loss: 0.022101545551170904, R2 Score (Train): 0.9948117151013876, RMSE (Train): 0.09827647358179092\n",
      "Test Loss: 0.7685687243938446, R2 Score (Test): 0.6736133536755098, RMSE (Test): 0.8961221575737\n",
      "Epoch [731/5000], Train Loss: 0.025700964033603668, R2 Score (Train): 0.9958261745548601, RMSE (Train): 0.11033168435096741\n",
      "Test Loss: 0.831359475851059, R2 Score (Test): 0.6491334867754928, RMSE (Test): 0.9291204810142517\n",
      "Epoch [736/5000], Train Loss: 0.023142851578692596, R2 Score (Train): 0.9967987337489522, RMSE (Train): 0.09472382068634033\n",
      "Test Loss: 0.7919201552867889, R2 Score (Test): 0.6637495895204135, RMSE (Test): 0.9095622301101685\n",
      "Epoch [741/5000], Train Loss: 0.03180460228274266, R2 Score (Train): 0.9968195755702198, RMSE (Train): 0.0986236184835434\n",
      "Test Loss: 0.7925429344177246, R2 Score (Test): 0.6663249260012674, RMSE (Test): 0.9060723781585693\n",
      "Epoch [746/5000], Train Loss: 0.02166596722478668, R2 Score (Train): 0.9968209089135046, RMSE (Train): 0.10089094936847687\n",
      "Test Loss: 0.8439052104949951, R2 Score (Test): 0.6449597799748319, RMSE (Test): 0.9346302151679993\n",
      "Epoch [751/5000], Train Loss: 0.01565868578230341, R2 Score (Train): 0.9973177828810639, RMSE (Train): 0.08277078717947006\n",
      "Test Loss: 0.7862030863761902, R2 Score (Test): 0.6692953834278912, RMSE (Test): 0.9020304083824158\n",
      "Epoch [756/5000], Train Loss: 0.025835209836562473, R2 Score (Train): 0.9970628420078401, RMSE (Train): 0.0924684926867485\n",
      "Test Loss: 0.7371254861354828, R2 Score (Test): 0.6884557078935758, RMSE (Test): 0.8755096793174744\n",
      "Epoch [761/5000], Train Loss: 0.0159429971439143, R2 Score (Train): 0.9979002491889428, RMSE (Train): 0.07905121147632599\n",
      "Test Loss: 0.7945709824562073, R2 Score (Test): 0.6679653918930326, RMSE (Test): 0.9038424491882324\n",
      "Epoch [766/5000], Train Loss: 0.0208674989019831, R2 Score (Train): 0.9978211712696562, RMSE (Train): 0.08814988285303116\n",
      "Test Loss: 0.7628293037414551, R2 Score (Test): 0.6788442651090245, RMSE (Test): 0.8889122009277344\n",
      "Epoch [771/5000], Train Loss: 0.016700781260927517, R2 Score (Train): 0.9964431338112018, RMSE (Train): 0.10305029898881912\n",
      "Test Loss: 0.8128747344017029, R2 Score (Test): 0.6580756260088991, RMSE (Test): 0.9172043204307556\n",
      "Epoch [776/5000], Train Loss: 0.015111002915849289, R2 Score (Train): 0.9974160300340971, RMSE (Train): 0.08921144902706146\n",
      "Test Loss: 0.78534135222435, R2 Score (Test): 0.6732000288079878, RMSE (Test): 0.8966894745826721\n",
      "Epoch [781/5000], Train Loss: 0.013062138576060534, R2 Score (Train): 0.996575307993848, RMSE (Train): 0.10151726007461548\n",
      "Test Loss: 0.7733522653579712, R2 Score (Test): 0.6737015803270132, RMSE (Test): 0.8960011601448059\n",
      "Epoch [786/5000], Train Loss: 0.01693320708970229, R2 Score (Train): 0.9965711111204723, RMSE (Train): 0.11054162681102753\n",
      "Test Loss: 0.808068573474884, R2 Score (Test): 0.6609008284020883, RMSE (Test): 0.9134071469306946\n",
      "Epoch [791/5000], Train Loss: 0.01388148901363214, R2 Score (Train): 0.9970032004392688, RMSE (Train): 0.09565069526433945\n",
      "Test Loss: 0.7715297341346741, R2 Score (Test): 0.6778155747520137, RMSE (Test): 0.8903348445892334\n",
      "Epoch [796/5000], Train Loss: 0.02658832274998228, R2 Score (Train): 0.9978514351819856, RMSE (Train): 0.07497408986091614\n",
      "Test Loss: 0.7276982963085175, R2 Score (Test): 0.6924033776298695, RMSE (Test): 0.8699450492858887\n",
      "Epoch [801/5000], Train Loss: 0.021607366700967152, R2 Score (Train): 0.9961482547955687, RMSE (Train): 0.09573345631361008\n",
      "Test Loss: 0.7838199436664581, R2 Score (Test): 0.6762632174818652, RMSE (Test): 0.8924770951271057\n",
      "Epoch [806/5000], Train Loss: 0.013703657779842615, R2 Score (Train): 0.9956878616754876, RMSE (Train): 0.10165994614362717\n",
      "Test Loss: 0.7735423445701599, R2 Score (Test): 0.6730941295032448, RMSE (Test): 0.896834671497345\n",
      "Epoch [811/5000], Train Loss: 0.026801861201723415, R2 Score (Train): 0.9964306199721535, RMSE (Train): 0.10087087750434875\n",
      "Test Loss: 0.7810282111167908, R2 Score (Test): 0.6722107812354585, RMSE (Test): 0.8980455994606018\n",
      "Epoch [816/5000], Train Loss: 0.02323554301013549, R2 Score (Train): 0.9932412113556385, RMSE (Train): 0.13370077311992645\n",
      "Test Loss: 0.8034349083900452, R2 Score (Test): 0.6602751882746432, RMSE (Test): 0.9142494201660156\n",
      "Epoch [821/5000], Train Loss: 0.01899451979746421, R2 Score (Train): 0.9958198991411423, RMSE (Train): 0.10202488303184509\n",
      "Test Loss: 0.7580256462097168, R2 Score (Test): 0.6787192574711626, RMSE (Test): 0.8890852928161621\n",
      "Epoch [826/5000], Train Loss: 0.015632908791303635, R2 Score (Train): 0.9969244127864415, RMSE (Train): 0.08448203653097153\n",
      "Test Loss: 0.7999182641506195, R2 Score (Test): 0.6649194448621696, RMSE (Test): 0.9079786539077759\n",
      "Epoch [831/5000], Train Loss: 0.011836144141852856, R2 Score (Train): 0.9980021065329138, RMSE (Train): 0.07598339021205902\n",
      "Test Loss: 0.7670952081680298, R2 Score (Test): 0.6790216171209236, RMSE (Test): 0.8886666893959045\n",
      "Epoch [836/5000], Train Loss: 0.01569893304258585, R2 Score (Train): 0.99615241088074, RMSE (Train): 0.09840337187051773\n",
      "Test Loss: 0.7989824116230011, R2 Score (Test): 0.6697666156743112, RMSE (Test): 0.9013874530792236\n",
      "Epoch [841/5000], Train Loss: 0.01720394582177202, R2 Score (Train): 0.9964164063041101, RMSE (Train): 0.10301464796066284\n",
      "Test Loss: 0.7498461008071899, R2 Score (Test): 0.6857030754871414, RMSE (Test): 0.8793689608573914\n",
      "Epoch [846/5000], Train Loss: 0.020663568439582985, R2 Score (Train): 0.9943957082909101, RMSE (Train): 0.11071193218231201\n",
      "Test Loss: 0.8050384819507599, R2 Score (Test): 0.66490657336401, RMSE (Test): 0.9079960584640503\n",
      "Epoch [851/5000], Train Loss: 0.019170872401446104, R2 Score (Train): 0.9969786879163164, RMSE (Train): 0.08942282944917679\n",
      "Test Loss: 0.7580582797527313, R2 Score (Test): 0.6828641268085713, RMSE (Test): 0.8833315372467041\n",
      "Epoch [856/5000], Train Loss: 0.02970724335561196, R2 Score (Train): 0.9941490011935843, RMSE (Train): 0.13183921575546265\n",
      "Test Loss: 0.8475600779056549, R2 Score (Test): 0.6497062369368021, RMSE (Test): 0.9283618330955505\n",
      "Epoch [861/5000], Train Loss: 0.03345423564314842, R2 Score (Train): 0.9948007607262612, RMSE (Train): 0.12530332803726196\n",
      "Test Loss: 0.7799089848995209, R2 Score (Test): 0.6715845627382458, RMSE (Test): 0.898902952671051\n",
      "Epoch [866/5000], Train Loss: 0.02493996514628331, R2 Score (Train): 0.9957744371922713, RMSE (Train): 0.10769131779670715\n",
      "Test Loss: 0.7520608007907867, R2 Score (Test): 0.6833029983021475, RMSE (Test): 0.8827201128005981\n",
      "Epoch [871/5000], Train Loss: 0.018288908371080954, R2 Score (Train): 0.9963687286818529, RMSE (Train): 0.1021924689412117\n",
      "Test Loss: 0.783930093050003, R2 Score (Test): 0.6721372999761563, RMSE (Test): 0.8981462717056274\n",
      "Epoch [876/5000], Train Loss: 0.016871113640566666, R2 Score (Train): 0.9948924114156782, RMSE (Train): 0.12424281984567642\n",
      "Test Loss: 0.7886029779911041, R2 Score (Test): 0.6675714071287295, RMSE (Test): 0.9043785333633423\n",
      "Epoch [881/5000], Train Loss: 0.018147475086152554, R2 Score (Train): 0.9956233517657734, RMSE (Train): 0.10566826164722443\n",
      "Test Loss: 0.7785558104515076, R2 Score (Test): 0.6726706590078071, RMSE (Test): 0.8974153995513916\n",
      "Epoch [886/5000], Train Loss: 0.014891000464558601, R2 Score (Train): 0.9972132068185668, RMSE (Train): 0.09552424401044846\n",
      "Test Loss: 0.7598164081573486, R2 Score (Test): 0.6823433625475307, RMSE (Test): 0.8840565085411072\n",
      "Epoch [891/5000], Train Loss: 0.021312491036951542, R2 Score (Train): 0.9957992008410348, RMSE (Train): 0.10121449083089828\n",
      "Test Loss: 0.7710279226303101, R2 Score (Test): 0.6776035816668928, RMSE (Test): 0.8906276226043701\n",
      "Epoch [896/5000], Train Loss: 0.02117411109308402, R2 Score (Train): 0.9952914934657634, RMSE (Train): 0.11613485217094421\n",
      "Test Loss: 0.8116913735866547, R2 Score (Test): 0.6581612038116467, RMSE (Test): 0.9170894622802734\n",
      "Epoch [901/5000], Train Loss: 0.019539159256964922, R2 Score (Train): 0.9952103702462944, RMSE (Train): 0.09998013824224472\n",
      "Test Loss: 0.7366406321525574, R2 Score (Test): 0.6904592061802459, RMSE (Test): 0.8726900219917297\n",
      "Epoch [906/5000], Train Loss: 0.016248499664167564, R2 Score (Train): 0.9975611943058253, RMSE (Train): 0.07794557511806488\n",
      "Test Loss: 0.7580941617488861, R2 Score (Test): 0.6831327403197864, RMSE (Test): 0.882957398891449\n",
      "Epoch [911/5000], Train Loss: 0.016063461856295664, R2 Score (Train): 0.9966407009716745, RMSE (Train): 0.1029861643910408\n",
      "Test Loss: 0.7531355917453766, R2 Score (Test): 0.6884092315103033, RMSE (Test): 0.875575065612793\n",
      "Epoch [916/5000], Train Loss: 0.019066279598822195, R2 Score (Train): 0.9972651650018614, RMSE (Train): 0.09120996296405792\n",
      "Test Loss: 0.7653698027133942, R2 Score (Test): 0.680046545773331, RMSE (Test): 0.88724684715271\n",
      "Epoch [921/5000], Train Loss: 0.019721753274401028, R2 Score (Train): 0.9969396215349473, RMSE (Train): 0.10583470016717911\n",
      "Test Loss: 0.765739232301712, R2 Score (Test): 0.682086362221499, RMSE (Test): 0.88441401720047\n",
      "Epoch [926/5000], Train Loss: 0.020832032586137455, R2 Score (Train): 0.9953057358040734, RMSE (Train): 0.10600943863391876\n",
      "Test Loss: 0.7464821934700012, R2 Score (Test): 0.6903854603399446, RMSE (Test): 0.872793972492218\n",
      "Epoch [931/5000], Train Loss: 0.016951290890574455, R2 Score (Train): 0.9980187369496328, RMSE (Train): 0.08958076685667038\n",
      "Test Loss: 0.7469693422317505, R2 Score (Test): 0.6886954508649629, RMSE (Test): 0.8751726746559143\n",
      "Epoch [936/5000], Train Loss: 0.016359196820606787, R2 Score (Train): 0.9969851657948311, RMSE (Train): 0.09140347689390182\n",
      "Test Loss: 0.7562488615512848, R2 Score (Test): 0.6840746325259823, RMSE (Test): 0.8816440105438232\n",
      "Epoch [941/5000], Train Loss: 0.019238004460930824, R2 Score (Train): 0.9968793227815161, RMSE (Train): 0.09743139892816544\n",
      "Test Loss: 0.7503567039966583, R2 Score (Test): 0.6870244879272438, RMSE (Test): 0.8775184154510498\n",
      "Epoch [946/5000], Train Loss: 0.01966364790375034, R2 Score (Train): 0.996663470736232, RMSE (Train): 0.09554553031921387\n",
      "Test Loss: 0.7619229853153229, R2 Score (Test): 0.6816018641818873, RMSE (Test): 0.8850876688957214\n",
      "Epoch [951/5000], Train Loss: 0.019630133950461943, R2 Score (Train): 0.9961050543896949, RMSE (Train): 0.10270248353481293\n",
      "Test Loss: 0.7090605795383453, R2 Score (Test): 0.7013501784699607, RMSE (Test): 0.857200026512146\n",
      "Epoch [956/5000], Train Loss: 0.019782342637578647, R2 Score (Train): 0.99412232893319, RMSE (Train): 0.12510986626148224\n",
      "Test Loss: 0.7496338486671448, R2 Score (Test): 0.6880625142378252, RMSE (Test): 0.8760619759559631\n",
      "Epoch [961/5000], Train Loss: 0.017858018943419058, R2 Score (Train): 0.9967905235673957, RMSE (Train): 0.09695659577846527\n",
      "Test Loss: 0.7175123393535614, R2 Score (Test): 0.7003470423872269, RMSE (Test): 0.8586384654045105\n",
      "Epoch [966/5000], Train Loss: 0.020780133393903572, R2 Score (Train): 0.9942511995597514, RMSE (Train): 0.11926727741956711\n",
      "Test Loss: 0.7336533665657043, R2 Score (Test): 0.6944158095442519, RMSE (Test): 0.8670945763587952\n",
      "Epoch [971/5000], Train Loss: 0.017962550123532612, R2 Score (Train): 0.9958950428187964, RMSE (Train): 0.10965277254581451\n",
      "Test Loss: 0.7287474572658539, R2 Score (Test): 0.6988529028033326, RMSE (Test): 0.8607765436172485\n",
      "Epoch [976/5000], Train Loss: 0.014921719829241434, R2 Score (Train): 0.9971279582859397, RMSE (Train): 0.08589372783899307\n",
      "Test Loss: 0.7413258254528046, R2 Score (Test): 0.689800605720222, RMSE (Test): 0.8736178874969482\n",
      "Epoch [981/5000], Train Loss: 0.014015601327021917, R2 Score (Train): 0.9958807521469931, RMSE (Train): 0.10644415020942688\n",
      "Test Loss: 0.7478820383548737, R2 Score (Test): 0.6864217676202737, RMSE (Test): 0.8783629536628723\n",
      "Epoch [986/5000], Train Loss: 0.016118033323436975, R2 Score (Train): 0.997141859113647, RMSE (Train): 0.08688695728778839\n",
      "Test Loss: 0.730270117521286, R2 Score (Test): 0.6965889529673983, RMSE (Test): 0.864005982875824\n",
      "Epoch [991/5000], Train Loss: 0.013537374635537466, R2 Score (Train): 0.998083234610239, RMSE (Train): 0.0731995701789856\n",
      "Test Loss: 0.7205951809883118, R2 Score (Test): 0.7012068275376515, RMSE (Test): 0.8574056625366211\n",
      "Epoch [996/5000], Train Loss: 0.013178637639308969, R2 Score (Train): 0.9961370687781467, RMSE (Train): 0.1041608452796936\n",
      "Test Loss: 0.736825555562973, R2 Score (Test): 0.6910444951298305, RMSE (Test): 0.8718644976615906\n",
      "Epoch [1001/5000], Train Loss: 0.02202722088744243, R2 Score (Train): 0.9950258179982989, RMSE (Train): 0.12102929502725601\n",
      "Test Loss: 0.7649336755275726, R2 Score (Test): 0.6793949802805017, RMSE (Test): 0.8881497979164124\n",
      "Epoch [1006/5000], Train Loss: 0.0121256901572148, R2 Score (Train): 0.9958137458229163, RMSE (Train): 0.10859310626983643\n",
      "Test Loss: 0.7262203097343445, R2 Score (Test): 0.695026534971232, RMSE (Test): 0.8662277460098267\n",
      "Epoch [1011/5000], Train Loss: 0.018375732159862917, R2 Score (Train): 0.9974566667260231, RMSE (Train): 0.08772086352109909\n",
      "Test Loss: 0.7158379256725311, R2 Score (Test): 0.7001756474167596, RMSE (Test): 0.8588839769363403\n",
      "Epoch [1016/5000], Train Loss: 0.014188443310558796, R2 Score (Train): 0.9971364892670674, RMSE (Train): 0.09287149459123611\n",
      "Test Loss: 0.7255479097366333, R2 Score (Test): 0.7022861123124851, RMSE (Test): 0.8558557629585266\n",
      "Epoch [1021/5000], Train Loss: 0.012971703118334213, R2 Score (Train): 0.9967904790158302, RMSE (Train): 0.09577453881502151\n",
      "Test Loss: 0.7006827294826508, R2 Score (Test): 0.7076968134533017, RMSE (Test): 0.8480429649353027\n",
      "Epoch [1026/5000], Train Loss: 0.015456140041351318, R2 Score (Train): 0.9970983928384438, RMSE (Train): 0.09256197512149811\n",
      "Test Loss: 0.7176036834716797, R2 Score (Test): 0.7039149756235135, RMSE (Test): 0.8535112738609314\n",
      "Epoch [1031/5000], Train Loss: 0.014120313183714947, R2 Score (Train): 0.9974377581717522, RMSE (Train): 0.07904718816280365\n",
      "Test Loss: 0.7193400859832764, R2 Score (Test): 0.7000638985095143, RMSE (Test): 0.859044075012207\n",
      "Epoch [1036/5000], Train Loss: 0.0176100501169761, R2 Score (Train): 0.9977598850698753, RMSE (Train): 0.07934825122356415\n",
      "Test Loss: 0.7224160134792328, R2 Score (Test): 0.6980192551098581, RMSE (Test): 0.8619669675827026\n",
      "Epoch [1041/5000], Train Loss: 0.012029118370264769, R2 Score (Train): 0.9976586509478802, RMSE (Train): 0.08268123865127563\n",
      "Test Loss: 0.7206155061721802, R2 Score (Test): 0.7000967513471656, RMSE (Test): 0.8589969277381897\n",
      "Epoch [1046/5000], Train Loss: 0.017723925101260345, R2 Score (Train): 0.9963454236125572, RMSE (Train): 0.11276090890169144\n",
      "Test Loss: 0.7085945904254913, R2 Score (Test): 0.7023853715896795, RMSE (Test): 0.8557131290435791\n",
      "Epoch [1051/5000], Train Loss: 0.011060050378243128, R2 Score (Train): 0.9976054616058914, RMSE (Train): 0.0893181562423706\n",
      "Test Loss: 0.7415498495101929, R2 Score (Test): 0.6951846728666382, RMSE (Test): 0.8660030364990234\n",
      "Epoch [1056/5000], Train Loss: 0.015364048536866903, R2 Score (Train): 0.9956119219312078, RMSE (Train): 0.09804242104291916\n",
      "Test Loss: 0.7335943877696991, R2 Score (Test): 0.6972774656101852, RMSE (Test): 0.8630250692367554\n",
      "Epoch [1061/5000], Train Loss: 0.017891416947046917, R2 Score (Train): 0.9976463448391825, RMSE (Train): 0.08837659657001495\n",
      "Test Loss: 0.7515460550785065, R2 Score (Test): 0.6904861490321212, RMSE (Test): 0.872651994228363\n",
      "Epoch [1066/5000], Train Loss: 0.014942620570460955, R2 Score (Train): 0.9968931755491766, RMSE (Train): 0.10563863813877106\n",
      "Test Loss: 0.6844313442707062, R2 Score (Test): 0.7148950303540325, RMSE (Test): 0.8375359177589417\n",
      "Epoch [1071/5000], Train Loss: 0.017996401991695166, R2 Score (Train): 0.9955284161818142, RMSE (Train): 0.10392897576093674\n",
      "Test Loss: 0.720384269952774, R2 Score (Test): 0.7007218600891083, RMSE (Test): 0.8581012487411499\n",
      "Epoch [1076/5000], Train Loss: 0.017044850004216034, R2 Score (Train): 0.9970871049957615, RMSE (Train): 0.08719423413276672\n",
      "Test Loss: 0.7390734255313873, R2 Score (Test): 0.695870991805787, RMSE (Test): 0.865027666091919\n",
      "Epoch [1081/5000], Train Loss: 0.022890105843544006, R2 Score (Train): 0.9938060252787959, RMSE (Train): 0.13077299296855927\n",
      "Test Loss: 0.7141077220439911, R2 Score (Test): 0.7080959359351378, RMSE (Test): 0.8474636673927307\n",
      "Epoch [1086/5000], Train Loss: 0.01891410645718376, R2 Score (Train): 0.9968819281642243, RMSE (Train): 0.09703198820352554\n",
      "Test Loss: 0.7006344795227051, R2 Score (Test): 0.710298775261879, RMSE (Test): 0.844260036945343\n",
      "Epoch [1091/5000], Train Loss: 0.01683979605635007, R2 Score (Train): 0.9971810282855993, RMSE (Train): 0.09731966257095337\n",
      "Test Loss: 0.7112674117088318, R2 Score (Test): 0.7010022845130943, RMSE (Test): 0.8576992154121399\n",
      "Epoch [1096/5000], Train Loss: 0.015149290828655163, R2 Score (Train): 0.9953438758885819, RMSE (Train): 0.09963305294513702\n",
      "Test Loss: 0.7213273644447327, R2 Score (Test): 0.7033519919502909, RMSE (Test): 0.8543223738670349\n",
      "Epoch [1101/5000], Train Loss: 0.017088840405146282, R2 Score (Train): 0.9936967113081449, RMSE (Train): 0.1214742586016655\n",
      "Test Loss: 0.75172358751297, R2 Score (Test): 0.6844570496951838, RMSE (Test): 0.8811102509498596\n",
      "Epoch [1106/5000], Train Loss: 0.014515334895501534, R2 Score (Train): 0.9976357670786924, RMSE (Train): 0.08589432388544083\n",
      "Test Loss: 0.697542816400528, R2 Score (Test): 0.7099208219593738, RMSE (Test): 0.8448104858398438\n",
      "Epoch [1111/5000], Train Loss: 0.013489815561721722, R2 Score (Train): 0.9975181593864213, RMSE (Train): 0.09150031208992004\n",
      "Test Loss: 0.7400615811347961, R2 Score (Test): 0.693356360928324, RMSE (Test): 0.8685963749885559\n",
      "Epoch [1116/5000], Train Loss: 0.021741411027808983, R2 Score (Train): 0.9956544738786361, RMSE (Train): 0.11707402020692825\n",
      "Test Loss: 0.6899310350418091, R2 Score (Test): 0.7136122354883252, RMSE (Test): 0.8394179940223694\n",
      "Epoch [1121/5000], Train Loss: 0.02449804327140252, R2 Score (Train): 0.9947235119781023, RMSE (Train): 0.12150246649980545\n",
      "Test Loss: 0.7739403545856476, R2 Score (Test): 0.6745324701850561, RMSE (Test): 0.8948594927787781\n",
      "Epoch [1126/5000], Train Loss: 0.028209694971640904, R2 Score (Train): 0.9942449042912304, RMSE (Train): 0.11356043070554733\n",
      "Test Loss: 0.7061710953712463, R2 Score (Test): 0.7006105340556786, RMSE (Test): 0.8582608699798584\n",
      "Epoch [1131/5000], Train Loss: 0.019229879913230736, R2 Score (Train): 0.9942542035648273, RMSE (Train): 0.12379113584756851\n",
      "Test Loss: 0.6922812759876251, R2 Score (Test): 0.7095448110805352, RMSE (Test): 0.8453578948974609\n",
      "Epoch [1136/5000], Train Loss: 0.009824650439744195, R2 Score (Train): 0.9968001017534557, RMSE (Train): 0.09014792740345001\n",
      "Test Loss: 0.7020070850849152, R2 Score (Test): 0.7047119600018952, RMSE (Test): 0.852361798286438\n",
      "Epoch [1141/5000], Train Loss: 0.010152619642515978, R2 Score (Train): 0.9975987999272137, RMSE (Train): 0.08088980615139008\n",
      "Test Loss: 0.6958912014961243, R2 Score (Test): 0.7089124701124756, RMSE (Test): 0.8462775945663452\n",
      "Epoch [1146/5000], Train Loss: 0.012839714375634989, R2 Score (Train): 0.9940461552944985, RMSE (Train): 0.12471169233322144\n",
      "Test Loss: 0.715172290802002, R2 Score (Test): 0.7028495649018165, RMSE (Test): 0.85504549741745\n",
      "Epoch [1151/5000], Train Loss: 0.011298257935171327, R2 Score (Train): 0.9981960241831127, RMSE (Train): 0.0726168230175972\n",
      "Test Loss: 0.7058221995830536, R2 Score (Test): 0.7083057724091788, RMSE (Test): 0.8471590876579285\n",
      "Epoch [1156/5000], Train Loss: 0.011517451765636602, R2 Score (Train): 0.9971712954344379, RMSE (Train): 0.09599878638982773\n",
      "Test Loss: 0.6984517276287079, R2 Score (Test): 0.7115494311907912, RMSE (Test): 0.8424357175827026\n",
      "Epoch [1161/5000], Train Loss: 0.010365293206026157, R2 Score (Train): 0.9968065605529699, RMSE (Train): 0.09823931008577347\n",
      "Test Loss: 0.6996812522411346, R2 Score (Test): 0.708955095737942, RMSE (Test): 0.8462156653404236\n",
      "Epoch [1166/5000], Train Loss: 0.010271938983350992, R2 Score (Train): 0.9975994638373404, RMSE (Train): 0.08464615792036057\n",
      "Test Loss: 0.6976476311683655, R2 Score (Test): 0.7117865297865661, RMSE (Test): 0.8420893549919128\n",
      "Epoch [1171/5000], Train Loss: 0.010375070696075758, R2 Score (Train): 0.9967049506358168, RMSE (Train): 0.08676137030124664\n",
      "Test Loss: 0.703978031873703, R2 Score (Test): 0.708924049627828, RMSE (Test): 0.8462607860565186\n",
      "Epoch [1176/5000], Train Loss: 0.011053902407487234, R2 Score (Train): 0.9968765198297348, RMSE (Train): 0.09403324872255325\n",
      "Test Loss: 0.6996972262859344, R2 Score (Test): 0.7086672229631519, RMSE (Test): 0.846634030342102\n",
      "Epoch [1181/5000], Train Loss: 0.009364735800772905, R2 Score (Train): 0.997430073981314, RMSE (Train): 0.08263285458087921\n",
      "Test Loss: 0.7057139575481415, R2 Score (Test): 0.708600369548149, RMSE (Test): 0.8467311859130859\n",
      "Epoch [1186/5000], Train Loss: 0.013925788458436728, R2 Score (Train): 0.9973285378140884, RMSE (Train): 0.08739180117845535\n",
      "Test Loss: 0.7233251333236694, R2 Score (Test): 0.7016777049046263, RMSE (Test): 0.8567298054695129\n",
      "Epoch [1191/5000], Train Loss: 0.013274939265102148, R2 Score (Train): 0.9963678818489328, RMSE (Train): 0.0947677493095398\n",
      "Test Loss: 0.7031517922878265, R2 Score (Test): 0.7073853949178954, RMSE (Test): 0.8484945893287659\n",
      "Epoch [1196/5000], Train Loss: 0.009287386666983366, R2 Score (Train): 0.9978278485405258, RMSE (Train): 0.0837102010846138\n",
      "Test Loss: 0.7004371583461761, R2 Score (Test): 0.7104464205766359, RMSE (Test): 0.8440448641777039\n",
      "Epoch [1201/5000], Train Loss: 0.015457159373909235, R2 Score (Train): 0.9944770482879722, RMSE (Train): 0.12074794620275497\n",
      "Test Loss: 0.7033692002296448, R2 Score (Test): 0.7031399293710372, RMSE (Test): 0.8546276688575745\n",
      "Epoch [1206/5000], Train Loss: 0.012648762203752995, R2 Score (Train): 0.9944119901047787, RMSE (Train): 0.12935194373130798\n",
      "Test Loss: 0.7205358445644379, R2 Score (Test): 0.704834518413439, RMSE (Test): 0.8521848320960999\n",
      "Epoch [1211/5000], Train Loss: 0.012900238701452812, R2 Score (Train): 0.9976688080187908, RMSE (Train): 0.08347111940383911\n",
      "Test Loss: 0.6953420639038086, R2 Score (Test): 0.715194738091218, RMSE (Test): 0.8370956182479858\n",
      "Epoch [1216/5000], Train Loss: 0.017123248583326738, R2 Score (Train): 0.9972421942057574, RMSE (Train): 0.08152011781930923\n",
      "Test Loss: 0.7103290557861328, R2 Score (Test): 0.7083429950019648, RMSE (Test): 0.8471049666404724\n",
      "Epoch [1221/5000], Train Loss: 0.018989463802427053, R2 Score (Train): 0.9964926432980955, RMSE (Train): 0.09124383330345154\n",
      "Test Loss: 0.7202456593513489, R2 Score (Test): 0.702256133647119, RMSE (Test): 0.8558988571166992\n",
      "Epoch [1226/5000], Train Loss: 0.013797834049910307, R2 Score (Train): 0.9969803953370887, RMSE (Train): 0.10102907568216324\n",
      "Test Loss: 0.6790906190872192, R2 Score (Test): 0.7193712400226336, RMSE (Test): 0.8309351801872253\n",
      "Epoch [1231/5000], Train Loss: 0.011998464819043875, R2 Score (Train): 0.9958197710708061, RMSE (Train): 0.11174951493740082\n",
      "Test Loss: 0.6952440440654755, R2 Score (Test): 0.7117007108888738, RMSE (Test): 0.8422147631645203\n",
      "Epoch [1236/5000], Train Loss: 0.012119981925934553, R2 Score (Train): 0.9969552574995962, RMSE (Train): 0.10182129591703415\n",
      "Test Loss: 0.711217612028122, R2 Score (Test): 0.7071660680401869, RMSE (Test): 0.8488124012947083\n",
      "Epoch [1241/5000], Train Loss: 0.012558306101709604, R2 Score (Train): 0.9968589630927771, RMSE (Train): 0.087067149579525\n",
      "Test Loss: 0.7053394317626953, R2 Score (Test): 0.7038228509054338, RMSE (Test): 0.853644073009491\n",
      "Epoch [1246/5000], Train Loss: 0.01592038059607148, R2 Score (Train): 0.9974123161713251, RMSE (Train): 0.09177692979574203\n",
      "Test Loss: 0.6830593645572662, R2 Score (Test): 0.7160360914205681, RMSE (Test): 0.8358582258224487\n",
      "Epoch [1251/5000], Train Loss: 0.016379977731655043, R2 Score (Train): 0.9957565858560753, RMSE (Train): 0.10429423302412033\n",
      "Test Loss: 0.6916344165802002, R2 Score (Test): 0.7124545885710702, RMSE (Test): 0.8411127924919128\n",
      "Epoch [1256/5000], Train Loss: 0.014041237998753786, R2 Score (Train): 0.9979476005947705, RMSE (Train): 0.08325612545013428\n",
      "Test Loss: 0.6789841949939728, R2 Score (Test): 0.7204723055737832, RMSE (Test): 0.8293033838272095\n",
      "Epoch [1261/5000], Train Loss: 0.01132404524832964, R2 Score (Train): 0.9977459736791451, RMSE (Train): 0.08842479437589645\n",
      "Test Loss: 0.6938994824886322, R2 Score (Test): 0.7134874812025613, RMSE (Test): 0.8396008014678955\n",
      "Epoch [1266/5000], Train Loss: 0.0095394446204106, R2 Score (Train): 0.995250793897875, RMSE (Train): 0.11106765270233154\n",
      "Test Loss: 0.703211784362793, R2 Score (Test): 0.7111032082023951, RMSE (Test): 0.8430870175361633\n",
      "Epoch [1271/5000], Train Loss: 0.013601907606547078, R2 Score (Train): 0.9960367022417711, RMSE (Train): 0.10294299572706223\n",
      "Test Loss: 0.6977047026157379, R2 Score (Test): 0.7106731238126681, RMSE (Test): 0.8437143564224243\n",
      "Epoch [1276/5000], Train Loss: 0.01649948473398884, R2 Score (Train): 0.997604414239231, RMSE (Train): 0.07903191447257996\n",
      "Test Loss: 0.6856660842895508, R2 Score (Test): 0.7170883612516182, RMSE (Test): 0.8343080878257751\n",
      "Epoch [1281/5000], Train Loss: 0.014144596954186758, R2 Score (Train): 0.9976812798840614, RMSE (Train): 0.08293488621711731\n",
      "Test Loss: 0.69615238904953, R2 Score (Test): 0.7136845972306172, RMSE (Test): 0.839311957359314\n",
      "Epoch [1286/5000], Train Loss: 0.01693175546824932, R2 Score (Train): 0.9959106430242894, RMSE (Train): 0.0989130437374115\n",
      "Test Loss: 0.6929636001586914, R2 Score (Test): 0.7166133961822634, RMSE (Test): 0.8350081443786621\n",
      "Epoch [1291/5000], Train Loss: 0.013175726092110077, R2 Score (Train): 0.9968528780577118, RMSE (Train): 0.08958478271961212\n",
      "Test Loss: 0.6899624764919281, R2 Score (Test): 0.7169658829017718, RMSE (Test): 0.8344886302947998\n",
      "Epoch [1296/5000], Train Loss: 0.011760497155288855, R2 Score (Train): 0.9914686548577017, RMSE (Train): 0.1385476440191269\n",
      "Test Loss: 0.68888720870018, R2 Score (Test): 0.7166794477865002, RMSE (Test): 0.8349108695983887\n",
      "Epoch [1301/5000], Train Loss: 0.013279955368489027, R2 Score (Train): 0.9964992379858245, RMSE (Train): 0.10333304852247238\n",
      "Test Loss: 0.7040553092956543, R2 Score (Test): 0.709287723101954, RMSE (Test): 0.8457319736480713\n",
      "Epoch [1306/5000], Train Loss: 0.013811404506365458, R2 Score (Train): 0.9959589619443843, RMSE (Train): 0.10785665363073349\n",
      "Test Loss: 0.6892634332180023, R2 Score (Test): 0.7147674837150961, RMSE (Test): 0.8377232551574707\n",
      "Epoch [1311/5000], Train Loss: 0.010912734549492598, R2 Score (Train): 0.9982632545129799, RMSE (Train): 0.07177521288394928\n",
      "Test Loss: 0.689988523721695, R2 Score (Test): 0.7159163213960444, RMSE (Test): 0.8360344767570496\n",
      "Epoch [1316/5000], Train Loss: 0.013928273847947517, R2 Score (Train): 0.9968936991185874, RMSE (Train): 0.09734127670526505\n",
      "Test Loss: 0.6923985779285431, R2 Score (Test): 0.7157569670553408, RMSE (Test): 0.8362689018249512\n",
      "Epoch [1321/5000], Train Loss: 0.01612694716701905, R2 Score (Train): 0.9973112383707768, RMSE (Train): 0.092408187687397\n",
      "Test Loss: 0.7489148676395416, R2 Score (Test): 0.6953111606938636, RMSE (Test): 0.8658234477043152\n",
      "Epoch [1326/5000], Train Loss: 0.023351186731209356, R2 Score (Train): 0.9942509465435279, RMSE (Train): 0.12628687918186188\n",
      "Test Loss: 0.6747525930404663, R2 Score (Test): 0.7198120173276288, RMSE (Test): 0.8302823305130005\n",
      "Epoch [1331/5000], Train Loss: 0.017657962782929342, R2 Score (Train): 0.9957296644708946, RMSE (Train): 0.11029685288667679\n",
      "Test Loss: 0.6839938759803772, R2 Score (Test): 0.7162545581748192, RMSE (Test): 0.8355365991592407\n",
      "Epoch [1336/5000], Train Loss: 0.018193994959195454, R2 Score (Train): 0.9967941098112548, RMSE (Train): 0.09711283445358276\n",
      "Test Loss: 0.6771130859851837, R2 Score (Test): 0.7239737471167327, RMSE (Test): 0.8240929841995239\n",
      "Epoch [1341/5000], Train Loss: 0.01607788462812702, R2 Score (Train): 0.992641598295029, RMSE (Train): 0.14059697091579437\n",
      "Test Loss: 0.670344203710556, R2 Score (Test): 0.7264104252049712, RMSE (Test): 0.8204475045204163\n",
      "Epoch [1346/5000], Train Loss: 0.015731831546872854, R2 Score (Train): 0.9980481975199818, RMSE (Train): 0.07296784967184067\n",
      "Test Loss: 0.6844131350517273, R2 Score (Test): 0.7182721748498133, RMSE (Test): 0.8325607180595398\n",
      "Epoch [1351/5000], Train Loss: 0.016724077829470236, R2 Score (Train): 0.9926524131284219, RMSE (Train): 0.130891352891922\n",
      "Test Loss: 0.6893779039382935, R2 Score (Test): 0.7174510675410721, RMSE (Test): 0.833773136138916\n",
      "Epoch [1356/5000], Train Loss: 0.01671952521428466, R2 Score (Train): 0.998237595198017, RMSE (Train): 0.07073460519313812\n",
      "Test Loss: 0.6510723531246185, R2 Score (Test): 0.72902168266329, RMSE (Test): 0.8165227770805359\n",
      "Epoch [1361/5000], Train Loss: 0.014382362055281797, R2 Score (Train): 0.9957201120451258, RMSE (Train): 0.11453130841255188\n",
      "Test Loss: 0.6621576249599457, R2 Score (Test): 0.7266580529233162, RMSE (Test): 0.8200761079788208\n",
      "Epoch [1366/5000], Train Loss: 0.010449260628471771, R2 Score (Train): 0.9978635730057952, RMSE (Train): 0.07374708354473114\n",
      "Test Loss: 0.6600207090377808, R2 Score (Test): 0.7278625326830974, RMSE (Test): 0.8182674050331116\n",
      "Epoch [1371/5000], Train Loss: 0.008087459796418747, R2 Score (Train): 0.9967893811894207, RMSE (Train): 0.09989478439092636\n",
      "Test Loss: 0.6692469120025635, R2 Score (Test): 0.7261311638033182, RMSE (Test): 0.8208661675453186\n",
      "Epoch [1376/5000], Train Loss: 0.010646940053751072, R2 Score (Train): 0.996702519045542, RMSE (Train): 0.10030924528837204\n",
      "Test Loss: 0.655579149723053, R2 Score (Test): 0.7315238340438225, RMSE (Test): 0.8127442598342896\n",
      "Epoch [1381/5000], Train Loss: 0.010955954746653637, R2 Score (Train): 0.9970678675444729, RMSE (Train): 0.09007403999567032\n",
      "Test Loss: 0.6852211654186249, R2 Score (Test): 0.722135668268892, RMSE (Test): 0.8268324136734009\n",
      "Epoch [1386/5000], Train Loss: 0.015444240843256315, R2 Score (Train): 0.9974875505354573, RMSE (Train): 0.08938561379909515\n",
      "Test Loss: 0.6846459209918976, R2 Score (Test): 0.7197743763060258, RMSE (Test): 0.8303381204605103\n",
      "Epoch [1391/5000], Train Loss: 0.0160056723592182, R2 Score (Train): 0.9958891223274918, RMSE (Train): 0.11107462644577026\n",
      "Test Loss: 0.6762850284576416, R2 Score (Test): 0.7249053344885505, RMSE (Test): 0.8227011561393738\n",
      "Epoch [1396/5000], Train Loss: 0.011742729848871628, R2 Score (Train): 0.9967056464096519, RMSE (Train): 0.09591902047395706\n",
      "Test Loss: 0.669713705778122, R2 Score (Test): 0.7285199415682508, RMSE (Test): 0.8172783255577087\n",
      "Epoch [1401/5000], Train Loss: 0.012355949264019728, R2 Score (Train): 0.9974027488651055, RMSE (Train): 0.08913488686084747\n",
      "Test Loss: 0.6780259013175964, R2 Score (Test): 0.7254307322936413, RMSE (Test): 0.8219152092933655\n",
      "Epoch [1406/5000], Train Loss: 0.01708355126902461, R2 Score (Train): 0.9958332047564764, RMSE (Train): 0.1123165711760521\n",
      "Test Loss: 0.6906352341175079, R2 Score (Test): 0.7151640419692495, RMSE (Test): 0.837140679359436\n",
      "Epoch [1411/5000], Train Loss: 0.014606278855353594, R2 Score (Train): 0.9976216671669667, RMSE (Train): 0.0784478634595871\n",
      "Test Loss: 0.6739494204521179, R2 Score (Test): 0.7246305692724059, RMSE (Test): 0.8231119513511658\n",
      "Epoch [1416/5000], Train Loss: 0.020285452095170815, R2 Score (Train): 0.9950005446272843, RMSE (Train): 0.11402980983257294\n",
      "Test Loss: 0.6810017228126526, R2 Score (Test): 0.719701435937725, RMSE (Test): 0.830446183681488\n",
      "Epoch [1421/5000], Train Loss: 0.015654788197328646, R2 Score (Train): 0.997972983991659, RMSE (Train): 0.06975944340229034\n",
      "Test Loss: 0.6552084684371948, R2 Score (Test): 0.732554973782763, RMSE (Test): 0.8111820220947266\n",
      "Epoch [1426/5000], Train Loss: 0.011672741267830133, R2 Score (Train): 0.9972880056199828, RMSE (Train): 0.08174898475408554\n",
      "Test Loss: 0.6805128157138824, R2 Score (Test): 0.7208902633292753, RMSE (Test): 0.8286832571029663\n",
      "Epoch [1431/5000], Train Loss: 0.013878422634055218, R2 Score (Train): 0.9968155188639308, RMSE (Train): 0.09044297784566879\n",
      "Test Loss: 0.6588637828826904, R2 Score (Test): 0.7280010354632502, RMSE (Test): 0.8180590867996216\n",
      "Epoch [1436/5000], Train Loss: 0.012248072229946652, R2 Score (Train): 0.9972871658201754, RMSE (Train): 0.08974806219339371\n",
      "Test Loss: 0.6524935066699982, R2 Score (Test): 0.7313931234006019, RMSE (Test): 0.8129420280456543\n",
      "Epoch [1441/5000], Train Loss: 0.012320333470900854, R2 Score (Train): 0.9961197454505508, RMSE (Train): 0.1001458466053009\n",
      "Test Loss: 0.6657401621341705, R2 Score (Test): 0.7264522791908836, RMSE (Test): 0.8203847408294678\n",
      "Epoch [1446/5000], Train Loss: 0.012412497152884802, R2 Score (Train): 0.9973253424641514, RMSE (Train): 0.09596777707338333\n",
      "Test Loss: 0.652408629655838, R2 Score (Test): 0.7317521353450042, RMSE (Test): 0.8123986721038818\n",
      "Epoch [1451/5000], Train Loss: 0.010382824577391148, R2 Score (Train): 0.9956220178953848, RMSE (Train): 0.10828883945941925\n",
      "Test Loss: 0.6673342883586884, R2 Score (Test): 0.725157421982594, RMSE (Test): 0.8223242163658142\n",
      "Epoch [1456/5000], Train Loss: 0.0121333214143912, R2 Score (Train): 0.9964873703003987, RMSE (Train): 0.0986754447221756\n",
      "Test Loss: 0.6810078024864197, R2 Score (Test): 0.7201083653655135, RMSE (Test): 0.8298431634902954\n",
      "Epoch [1461/5000], Train Loss: 0.014661365188658237, R2 Score (Train): 0.9977676310385449, RMSE (Train): 0.07947210967540741\n",
      "Test Loss: 0.6687015891075134, R2 Score (Test): 0.7255125394691697, RMSE (Test): 0.8217927813529968\n",
      "Epoch [1466/5000], Train Loss: 0.01098656483615438, R2 Score (Train): 0.9967345443978471, RMSE (Train): 0.08930139243602753\n",
      "Test Loss: 0.6443987190723419, R2 Score (Test): 0.733905331484529, RMSE (Test): 0.8091315627098083\n",
      "Epoch [1471/5000], Train Loss: 0.010845758020877838, R2 Score (Train): 0.9970757751106476, RMSE (Train): 0.09443444013595581\n",
      "Test Loss: 0.6601489186286926, R2 Score (Test): 0.7301318605657063, RMSE (Test): 0.8148484230041504\n",
      "Epoch [1476/5000], Train Loss: 0.008596036427964767, R2 Score (Train): 0.9970469686247222, RMSE (Train): 0.08869312703609467\n",
      "Test Loss: 0.6770650148391724, R2 Score (Test): 0.7253885814371529, RMSE (Test): 0.8219783306121826\n",
      "Epoch [1481/5000], Train Loss: 0.01318096819644173, R2 Score (Train): 0.997936179595999, RMSE (Train): 0.07958398759365082\n",
      "Test Loss: 0.6618151962757111, R2 Score (Test): 0.7317109278751575, RMSE (Test): 0.8124610185623169\n",
      "Epoch [1486/5000], Train Loss: 0.012895787910868725, R2 Score (Train): 0.9956386319231136, RMSE (Train): 0.10650788992643356\n",
      "Test Loss: 0.6800282895565033, R2 Score (Test): 0.7231688291927538, RMSE (Test): 0.8252936601638794\n",
      "Epoch [1491/5000], Train Loss: 0.012645152009402713, R2 Score (Train): 0.9959760455669393, RMSE (Train): 0.09814270585775375\n",
      "Test Loss: 0.6531492173671722, R2 Score (Test): 0.7337409814209361, RMSE (Test): 0.809381365776062\n",
      "Epoch [1496/5000], Train Loss: 0.014213690922285119, R2 Score (Train): 0.9953353041968283, RMSE (Train): 0.10985350608825684\n",
      "Test Loss: 0.6980793178081512, R2 Score (Test): 0.7127278864698896, RMSE (Test): 0.8407130241394043\n",
      "Epoch [1501/5000], Train Loss: 0.0116739043345054, R2 Score (Train): 0.997281330382621, RMSE (Train): 0.08800795674324036\n",
      "Test Loss: 0.6479596793651581, R2 Score (Test): 0.7325287122867268, RMSE (Test): 0.8112217783927917\n",
      "Epoch [1506/5000], Train Loss: 0.011667893423388401, R2 Score (Train): 0.9980802062528739, RMSE (Train): 0.07407032698392868\n",
      "Test Loss: 0.6566822230815887, R2 Score (Test): 0.7318972468105882, RMSE (Test): 0.8121788501739502\n",
      "Epoch [1511/5000], Train Loss: 0.010072341576839486, R2 Score (Train): 0.993379832299742, RMSE (Train): 0.1393149495124817\n",
      "Test Loss: 0.6552159786224365, R2 Score (Test): 0.7329059810994509, RMSE (Test): 0.8106495141983032\n",
      "Epoch [1516/5000], Train Loss: 0.009539175468186537, R2 Score (Train): 0.9965878026573461, RMSE (Train): 0.11161720007658005\n",
      "Test Loss: 0.6505095660686493, R2 Score (Test): 0.7332758743958036, RMSE (Test): 0.810088038444519\n",
      "Epoch [1521/5000], Train Loss: 0.010784244164824486, R2 Score (Train): 0.9957734666275522, RMSE (Train): 0.10404781997203827\n",
      "Test Loss: 0.6945331692695618, R2 Score (Test): 0.7190607260912159, RMSE (Test): 0.8313947319984436\n",
      "Epoch [1526/5000], Train Loss: 0.009687355564286312, R2 Score (Train): 0.9970533946252473, RMSE (Train): 0.09492676705121994\n",
      "Test Loss: 0.6783376336097717, R2 Score (Test): 0.7224276183498726, RMSE (Test): 0.8263978362083435\n",
      "Epoch [1531/5000], Train Loss: 0.010126973269507289, R2 Score (Train): 0.9978423956485859, RMSE (Train): 0.07736769318580627\n",
      "Test Loss: 0.6707432270050049, R2 Score (Test): 0.7305325031173753, RMSE (Test): 0.8142434358596802\n",
      "Epoch [1536/5000], Train Loss: 0.009990306726346413, R2 Score (Train): 0.996910211266074, RMSE (Train): 0.09009400755167007\n",
      "Test Loss: 0.6622224748134613, R2 Score (Test): 0.727928031044591, RMSE (Test): 0.8181688785552979\n",
      "Epoch [1541/5000], Train Loss: 0.011232523713260889, R2 Score (Train): 0.9971942667049715, RMSE (Train): 0.08962953090667725\n",
      "Test Loss: 0.650640994310379, R2 Score (Test): 0.7356602520456332, RMSE (Test): 0.8064588904380798\n",
      "Epoch [1546/5000], Train Loss: 0.01829211328489085, R2 Score (Train): 0.9914954736394811, RMSE (Train): 0.1413431465625763\n",
      "Test Loss: 0.6707295775413513, R2 Score (Test): 0.7277749239283873, RMSE (Test): 0.8183990120887756\n",
      "Epoch [1551/5000], Train Loss: 0.01201319337512056, R2 Score (Train): 0.9981812192348928, RMSE (Train): 0.0741206631064415\n",
      "Test Loss: 0.6514869332313538, R2 Score (Test): 0.7325701705785868, RMSE (Test): 0.8111589550971985\n",
      "Epoch [1556/5000], Train Loss: 0.009972400575255355, R2 Score (Train): 0.9961181412259776, RMSE (Train): 0.09786945581436157\n",
      "Test Loss: 0.6713410913944244, R2 Score (Test): 0.7263122200621678, RMSE (Test): 0.8205947875976562\n",
      "Epoch [1561/5000], Train Loss: 0.012085580344622334, R2 Score (Train): 0.9975542229779426, RMSE (Train): 0.08340767025947571\n",
      "Test Loss: 0.6682313978672028, R2 Score (Test): 0.7269920885977901, RMSE (Test): 0.8195749521255493\n",
      "Epoch [1566/5000], Train Loss: 0.008439568104222417, R2 Score (Train): 0.9961262290915265, RMSE (Train): 0.1028609424829483\n",
      "Test Loss: 0.657188355922699, R2 Score (Test): 0.7337286392708556, RMSE (Test): 0.8094001412391663\n",
      "Epoch [1571/5000], Train Loss: 0.01087179702396194, R2 Score (Train): 0.9961764619821486, RMSE (Train): 0.1023644357919693\n",
      "Test Loss: 0.6822129189968109, R2 Score (Test): 0.7238552896910964, RMSE (Test): 0.8242698311805725\n",
      "Epoch [1576/5000], Train Loss: 0.01086724145958821, R2 Score (Train): 0.9965011509227764, RMSE (Train): 0.09951789677143097\n",
      "Test Loss: 0.6660242974758148, R2 Score (Test): 0.729334214364123, RMSE (Test): 0.8160517811775208\n",
      "Epoch [1581/5000], Train Loss: 0.012287616729736328, R2 Score (Train): 0.9972567393154927, RMSE (Train): 0.09086446464061737\n",
      "Test Loss: 0.6551883220672607, R2 Score (Test): 0.7307784825855856, RMSE (Test): 0.8138716220855713\n",
      "Epoch [1586/5000], Train Loss: 0.01020835448677341, R2 Score (Train): 0.9978186115402156, RMSE (Train): 0.07771071791648865\n",
      "Test Loss: 0.6779817044734955, R2 Score (Test): 0.724346684864211, RMSE (Test): 0.8235360980033875\n",
      "Epoch [1591/5000], Train Loss: 0.010742807916055122, R2 Score (Train): 0.9964568510035677, RMSE (Train): 0.09809239953756332\n",
      "Test Loss: 0.6576854288578033, R2 Score (Test): 0.7316559053198493, RMSE (Test): 0.8125443458557129\n",
      "Epoch [1596/5000], Train Loss: 0.011659971593568722, R2 Score (Train): 0.9968521207158556, RMSE (Train): 0.0895671546459198\n",
      "Test Loss: 0.6776009798049927, R2 Score (Test): 0.7237549050957184, RMSE (Test): 0.8244196772575378\n",
      "Epoch [1601/5000], Train Loss: 0.009436552102367083, R2 Score (Train): 0.9962127599701657, RMSE (Train): 0.09429705888032913\n",
      "Test Loss: 0.6683104038238525, R2 Score (Test): 0.727518812713279, RMSE (Test): 0.8187839388847351\n",
      "Epoch [1606/5000], Train Loss: 0.015748586039990187, R2 Score (Train): 0.9971255017726536, RMSE (Train): 0.09146043658256531\n",
      "Test Loss: 0.657414585351944, R2 Score (Test): 0.7308189931798773, RMSE (Test): 0.8138104677200317\n",
      "Epoch [1611/5000], Train Loss: 0.0071442359282324714, R2 Score (Train): 0.9968103263360644, RMSE (Train): 0.08764633536338806\n",
      "Test Loss: 0.6776593327522278, R2 Score (Test): 0.7253891189526731, RMSE (Test): 0.8219774961471558\n",
      "Epoch [1616/5000], Train Loss: 0.011923841511209806, R2 Score (Train): 0.996544190315378, RMSE (Train): 0.10859179496765137\n",
      "Test Loss: 0.6750158071517944, R2 Score (Test): 0.7246642706009312, RMSE (Test): 0.8230615854263306\n",
      "Epoch [1621/5000], Train Loss: 0.009165737234676877, R2 Score (Train): 0.9980276066284272, RMSE (Train): 0.08293081074953079\n",
      "Test Loss: 0.656456708908081, R2 Score (Test): 0.7327543169675002, RMSE (Test): 0.810879647731781\n",
      "Epoch [1626/5000], Train Loss: 0.009415980040406188, R2 Score (Train): 0.9964174285502584, RMSE (Train): 0.09101349115371704\n",
      "Test Loss: 0.6652070879936218, R2 Score (Test): 0.723934743126375, RMSE (Test): 0.8241512775421143\n",
      "Epoch [1631/5000], Train Loss: 0.01090908826639255, R2 Score (Train): 0.9964391351482271, RMSE (Train): 0.08306030184030533\n",
      "Test Loss: 0.6356859505176544, R2 Score (Test): 0.7406974159431643, RMSE (Test): 0.7987381815910339\n",
      "Epoch [1636/5000], Train Loss: 0.00819456639389197, R2 Score (Train): 0.9978748460272217, RMSE (Train): 0.08245762437582016\n",
      "Test Loss: 0.664108008146286, R2 Score (Test): 0.7285304424873458, RMSE (Test): 0.8172625303268433\n",
      "Epoch [1641/5000], Train Loss: 0.010198901562641064, R2 Score (Train): 0.9979860969254911, RMSE (Train): 0.07815933972597122\n",
      "Test Loss: 0.6553037166595459, R2 Score (Test): 0.7335966301767751, RMSE (Test): 0.8096007704734802\n",
      "Epoch [1646/5000], Train Loss: 0.00975052205224832, R2 Score (Train): 0.9979618410813417, RMSE (Train): 0.08274918049573898\n",
      "Test Loss: 0.6702015101909637, R2 Score (Test): 0.7282588920077142, RMSE (Test): 0.8176711797714233\n",
      "Epoch [1651/5000], Train Loss: 0.010226934216916561, R2 Score (Train): 0.9970037698434654, RMSE (Train): 0.08822797983884811\n",
      "Test Loss: 0.6475522220134735, R2 Score (Test): 0.734772001247715, RMSE (Test): 0.8078127503395081\n",
      "Epoch [1656/5000], Train Loss: 0.013493895530700684, R2 Score (Train): 0.9941866522031901, RMSE (Train): 0.12292300164699554\n",
      "Test Loss: 0.6827122569084167, R2 Score (Test): 0.7247955343154293, RMSE (Test): 0.82286536693573\n",
      "Epoch [1661/5000], Train Loss: 0.010265978053212166, R2 Score (Train): 0.9969794195814071, RMSE (Train): 0.08838044106960297\n",
      "Test Loss: 0.6704366505146027, R2 Score (Test): 0.7306030769069336, RMSE (Test): 0.814136803150177\n",
      "Epoch [1666/5000], Train Loss: 0.008655175489063064, R2 Score (Train): 0.9980440611637401, RMSE (Train): 0.08355768769979477\n",
      "Test Loss: 0.6428089141845703, R2 Score (Test): 0.7405312228324135, RMSE (Test): 0.7989941835403442\n",
      "Epoch [1671/5000], Train Loss: 0.008121225594853362, R2 Score (Train): 0.9962069707085005, RMSE (Train): 0.10913126915693283\n",
      "Test Loss: 0.6517680883407593, R2 Score (Test): 0.7355003486119045, RMSE (Test): 0.8067028522491455\n",
      "Epoch [1676/5000], Train Loss: 0.010947785262639323, R2 Score (Train): 0.9986713150950032, RMSE (Train): 0.06039859354496002\n",
      "Test Loss: 0.6392171680927277, R2 Score (Test): 0.7410159148303259, RMSE (Test): 0.7982475161552429\n",
      "Epoch [1681/5000], Train Loss: 0.008451899668822685, R2 Score (Train): 0.9976926300196033, RMSE (Train): 0.08610229194164276\n",
      "Test Loss: 0.6517466902732849, R2 Score (Test): 0.7360998893035655, RMSE (Test): 0.8057880997657776\n",
      "Epoch [1686/5000], Train Loss: 0.008994865774487456, R2 Score (Train): 0.9981172184581205, RMSE (Train): 0.07507248967885971\n",
      "Test Loss: 0.6459672451019287, R2 Score (Test): 0.7383977687965935, RMSE (Test): 0.8022722601890564\n",
      "Epoch [1691/5000], Train Loss: 0.0074528486002236605, R2 Score (Train): 0.9978268643547185, RMSE (Train): 0.07770197838544846\n",
      "Test Loss: 0.659500777721405, R2 Score (Test): 0.7342878468756221, RMSE (Test): 0.8085497617721558\n",
      "Epoch [1696/5000], Train Loss: 0.008517860745390257, R2 Score (Train): 0.9971845733691501, RMSE (Train): 0.08032196760177612\n",
      "Test Loss: 0.6474450528621674, R2 Score (Test): 0.7386359185944247, RMSE (Test): 0.801906943321228\n",
      "Epoch [1701/5000], Train Loss: 0.008234575701256594, R2 Score (Train): 0.9973371851822828, RMSE (Train): 0.0894940197467804\n",
      "Test Loss: 0.6670778691768646, R2 Score (Test): 0.7301758266887437, RMSE (Test): 0.8147820830345154\n",
      "Epoch [1706/5000], Train Loss: 0.01048244241004189, R2 Score (Train): 0.9967166624976023, RMSE (Train): 0.09786593168973923\n",
      "Test Loss: 0.6726030111312866, R2 Score (Test): 0.7253171019575086, RMSE (Test): 0.8220852613449097\n",
      "Epoch [1711/5000], Train Loss: 0.010643713952352604, R2 Score (Train): 0.9969787263638923, RMSE (Train): 0.08775099366903305\n",
      "Test Loss: 0.6541827917098999, R2 Score (Test): 0.737476161810564, RMSE (Test): 0.8036841750144958\n",
      "Epoch [1716/5000], Train Loss: 0.009962224867194891, R2 Score (Train): 0.9986115556971609, RMSE (Train): 0.06570474058389664\n",
      "Test Loss: 0.6740519106388092, R2 Score (Test): 0.7284032891663657, RMSE (Test): 0.817453920841217\n",
      "Epoch [1721/5000], Train Loss: 0.008343616966158152, R2 Score (Train): 0.9970588924420383, RMSE (Train): 0.08918436616659164\n",
      "Test Loss: 0.6615109741687775, R2 Score (Test): 0.7323127695155907, RMSE (Test): 0.8115492463111877\n",
      "Epoch [1726/5000], Train Loss: 0.012124511878937483, R2 Score (Train): 0.9948642176426452, RMSE (Train): 0.11887935549020767\n",
      "Test Loss: 0.6741913557052612, R2 Score (Test): 0.7263569315228313, RMSE (Test): 0.8205277323722839\n",
      "Epoch [1731/5000], Train Loss: 0.014156752110769352, R2 Score (Train): 0.9972044232047624, RMSE (Train): 0.08266425877809525\n",
      "Test Loss: 0.6681614816188812, R2 Score (Test): 0.7281194062661049, RMSE (Test): 0.8178811073303223\n",
      "Epoch [1736/5000], Train Loss: 0.013761195509384075, R2 Score (Train): 0.9954375489184104, RMSE (Train): 0.11223422735929489\n",
      "Test Loss: 0.6996809542179108, R2 Score (Test): 0.7173391421633823, RMSE (Test): 0.8339382410049438\n",
      "Epoch [1741/5000], Train Loss: 0.008342330964903036, R2 Score (Train): 0.9980452551282806, RMSE (Train): 0.07234497368335724\n",
      "Test Loss: 0.6650715470314026, R2 Score (Test): 0.7308241650446377, RMSE (Test): 0.8138025999069214\n",
      "Epoch [1746/5000], Train Loss: 0.010596300242468715, R2 Score (Train): 0.9966135833285834, RMSE (Train): 0.08797816187143326\n",
      "Test Loss: 0.6503293216228485, R2 Score (Test): 0.7361262013219403, RMSE (Test): 0.805747926235199\n",
      "Epoch [1751/5000], Train Loss: 0.014648716275890669, R2 Score (Train): 0.9957159244669693, RMSE (Train): 0.10615650564432144\n",
      "Test Loss: 0.6623534262180328, R2 Score (Test): 0.7291279756040037, RMSE (Test): 0.8163626194000244\n",
      "Epoch [1756/5000], Train Loss: 0.017558428769310314, R2 Score (Train): 0.994827598933396, RMSE (Train): 0.12827330827713013\n",
      "Test Loss: 0.6432183086872101, R2 Score (Test): 0.7410471049868235, RMSE (Test): 0.798199474811554\n",
      "Epoch [1761/5000], Train Loss: 0.010519894460837046, R2 Score (Train): 0.9943971790434822, RMSE (Train): 0.12879560887813568\n",
      "Test Loss: 0.6460315585136414, R2 Score (Test): 0.7408909786216323, RMSE (Test): 0.7984400987625122\n",
      "Epoch [1766/5000], Train Loss: 0.009982078801840544, R2 Score (Train): 0.9972859493616487, RMSE (Train): 0.08871690183877945\n",
      "Test Loss: 0.6650502979755402, R2 Score (Test): 0.7290774736035066, RMSE (Test): 0.8164387941360474\n",
      "Epoch [1771/5000], Train Loss: 0.013161583105102181, R2 Score (Train): 0.998177676037091, RMSE (Train): 0.07937462627887726\n",
      "Test Loss: 0.6591700315475464, R2 Score (Test): 0.7319068048927588, RMSE (Test): 0.8121644258499146\n",
      "Epoch [1776/5000], Train Loss: 0.013913343194872141, R2 Score (Train): 0.9971902528996075, RMSE (Train): 0.09789775311946869\n",
      "Test Loss: 0.6649497449398041, R2 Score (Test): 0.7319151290663795, RMSE (Test): 0.8121517896652222\n",
      "Epoch [1781/5000], Train Loss: 0.013142702945818504, R2 Score (Train): 0.9957008048285536, RMSE (Train): 0.09937234222888947\n",
      "Test Loss: 0.6535674929618835, R2 Score (Test): 0.73875476285303, RMSE (Test): 0.8017246723175049\n",
      "Epoch [1786/5000], Train Loss: 0.011160330614075065, R2 Score (Train): 0.9961759263314526, RMSE (Train): 0.09384432435035706\n",
      "Test Loss: 0.6457500457763672, R2 Score (Test): 0.7364291695669478, RMSE (Test): 0.8052851557731628\n",
      "Epoch [1791/5000], Train Loss: 0.008535975745568672, R2 Score (Train): 0.9972635497007369, RMSE (Train): 0.09422221034765244\n",
      "Test Loss: 0.6631773412227631, R2 Score (Test): 0.7303196532603602, RMSE (Test): 0.8145648837089539\n",
      "Epoch [1796/5000], Train Loss: 0.011963204480707645, R2 Score (Train): 0.9972632640722391, RMSE (Train): 0.08190741389989853\n",
      "Test Loss: 0.6496129930019379, R2 Score (Test): 0.7374017394663204, RMSE (Test): 0.8037980794906616\n",
      "Epoch [1801/5000], Train Loss: 0.014023654783765474, R2 Score (Train): 0.996799506813947, RMSE (Train): 0.0879369005560875\n",
      "Test Loss: 0.6466151177883148, R2 Score (Test): 0.7370201248859762, RMSE (Test): 0.8043819069862366\n",
      "Epoch [1806/5000], Train Loss: 0.008994985527048508, R2 Score (Train): 0.9957599590748301, RMSE (Train): 0.10183262079954147\n",
      "Test Loss: 0.6504351198673248, R2 Score (Test): 0.7358571723262628, RMSE (Test): 0.8061585426330566\n",
      "Epoch [1811/5000], Train Loss: 0.00784681411460042, R2 Score (Train): 0.9972619583608228, RMSE (Train): 0.09229704737663269\n",
      "Test Loss: 0.6520331501960754, R2 Score (Test): 0.7348186709271002, RMSE (Test): 0.8077417016029358\n",
      "Epoch [1816/5000], Train Loss: 0.00823842128738761, R2 Score (Train): 0.9976927009839989, RMSE (Train): 0.08288808912038803\n",
      "Test Loss: 0.6369029879570007, R2 Score (Test): 0.7396414569407128, RMSE (Test): 0.8003629446029663\n",
      "Epoch [1821/5000], Train Loss: 0.011210486913720766, R2 Score (Train): 0.9965488653283153, RMSE (Train): 0.09925597161054611\n",
      "Test Loss: 0.6552237570285797, R2 Score (Test): 0.7353895966742362, RMSE (Test): 0.806871771812439\n",
      "Epoch [1826/5000], Train Loss: 0.01217489557651182, R2 Score (Train): 0.995966473746051, RMSE (Train): 0.10265398025512695\n",
      "Test Loss: 0.6679926812648773, R2 Score (Test): 0.7275741656476196, RMSE (Test): 0.8187007308006287\n",
      "Epoch [1831/5000], Train Loss: 0.00796553997012476, R2 Score (Train): 0.9944940808184688, RMSE (Train): 0.09958113729953766\n",
      "Test Loss: 0.6444052755832672, R2 Score (Test): 0.737082293077586, RMSE (Test): 0.8042868375778198\n",
      "Epoch [1836/5000], Train Loss: 0.01234983125080665, R2 Score (Train): 0.9987178741948217, RMSE (Train): 0.06047312915325165\n",
      "Test Loss: 0.6520207822322845, R2 Score (Test): 0.735171814212252, RMSE (Test): 0.8072037100791931\n",
      "Epoch [1841/5000], Train Loss: 0.01209203510855635, R2 Score (Train): 0.9981336980753641, RMSE (Train): 0.07223722338676453\n",
      "Test Loss: 0.6755882203578949, R2 Score (Test): 0.7307243076616652, RMSE (Test): 0.8139535188674927\n",
      "Epoch [1846/5000], Train Loss: 0.012493038550019264, R2 Score (Train): 0.9984718493928947, RMSE (Train): 0.0685092955827713\n",
      "Test Loss: 0.6310304701328278, R2 Score (Test): 0.7416654948477301, RMSE (Test): 0.7972458004951477\n",
      "Epoch [1851/5000], Train Loss: 0.011276629753410816, R2 Score (Train): 0.996273462436846, RMSE (Train): 0.10383985191583633\n",
      "Test Loss: 0.6568434834480286, R2 Score (Test): 0.7330239314506, RMSE (Test): 0.8104705214500427\n",
      "Epoch [1856/5000], Train Loss: 0.009100530917445818, R2 Score (Train): 0.9964609104925288, RMSE (Train): 0.09575854986906052\n",
      "Test Loss: 0.6367648839950562, R2 Score (Test): 0.7385543805140542, RMSE (Test): 0.8020319938659668\n",
      "Epoch [1861/5000], Train Loss: 0.007786993791038792, R2 Score (Train): 0.9971773369983201, RMSE (Train): 0.08825502544641495\n",
      "Test Loss: 0.6508001387119293, R2 Score (Test): 0.7345565947857298, RMSE (Test): 0.808140754699707\n",
      "Epoch [1866/5000], Train Loss: 0.008920932111019889, R2 Score (Train): 0.9977544308758935, RMSE (Train): 0.08212581276893616\n",
      "Test Loss: 0.628544420003891, R2 Score (Test): 0.7387244198423383, RMSE (Test): 0.8017712235450745\n",
      "Epoch [1871/5000], Train Loss: 0.0104880693834275, R2 Score (Train): 0.9944839303070802, RMSE (Train): 0.12309275567531586\n",
      "Test Loss: 0.6433500051498413, R2 Score (Test): 0.7377756521143014, RMSE (Test): 0.8032256364822388\n",
      "Epoch [1876/5000], Train Loss: 0.010823939771701893, R2 Score (Train): 0.9970952658969432, RMSE (Train): 0.09300386905670166\n",
      "Test Loss: 0.6542258560657501, R2 Score (Test): 0.7331526583077363, RMSE (Test): 0.8102750778198242\n",
      "Epoch [1881/5000], Train Loss: 0.013245166434595982, R2 Score (Train): 0.9977659732571579, RMSE (Train): 0.0899052694439888\n",
      "Test Loss: 0.6138246059417725, R2 Score (Test): 0.7483189878587638, RMSE (Test): 0.786912202835083\n",
      "Epoch [1886/5000], Train Loss: 0.011138184306522211, R2 Score (Train): 0.9968291722029162, RMSE (Train): 0.09773992002010345\n",
      "Test Loss: 0.6506544053554535, R2 Score (Test): 0.7377763076745516, RMSE (Test): 0.8032246232032776\n",
      "Epoch [1891/5000], Train Loss: 0.010342620934049288, R2 Score (Train): 0.997944959865821, RMSE (Train): 0.07134965807199478\n",
      "Test Loss: 0.6587303578853607, R2 Score (Test): 0.7304332554542146, RMSE (Test): 0.8143932819366455\n",
      "Epoch [1896/5000], Train Loss: 0.011639931549628576, R2 Score (Train): 0.9918693429178014, RMSE (Train): 0.13882143795490265\n",
      "Test Loss: 0.6675722002983093, R2 Score (Test): 0.7285551028317039, RMSE (Test): 0.817225456237793\n",
      "Epoch [1901/5000], Train Loss: 0.010695808567106724, R2 Score (Train): 0.9959418801448051, RMSE (Train): 0.10336051136255264\n",
      "Test Loss: 0.6474688649177551, R2 Score (Test): 0.7357672222776188, RMSE (Test): 0.8062957525253296\n",
      "Epoch [1906/5000], Train Loss: 0.011006565609325966, R2 Score (Train): 0.9981260010876135, RMSE (Train): 0.08052055537700653\n",
      "Test Loss: 0.6532642245292664, R2 Score (Test): 0.732888145592091, RMSE (Test): 0.8106765747070312\n",
      "Epoch [1911/5000], Train Loss: 0.010082109443222484, R2 Score (Train): 0.9943575895710937, RMSE (Train): 0.11719389259815216\n",
      "Test Loss: 0.6292409002780914, R2 Score (Test): 0.7415642485787882, RMSE (Test): 0.797402024269104\n",
      "Epoch [1916/5000], Train Loss: 0.01128728703285257, R2 Score (Train): 0.9959664972544388, RMSE (Train): 0.11975352466106415\n",
      "Test Loss: 0.6717053353786469, R2 Score (Test): 0.730536628048515, RMSE (Test): 0.8142371773719788\n",
      "Epoch [1921/5000], Train Loss: 0.00596560138122489, R2 Score (Train): 0.9957690667988357, RMSE (Train): 0.09664623439311981\n",
      "Test Loss: 0.6342400014400482, R2 Score (Test): 0.7421589622112494, RMSE (Test): 0.7964839935302734\n",
      "Epoch [1926/5000], Train Loss: 0.008102480477343002, R2 Score (Train): 0.9979760537758272, RMSE (Train): 0.0736994668841362\n",
      "Test Loss: 0.6416669487953186, R2 Score (Test): 0.7394572579696652, RMSE (Test): 0.8006459474563599\n",
      "Epoch [1931/5000], Train Loss: 0.0069344879205649095, R2 Score (Train): 0.9976752868714256, RMSE (Train): 0.0774812325835228\n",
      "Test Loss: 0.6526462137699127, R2 Score (Test): 0.733467615270198, RMSE (Test): 0.8097968101501465\n",
      "Epoch [1936/5000], Train Loss: 0.006781462890406449, R2 Score (Train): 0.9965814571440017, RMSE (Train): 0.09204056859016418\n",
      "Test Loss: 0.6611806452274323, R2 Score (Test): 0.7292672515658702, RMSE (Test): 0.8161526918411255\n",
      "Epoch [1941/5000], Train Loss: 0.008637087885290384, R2 Score (Train): 0.9960842086304469, RMSE (Train): 0.09240623563528061\n",
      "Test Loss: 0.6734205484390259, R2 Score (Test): 0.7268012004531259, RMSE (Test): 0.8198614120483398\n",
      "Epoch [1946/5000], Train Loss: 0.010851797570164004, R2 Score (Train): 0.9981783184701568, RMSE (Train): 0.07226192951202393\n",
      "Test Loss: 0.6487879157066345, R2 Score (Test): 0.7372708746064272, RMSE (Test): 0.8039983510971069\n",
      "Epoch [1951/5000], Train Loss: 0.009799958051492771, R2 Score (Train): 0.9964503015246571, RMSE (Train): 0.10409647226333618\n",
      "Test Loss: 0.6519691050052643, R2 Score (Test): 0.7347640169170875, RMSE (Test): 0.8078249096870422\n",
      "Epoch [1956/5000], Train Loss: 0.007135007297620177, R2 Score (Train): 0.9953229127154187, RMSE (Train): 0.10999064892530441\n",
      "Test Loss: 0.640235036611557, R2 Score (Test): 0.7407026695883426, RMSE (Test): 0.7987301349639893\n",
      "Epoch [1961/5000], Train Loss: 0.009519432671368122, R2 Score (Train): 0.9971301444992978, RMSE (Train): 0.07896088808774948\n",
      "Test Loss: 0.6769035160541534, R2 Score (Test): 0.7288097712008108, RMSE (Test): 0.8168420195579529\n",
      "Epoch [1966/5000], Train Loss: 0.012101952296992144, R2 Score (Train): 0.9954246436940202, RMSE (Train): 0.11337706446647644\n",
      "Test Loss: 0.6518446803092957, R2 Score (Test): 0.7340684414270164, RMSE (Test): 0.8088834881782532\n",
      "Epoch [1971/5000], Train Loss: 0.010017722146585584, R2 Score (Train): 0.9979569670886743, RMSE (Train): 0.07137402892112732\n",
      "Test Loss: 0.6654142439365387, R2 Score (Test): 0.7332979051559038, RMSE (Test): 0.8100544810295105\n",
      "Epoch [1976/5000], Train Loss: 0.011339384286353985, R2 Score (Train): 0.9970020189553321, RMSE (Train): 0.0843428373336792\n",
      "Test Loss: 0.6603236496448517, R2 Score (Test): 0.7320054870726312, RMSE (Test): 0.8120149374008179\n",
      "Epoch [1981/5000], Train Loss: 0.011662785895168781, R2 Score (Train): 0.9937928828882642, RMSE (Train): 0.12853242456912994\n",
      "Test Loss: 0.661952942609787, R2 Score (Test): 0.7334696146953814, RMSE (Test): 0.8097937107086182\n",
      "Epoch [1986/5000], Train Loss: 0.009517084419106444, R2 Score (Train): 0.998146412041046, RMSE (Train): 0.07513236999511719\n",
      "Test Loss: 0.6523046791553497, R2 Score (Test): 0.7321142929061146, RMSE (Test): 0.8118500113487244\n",
      "Epoch [1991/5000], Train Loss: 0.013250111602246761, R2 Score (Train): 0.9950659324829176, RMSE (Train): 0.12224513292312622\n",
      "Test Loss: 0.652447909116745, R2 Score (Test): 0.7345683135467781, RMSE (Test): 0.8081229329109192\n",
      "Epoch [1996/5000], Train Loss: 0.01044439454562962, R2 Score (Train): 0.9980491680555169, RMSE (Train): 0.07293431460857391\n",
      "Test Loss: 0.6490511298179626, R2 Score (Test): 0.736076285524989, RMSE (Test): 0.8058240413665771\n",
      "Epoch [2001/5000], Train Loss: 0.009917711528638998, R2 Score (Train): 0.995265075200002, RMSE (Train): 0.11252103000879288\n",
      "Test Loss: 0.6620773673057556, R2 Score (Test): 0.7303904722751244, RMSE (Test): 0.8144579529762268\n",
      "Epoch [2006/5000], Train Loss: 0.008675752207636833, R2 Score (Train): 0.997025052260698, RMSE (Train): 0.08563415706157684\n",
      "Test Loss: 0.622833639383316, R2 Score (Test): 0.745662400116488, RMSE (Test): 0.7910543084144592\n",
      "Epoch [2011/5000], Train Loss: 0.009389161636742452, R2 Score (Train): 0.9967548194011115, RMSE (Train): 0.09397393465042114\n",
      "Test Loss: 0.6482156217098236, R2 Score (Test): 0.7349331619942441, RMSE (Test): 0.8075673580169678\n",
      "Epoch [2016/5000], Train Loss: 0.007444395528485377, R2 Score (Train): 0.9964359065752827, RMSE (Train): 0.10198788344860077\n",
      "Test Loss: 0.638248085975647, R2 Score (Test): 0.7398484711775096, RMSE (Test): 0.8000446557998657\n",
      "Epoch [2021/5000], Train Loss: 0.007041883654892445, R2 Score (Train): 0.9963920739490679, RMSE (Train): 0.08830571919679642\n",
      "Test Loss: 0.6540873348712921, R2 Score (Test): 0.7334954205145088, RMSE (Test): 0.8097545504570007\n",
      "Epoch [2026/5000], Train Loss: 0.006326501180107395, R2 Score (Train): 0.9977031684972393, RMSE (Train): 0.08398409187793732\n",
      "Test Loss: 0.6539318263530731, R2 Score (Test): 0.7360339474014189, RMSE (Test): 0.8058887720108032\n",
      "Epoch [2031/5000], Train Loss: 0.009452528087422252, R2 Score (Train): 0.9960586025685434, RMSE (Train): 0.10652188211679459\n",
      "Test Loss: 0.624643087387085, R2 Score (Test): 0.7453463653404176, RMSE (Test): 0.7915456891059875\n",
      "Epoch [2036/5000], Train Loss: 0.009621358864630261, R2 Score (Train): 0.9974160539944037, RMSE (Train): 0.0838179811835289\n",
      "Test Loss: 0.651902586221695, R2 Score (Test): 0.7317969906282629, RMSE (Test): 0.8123307228088379\n",
      "Epoch [2041/5000], Train Loss: 0.011215765960514545, R2 Score (Train): 0.9973366305863787, RMSE (Train): 0.09068107604980469\n",
      "Test Loss: 0.6281230449676514, R2 Score (Test): 0.7459802208612678, RMSE (Test): 0.7905599474906921\n",
      "Epoch [2046/5000], Train Loss: 0.006536007315541307, R2 Score (Train): 0.9972180078144225, RMSE (Train): 0.08382463455200195\n",
      "Test Loss: 0.6531584560871124, R2 Score (Test): 0.7338374101582934, RMSE (Test): 0.8092347979545593\n",
      "Epoch [2051/5000], Train Loss: 0.009141326881945133, R2 Score (Train): 0.9979466417870486, RMSE (Train): 0.08260044455528259\n",
      "Test Loss: 0.6397881507873535, R2 Score (Test): 0.7424935304944105, RMSE (Test): 0.795967161655426\n",
      "Epoch [2056/5000], Train Loss: 0.008358231357609233, R2 Score (Train): 0.9974967377806727, RMSE (Train): 0.07523317635059357\n",
      "Test Loss: 0.6558040380477905, R2 Score (Test): 0.7334966321137815, RMSE (Test): 0.8097526431083679\n",
      "Epoch [2061/5000], Train Loss: 0.007605526829138398, R2 Score (Train): 0.9974055586872503, RMSE (Train): 0.0884934812784195\n",
      "Test Loss: 0.6500012874603271, R2 Score (Test): 0.7345073868104655, RMSE (Test): 0.8082156777381897\n",
      "Epoch [2066/5000], Train Loss: 0.007735420484095812, R2 Score (Train): 0.9981598730570089, RMSE (Train): 0.06961710751056671\n",
      "Test Loss: 0.6451098620891571, R2 Score (Test): 0.7393345068926118, RMSE (Test): 0.800834596157074\n",
      "Epoch [2071/5000], Train Loss: 0.009824297080437342, R2 Score (Train): 0.9973397886170365, RMSE (Train): 0.09028154611587524\n",
      "Test Loss: 0.6357889175415039, R2 Score (Test): 0.743281737729174, RMSE (Test): 0.7947479486465454\n",
      "Epoch [2076/5000], Train Loss: 0.007642753267039855, R2 Score (Train): 0.998895938499052, RMSE (Train): 0.060567717999219894\n",
      "Test Loss: 0.6569632589817047, R2 Score (Test): 0.7338970003943888, RMSE (Test): 0.8091441988945007\n",
      "Epoch [2081/5000], Train Loss: 0.011396062094718218, R2 Score (Train): 0.9972559267363742, RMSE (Train): 0.09146466851234436\n",
      "Test Loss: 0.650968611240387, R2 Score (Test): 0.7377689584574306, RMSE (Test): 0.8032358288764954\n",
      "Epoch [2086/5000], Train Loss: 0.010426670468101898, R2 Score (Train): 0.998483301600764, RMSE (Train): 0.0752423033118248\n",
      "Test Loss: 0.6558702886104584, R2 Score (Test): 0.7351737728720482, RMSE (Test): 0.8072007298469543\n",
      "Epoch [2091/5000], Train Loss: 0.013217315388222536, R2 Score (Train): 0.9938130826604744, RMSE (Train): 0.13043971359729767\n",
      "Test Loss: 0.6400640904903412, R2 Score (Test): 0.7398274993440559, RMSE (Test): 0.8000769019126892\n",
      "Epoch [2096/5000], Train Loss: 0.011958370994155606, R2 Score (Train): 0.9968244596951125, RMSE (Train): 0.09395507723093033\n",
      "Test Loss: 0.6783363819122314, R2 Score (Test): 0.7259172091654909, RMSE (Test): 0.8211867809295654\n",
      "Epoch [2101/5000], Train Loss: 0.010160214267671108, R2 Score (Train): 0.9976834493835591, RMSE (Train): 0.08403487503528595\n",
      "Test Loss: 0.6640745401382446, R2 Score (Test): 0.7288182412235156, RMSE (Test): 0.816829264163971\n",
      "Epoch [2106/5000], Train Loss: 0.011662111772845188, R2 Score (Train): 0.9955302019147826, RMSE (Train): 0.10832372307777405\n",
      "Test Loss: 0.654301255941391, R2 Score (Test): 0.7338908085368551, RMSE (Test): 0.80915367603302\n",
      "Epoch [2111/5000], Train Loss: 0.00936361487644414, R2 Score (Train): 0.9982940841172712, RMSE (Train): 0.06439701467752457\n",
      "Test Loss: 0.6626116037368774, R2 Score (Test): 0.7335982541568793, RMSE (Test): 0.8095982670783997\n",
      "Epoch [2116/5000], Train Loss: 0.009607213083654642, R2 Score (Train): 0.9962592960230576, RMSE (Train): 0.097514308989048\n",
      "Test Loss: 0.6515168249607086, R2 Score (Test): 0.7363654817480083, RMSE (Test): 0.8053824305534363\n",
      "Epoch [2121/5000], Train Loss: 0.012580237351357937, R2 Score (Train): 0.9972401736537668, RMSE (Train): 0.08303001523017883\n",
      "Test Loss: 0.667534351348877, R2 Score (Test): 0.7299632539589473, RMSE (Test): 0.8151029348373413\n",
      "Epoch [2126/5000], Train Loss: 0.007834770794336995, R2 Score (Train): 0.9957042613823057, RMSE (Train): 0.10689049959182739\n",
      "Test Loss: 0.6444035768508911, R2 Score (Test): 0.7366296704507669, RMSE (Test): 0.8049789071083069\n",
      "Epoch [2131/5000], Train Loss: 0.008904801681637764, R2 Score (Train): 0.9973617342431437, RMSE (Train): 0.07545392960309982\n",
      "Test Loss: 0.672565370798111, R2 Score (Test): 0.7260082219208086, RMSE (Test): 0.8210504055023193\n",
      "Epoch [2136/5000], Train Loss: 0.008540526886160174, R2 Score (Train): 0.9971026428330642, RMSE (Train): 0.08228272199630737\n",
      "Test Loss: 0.6727089881896973, R2 Score (Test): 0.7274520752189206, RMSE (Test): 0.8188842535018921\n",
      "Epoch [2141/5000], Train Loss: 0.008427182445302606, R2 Score (Train): 0.9967397391689613, RMSE (Train): 0.09211241453886032\n",
      "Test Loss: 0.6691131293773651, R2 Score (Test): 0.7291424519415677, RMSE (Test): 0.8163408041000366\n",
      "Epoch [2146/5000], Train Loss: 0.007721233181655407, R2 Score (Train): 0.9956505628424187, RMSE (Train): 0.11108626425266266\n",
      "Test Loss: 0.634686678647995, R2 Score (Test): 0.7424762342741053, RMSE (Test): 0.7959938049316406\n",
      "Epoch [2151/5000], Train Loss: 0.008999849203974009, R2 Score (Train): 0.9980898572892706, RMSE (Train): 0.06956139951944351\n",
      "Test Loss: 0.6527941226959229, R2 Score (Test): 0.737523995494439, RMSE (Test): 0.8036109805107117\n",
      "Epoch [2156/5000], Train Loss: 0.00936000708801051, R2 Score (Train): 0.9961310308346103, RMSE (Train): 0.10070358961820602\n",
      "Test Loss: 0.6387180387973785, R2 Score (Test): 0.7403194607336101, RMSE (Test): 0.7993201017379761\n",
      "Epoch [2161/5000], Train Loss: 0.0077508372875551386, R2 Score (Train): 0.9985822828975169, RMSE (Train): 0.06626905500888824\n",
      "Test Loss: 0.6607085466384888, R2 Score (Test): 0.7366014520347022, RMSE (Test): 0.8050219416618347\n",
      "Epoch [2166/5000], Train Loss: 0.009227332115794221, R2 Score (Train): 0.9959268413600199, RMSE (Train): 0.11065585166215897\n",
      "Test Loss: 0.6629423499107361, R2 Score (Test): 0.7323077862846297, RMSE (Test): 0.8115568161010742\n",
      "Epoch [2171/5000], Train Loss: 0.012049742508679628, R2 Score (Train): 0.996837735342061, RMSE (Train): 0.09478939324617386\n",
      "Test Loss: 0.6539947986602783, R2 Score (Test): 0.7359695076520847, RMSE (Test): 0.8059870600700378\n",
      "Epoch [2176/5000], Train Loss: 0.007591267349198461, R2 Score (Train): 0.9971421881794678, RMSE (Train): 0.07455889880657196\n",
      "Test Loss: 0.6660220921039581, R2 Score (Test): 0.7337655693087555, RMSE (Test): 0.8093439936637878\n",
      "Epoch [2181/5000], Train Loss: 0.00751936676291128, R2 Score (Train): 0.9955127247836134, RMSE (Train): 0.12094753980636597\n",
      "Test Loss: 0.6583754420280457, R2 Score (Test): 0.7356974130092664, RMSE (Test): 0.8064022660255432\n",
      "Epoch [2186/5000], Train Loss: 0.008896433205033341, R2 Score (Train): 0.9987124372547377, RMSE (Train): 0.05913574621081352\n",
      "Test Loss: 0.6518276631832123, R2 Score (Test): 0.7360014388478415, RMSE (Test): 0.8059383630752563\n",
      "Epoch [2191/5000], Train Loss: 0.006596091901883483, R2 Score (Train): 0.9977313410021353, RMSE (Train): 0.0916859433054924\n",
      "Test Loss: 0.6324413418769836, R2 Score (Test): 0.7442996496893555, RMSE (Test): 0.7931707501411438\n",
      "Epoch [2196/5000], Train Loss: 0.008670541302611431, R2 Score (Train): 0.998825989045556, RMSE (Train): 0.05844593420624733\n",
      "Test Loss: 0.6644958853721619, R2 Score (Test): 0.7298754465772879, RMSE (Test): 0.815235435962677\n",
      "Epoch [2201/5000], Train Loss: 0.00906966533511877, R2 Score (Train): 0.9968674441452118, RMSE (Train): 0.08658052235841751\n",
      "Test Loss: 0.6531792879104614, R2 Score (Test): 0.7359411461979102, RMSE (Test): 0.8060303330421448\n",
      "Epoch [2206/5000], Train Loss: 0.008558762337391576, R2 Score (Train): 0.9973102667688526, RMSE (Train): 0.08551948517560959\n",
      "Test Loss: 0.6611239612102509, R2 Score (Test): 0.7347018629913626, RMSE (Test): 0.8079196214675903\n",
      "Epoch [2211/5000], Train Loss: 0.008430213124180833, R2 Score (Train): 0.9941773693997704, RMSE (Train): 0.10837775468826294\n",
      "Test Loss: 0.6623788774013519, R2 Score (Test): 0.732194612295632, RMSE (Test): 0.8117283582687378\n",
      "Epoch [2216/5000], Train Loss: 0.008421868085861206, R2 Score (Train): 0.9973519164988628, RMSE (Train): 0.08398889750242233\n",
      "Test Loss: 0.6588246822357178, R2 Score (Test): 0.7355434020239509, RMSE (Test): 0.8066372275352478\n",
      "Epoch [2221/5000], Train Loss: 0.009087651036679745, R2 Score (Train): 0.9978932494976134, RMSE (Train): 0.07724887132644653\n",
      "Test Loss: 0.6550399363040924, R2 Score (Test): 0.7357205069170119, RMSE (Test): 0.806367039680481\n",
      "Epoch [2226/5000], Train Loss: 0.00821602949872613, R2 Score (Train): 0.9979376378674178, RMSE (Train): 0.0735972672700882\n",
      "Test Loss: 0.6654162704944611, R2 Score (Test): 0.7335386313369076, RMSE (Test): 0.8096888661384583\n",
      "Epoch [2231/5000], Train Loss: 0.00877883161107699, R2 Score (Train): 0.9972960906775922, RMSE (Train): 0.08504205197095871\n",
      "Test Loss: 0.6577156782150269, R2 Score (Test): 0.7335668883176236, RMSE (Test): 0.80964595079422\n",
      "Epoch [2236/5000], Train Loss: 0.013433885605384907, R2 Score (Train): 0.9956153899518513, RMSE (Train): 0.10561858117580414\n",
      "Test Loss: 0.6599111258983612, R2 Score (Test): 0.7319631528390249, RMSE (Test): 0.8120790719985962\n",
      "Epoch [2241/5000], Train Loss: 0.012111534519741932, R2 Score (Train): 0.9973358217862377, RMSE (Train): 0.07627352327108383\n",
      "Test Loss: 0.6572623550891876, R2 Score (Test): 0.7338206650986856, RMSE (Test): 0.8092603087425232\n",
      "Epoch [2246/5000], Train Loss: 0.007807174464687705, R2 Score (Train): 0.9983548395664386, RMSE (Train): 0.06300707906484604\n",
      "Test Loss: 0.6523710191249847, R2 Score (Test): 0.7354601737703306, RMSE (Test): 0.8067641854286194\n",
      "Epoch [2251/5000], Train Loss: 0.009605475313340625, R2 Score (Train): 0.9978893177357159, RMSE (Train): 0.0758143961429596\n",
      "Test Loss: 0.6429549157619476, R2 Score (Test): 0.7378070578283017, RMSE (Test): 0.8031774759292603\n",
      "Epoch [2256/5000], Train Loss: 0.00749258875536422, R2 Score (Train): 0.9972251392771572, RMSE (Train): 0.08918952196836472\n",
      "Test Loss: 0.638203352689743, R2 Score (Test): 0.7386221297813076, RMSE (Test): 0.8019281625747681\n",
      "Epoch [2261/5000], Train Loss: 0.008848382780949274, R2 Score (Train): 0.9976961530603877, RMSE (Train): 0.07542186230421066\n",
      "Test Loss: 0.6673881113529205, R2 Score (Test): 0.7296317368651206, RMSE (Test): 0.8156031370162964\n",
      "Epoch [2266/5000], Train Loss: 0.007298465119674802, R2 Score (Train): 0.9937595173024414, RMSE (Train): 0.12402090430259705\n",
      "Test Loss: 0.6359458565711975, R2 Score (Test): 0.7424287433839983, RMSE (Test): 0.7960672378540039\n",
      "Epoch [2271/5000], Train Loss: 0.007457134546712041, R2 Score (Train): 0.9979692813719752, RMSE (Train): 0.07462049275636673\n",
      "Test Loss: 0.6453823149204254, R2 Score (Test): 0.7403387241913768, RMSE (Test): 0.7992904782295227\n",
      "Epoch [2276/5000], Train Loss: 0.00848750319952766, R2 Score (Train): 0.9985835482925832, RMSE (Train): 0.0687117725610733\n",
      "Test Loss: 0.6523393392562866, R2 Score (Test): 0.7355999840204137, RMSE (Test): 0.8065508604049683\n",
      "Epoch [2281/5000], Train Loss: 0.006031659121314685, R2 Score (Train): 0.9966615911651734, RMSE (Train): 0.09334500879049301\n",
      "Test Loss: 0.6782706379890442, R2 Score (Test): 0.7255572952435353, RMSE (Test): 0.8217257261276245\n",
      "Epoch [2286/5000], Train Loss: 0.006097830094707509, R2 Score (Train): 0.9987464518727046, RMSE (Train): 0.056390341371297836\n",
      "Test Loss: 0.6514215767383575, R2 Score (Test): 0.7366070466825472, RMSE (Test): 0.8050134181976318\n",
      "Epoch [2291/5000], Train Loss: 0.011136827835192284, R2 Score (Train): 0.9957756183187053, RMSE (Train): 0.1087668240070343\n",
      "Test Loss: 0.6725516319274902, R2 Score (Test): 0.731495859754695, RMSE (Test): 0.8127865791320801\n",
      "Epoch [2296/5000], Train Loss: 0.008158623396108547, R2 Score (Train): 0.9979164042615873, RMSE (Train): 0.07736276835203171\n",
      "Test Loss: 0.6727227866649628, R2 Score (Test): 0.7290102234748457, RMSE (Test): 0.816540002822876\n",
      "Epoch [2301/5000], Train Loss: 0.00804298254661262, R2 Score (Train): 0.9969201241826738, RMSE (Train): 0.09021586924791336\n",
      "Test Loss: 0.671519011259079, R2 Score (Test): 0.7293163163413553, RMSE (Test): 0.816078782081604\n",
      "Epoch [2306/5000], Train Loss: 0.010351471758137146, R2 Score (Train): 0.9978864897782842, RMSE (Train): 0.08452428132295609\n",
      "Test Loss: 0.6551330983638763, R2 Score (Test): 0.7354102657881489, RMSE (Test): 0.8068402409553528\n",
      "Epoch [2311/5000], Train Loss: 0.01000084145925939, R2 Score (Train): 0.9972441436676562, RMSE (Train): 0.0825619101524353\n",
      "Test Loss: 0.6743188500404358, R2 Score (Test): 0.7291960219041043, RMSE (Test): 0.8162600994110107\n",
      "Epoch [2316/5000], Train Loss: 0.012496149943520626, R2 Score (Train): 0.9965844292350664, RMSE (Train): 0.08528093993663788\n",
      "Test Loss: 0.6674682796001434, R2 Score (Test): 0.7299656849838635, RMSE (Test): 0.8150993585586548\n",
      "Epoch [2321/5000], Train Loss: 0.009809120635812482, R2 Score (Train): 0.9952586217886273, RMSE (Train): 0.11041496694087982\n",
      "Test Loss: 0.6450722515583038, R2 Score (Test): 0.7364304097803257, RMSE (Test): 0.8052833080291748\n",
      "Epoch [2326/5000], Train Loss: 0.007242334618543585, R2 Score (Train): 0.9982347568879212, RMSE (Train): 0.07372777163982391\n",
      "Test Loss: 0.6547835469245911, R2 Score (Test): 0.7365275357879484, RMSE (Test): 0.8051348924636841\n",
      "Epoch [2331/5000], Train Loss: 0.007590337578828136, R2 Score (Train): 0.9988482928904351, RMSE (Train): 0.057969581335783005\n",
      "Test Loss: 0.6499781906604767, R2 Score (Test): 0.7370763136304862, RMSE (Test): 0.8042960166931152\n",
      "Epoch [2336/5000], Train Loss: 0.006641971801097195, R2 Score (Train): 0.9980954272116876, RMSE (Train): 0.07259364426136017\n",
      "Test Loss: 0.6491364240646362, R2 Score (Test): 0.7382065705044483, RMSE (Test): 0.8025653958320618\n",
      "Epoch [2341/5000], Train Loss: 0.008988890796899796, R2 Score (Train): 0.9975400903283175, RMSE (Train): 0.07839968055486679\n",
      "Test Loss: 0.6396870911121368, R2 Score (Test): 0.7431160502916105, RMSE (Test): 0.7950044870376587\n",
      "Epoch [2346/5000], Train Loss: 0.009739545717214545, R2 Score (Train): 0.998046344697502, RMSE (Train): 0.0790892019867897\n",
      "Test Loss: 0.6421546638011932, R2 Score (Test): 0.7402002454423149, RMSE (Test): 0.7995036244392395\n",
      "Epoch [2351/5000], Train Loss: 0.009828123496845365, R2 Score (Train): 0.9966817325452682, RMSE (Train): 0.10002324730157852\n",
      "Test Loss: 0.6429336369037628, R2 Score (Test): 0.7405472642969488, RMSE (Test): 0.7989694476127625\n",
      "Epoch [2356/5000], Train Loss: 0.007240037666633725, R2 Score (Train): 0.9982105959747338, RMSE (Train): 0.08016006648540497\n",
      "Test Loss: 0.6590138077735901, R2 Score (Test): 0.7328424068384392, RMSE (Test): 0.8107460141181946\n",
      "Epoch [2361/5000], Train Loss: 0.009322627447545528, R2 Score (Train): 0.9981551018218838, RMSE (Train): 0.07718462496995926\n",
      "Test Loss: 0.642147421836853, R2 Score (Test): 0.7390575916218736, RMSE (Test): 0.8012598156929016\n",
      "Epoch [2366/5000], Train Loss: 0.009194294301172098, R2 Score (Train): 0.9966085288407778, RMSE (Train): 0.0871422216296196\n",
      "Test Loss: 0.6574344336986542, R2 Score (Test): 0.7335771892337513, RMSE (Test): 0.8096302151679993\n",
      "Epoch [2371/5000], Train Loss: 0.008701644760246078, R2 Score (Train): 0.9972716272427803, RMSE (Train): 0.09215761721134186\n",
      "Test Loss: 0.639688104391098, R2 Score (Test): 0.7392534497120208, RMSE (Test): 0.8009591102600098\n",
      "Epoch [2376/5000], Train Loss: 0.00803611621571084, R2 Score (Train): 0.9967216598860754, RMSE (Train): 0.0961810052394867\n",
      "Test Loss: 0.6649650633335114, R2 Score (Test): 0.7313471925594008, RMSE (Test): 0.8130115866661072\n",
      "Epoch [2381/5000], Train Loss: 0.007168557106827696, R2 Score (Train): 0.9965269196620319, RMSE (Train): 0.09467243403196335\n",
      "Test Loss: 0.6506747305393219, R2 Score (Test): 0.7342292513243761, RMSE (Test): 0.8086389303207397\n",
      "Epoch [2386/5000], Train Loss: 0.009131694600606957, R2 Score (Train): 0.9985111379074638, RMSE (Train): 0.07403755187988281\n",
      "Test Loss: 0.6500595808029175, R2 Score (Test): 0.7367611083695907, RMSE (Test): 0.8047779202461243\n",
      "Epoch [2391/5000], Train Loss: 0.007610465322310726, R2 Score (Train): 0.9976173617494635, RMSE (Train): 0.07497692853212357\n",
      "Test Loss: 0.6573503017425537, R2 Score (Test): 0.7320263038067163, RMSE (Test): 0.8119834065437317\n",
      "Epoch [2396/5000], Train Loss: 0.006501801234359543, R2 Score (Train): 0.997876461780231, RMSE (Train): 0.06671684980392456\n",
      "Test Loss: 0.6496193408966064, R2 Score (Test): 0.7347666515749327, RMSE (Test): 0.8078209161758423\n",
      "Epoch [2401/5000], Train Loss: 0.008548634282002846, R2 Score (Train): 0.9939829533276011, RMSE (Train): 0.12358901649713516\n",
      "Test Loss: 0.6422453224658966, R2 Score (Test): 0.7387162461601136, RMSE (Test): 0.8017837405204773\n",
      "Epoch [2406/5000], Train Loss: 0.007553323249643047, R2 Score (Train): 0.9970665299762571, RMSE (Train): 0.09525902569293976\n",
      "Test Loss: 0.6507205069065094, R2 Score (Test): 0.7358641565228601, RMSE (Test): 0.8061479330062866\n",
      "Epoch [2411/5000], Train Loss: 0.006808245632176598, R2 Score (Train): 0.9979531758257568, RMSE (Train): 0.06995360553264618\n",
      "Test Loss: 0.6520511209964752, R2 Score (Test): 0.7351510930565638, RMSE (Test): 0.8072353005409241\n",
      "Epoch [2416/5000], Train Loss: 0.006903823930770159, R2 Score (Train): 0.9973544081039861, RMSE (Train): 0.0820791944861412\n",
      "Test Loss: 0.6581642627716064, R2 Score (Test): 0.7324864029424865, RMSE (Test): 0.8112860321998596\n",
      "Epoch [2421/5000], Train Loss: 0.012781675284107527, R2 Score (Train): 0.9925557743373976, RMSE (Train): 0.15078170597553253\n",
      "Test Loss: 0.6672438383102417, R2 Score (Test): 0.7300410363267766, RMSE (Test): 0.8149855732917786\n",
      "Epoch [2426/5000], Train Loss: 0.006047854898497462, R2 Score (Train): 0.997004204882997, RMSE (Train): 0.08840706199407578\n",
      "Test Loss: 0.6624108254909515, R2 Score (Test): 0.7346012939607374, RMSE (Test): 0.8080727458000183\n",
      "Epoch [2431/5000], Train Loss: 0.006152484643583496, R2 Score (Train): 0.9967441094358087, RMSE (Train): 0.08947940915822983\n",
      "Test Loss: 0.6622423827648163, R2 Score (Test): 0.7333074859601858, RMSE (Test): 0.8100399971008301\n",
      "Epoch [2436/5000], Train Loss: 0.00620240291270117, R2 Score (Train): 0.9981963545758512, RMSE (Train): 0.07126162946224213\n",
      "Test Loss: 0.6734907031059265, R2 Score (Test): 0.7297325316920696, RMSE (Test): 0.8154511451721191\n",
      "Epoch [2441/5000], Train Loss: 0.009087830781936646, R2 Score (Train): 0.9979695322197266, RMSE (Train): 0.08039899915456772\n",
      "Test Loss: 0.694011002779007, R2 Score (Test): 0.7250299746082945, RMSE (Test): 0.8225148320198059\n",
      "Epoch [2446/5000], Train Loss: 0.007211944554001093, R2 Score (Train): 0.998702667068225, RMSE (Train): 0.06166946142911911\n",
      "Test Loss: 0.6887278854846954, R2 Score (Test): 0.7247208347328187, RMSE (Test): 0.8229770660400391\n",
      "Epoch [2451/5000], Train Loss: 0.008898292823384205, R2 Score (Train): 0.997732970248851, RMSE (Train): 0.07416706532239914\n",
      "Test Loss: 0.6378747224807739, R2 Score (Test): 0.7438049825390436, RMSE (Test): 0.793937623500824\n",
      "Epoch [2456/5000], Train Loss: 0.00766249264900883, R2 Score (Train): 0.9974283494107411, RMSE (Train): 0.08827649056911469\n",
      "Test Loss: 0.6574458479881287, R2 Score (Test): 0.7359641852382829, RMSE (Test): 0.8059952259063721\n",
      "Epoch [2461/5000], Train Loss: 0.010493732445562879, R2 Score (Train): 0.9941430869303113, RMSE (Train): 0.12850850820541382\n",
      "Test Loss: 0.6536422073841095, R2 Score (Test): 0.7358687391392906, RMSE (Test): 0.8061408996582031\n",
      "Epoch [2466/5000], Train Loss: 0.0075530370231717825, R2 Score (Train): 0.9982931176520278, RMSE (Train): 0.062384892255067825\n",
      "Test Loss: 0.6646227240562439, R2 Score (Test): 0.7316760752242939, RMSE (Test): 0.8125138282775879\n",
      "Epoch [2471/5000], Train Loss: 0.00900761573575437, R2 Score (Train): 0.9980419148653383, RMSE (Train): 0.07961063832044601\n",
      "Test Loss: 0.6595586836338043, R2 Score (Test): 0.7337897986792088, RMSE (Test): 0.8093071579933167\n",
      "Epoch [2476/5000], Train Loss: 0.00702254722515742, R2 Score (Train): 0.9987358331959649, RMSE (Train): 0.05970950797200203\n",
      "Test Loss: 0.6822650134563446, R2 Score (Test): 0.7281423905253055, RMSE (Test): 0.8178464770317078\n",
      "Epoch [2481/5000], Train Loss: 0.008793374911571542, R2 Score (Train): 0.9960568195020285, RMSE (Train): 0.10442863404750824\n",
      "Test Loss: 0.6739462912082672, R2 Score (Test): 0.7295784770613278, RMSE (Test): 0.8156834840774536\n",
      "Epoch [2486/5000], Train Loss: 0.006908644922077656, R2 Score (Train): 0.9973722356186424, RMSE (Train): 0.09623357653617859\n",
      "Test Loss: 0.6604076623916626, R2 Score (Test): 0.7342215534971804, RMSE (Test): 0.8086506724357605\n",
      "Epoch [2491/5000], Train Loss: 0.005781203391961753, R2 Score (Train): 0.9980021935499189, RMSE (Train): 0.07046312093734741\n",
      "Test Loss: 0.6507232487201691, R2 Score (Test): 0.7383964673557846, RMSE (Test): 0.802274227142334\n",
      "Epoch [2496/5000], Train Loss: 0.007907577557489276, R2 Score (Train): 0.997604417621391, RMSE (Train): 0.08427871018648148\n",
      "Test Loss: 0.6748138964176178, R2 Score (Test): 0.7307612048274406, RMSE (Test): 0.8138977289199829\n",
      "Epoch [2501/5000], Train Loss: 0.006645059601093332, R2 Score (Train): 0.9974505429168571, RMSE (Train): 0.08083190768957138\n",
      "Test Loss: 0.6633241772651672, R2 Score (Test): 0.7338108343937064, RMSE (Test): 0.809275209903717\n",
      "Epoch [2506/5000], Train Loss: 0.008992551748330394, R2 Score (Train): 0.997028182307409, RMSE (Train): 0.07438448816537857\n",
      "Test Loss: 0.6551370620727539, R2 Score (Test): 0.7366807078697792, RMSE (Test): 0.8049008250236511\n",
      "Epoch [2511/5000], Train Loss: 0.006852379922444622, R2 Score (Train): 0.9973267470726419, RMSE (Train): 0.08335520327091217\n",
      "Test Loss: 0.6727548837661743, R2 Score (Test): 0.7300519481892613, RMSE (Test): 0.8149691224098206\n",
      "Epoch [2516/5000], Train Loss: 0.008200115989893675, R2 Score (Train): 0.9959203475436524, RMSE (Train): 0.08266352117061615\n",
      "Test Loss: 0.6645709574222565, R2 Score (Test): 0.7345557779405001, RMSE (Test): 0.8081420063972473\n",
      "Epoch [2521/5000], Train Loss: 0.006639959678674738, R2 Score (Train): 0.9981693673426397, RMSE (Train): 0.07286106795072556\n",
      "Test Loss: 0.6831741034984589, R2 Score (Test): 0.7279357663007231, RMSE (Test): 0.8181572556495667\n",
      "Epoch [2526/5000], Train Loss: 0.0064344679315884905, R2 Score (Train): 0.9969714292758205, RMSE (Train): 0.09004071354866028\n",
      "Test Loss: 0.6684988141059875, R2 Score (Test): 0.7308979535842445, RMSE (Test): 0.8136910796165466\n",
      "Epoch [2531/5000], Train Loss: 0.007650421156237523, R2 Score (Train): 0.9985948432064711, RMSE (Train): 0.06222335994243622\n",
      "Test Loss: 0.6746579706668854, R2 Score (Test): 0.7305980574852549, RMSE (Test): 0.8141443729400635\n",
      "Epoch [2536/5000], Train Loss: 0.00619859853759408, R2 Score (Train): 0.99668320493108, RMSE (Train): 0.09157615154981613\n",
      "Test Loss: 0.6740607917308807, R2 Score (Test): 0.727594073366364, RMSE (Test): 0.8186708092689514\n",
      "Epoch [2541/5000], Train Loss: 0.007032326345021526, R2 Score (Train): 0.9961083756654194, RMSE (Train): 0.10227248072624207\n",
      "Test Loss: 0.6601665616035461, R2 Score (Test): 0.7364190202179777, RMSE (Test): 0.8053007125854492\n",
      "Epoch [2546/5000], Train Loss: 0.01104831633468469, R2 Score (Train): 0.9970159416354827, RMSE (Train): 0.09309284389019012\n",
      "Test Loss: 0.6636591255664825, R2 Score (Test): 0.7327645200223979, RMSE (Test): 0.8108641505241394\n",
      "Epoch [2551/5000], Train Loss: 0.01032451920521756, R2 Score (Train): 0.995787446127377, RMSE (Train): 0.11240445077419281\n",
      "Test Loss: 0.6875497102737427, R2 Score (Test): 0.7217516852045339, RMSE (Test): 0.8274034261703491\n",
      "Epoch [2556/5000], Train Loss: 0.007370927293474476, R2 Score (Train): 0.9942185660433778, RMSE (Train): 0.12191447615623474\n",
      "Test Loss: 0.6583110392093658, R2 Score (Test): 0.7342070797082785, RMSE (Test): 0.8086726665496826\n",
      "Epoch [2561/5000], Train Loss: 0.007391654498254259, R2 Score (Train): 0.9984513844126489, RMSE (Train): 0.06752072274684906\n",
      "Test Loss: 0.6704162359237671, R2 Score (Test): 0.7277736230627676, RMSE (Test): 0.8184009790420532\n",
      "Epoch [2566/5000], Train Loss: 0.011920833416904012, R2 Score (Train): 0.9965577424661963, RMSE (Train): 0.08358007669448853\n",
      "Test Loss: 0.6721424758434296, R2 Score (Test): 0.7311134814134718, RMSE (Test): 0.8133651614189148\n",
      "Epoch [2571/5000], Train Loss: 0.00971937490006288, R2 Score (Train): 0.997803661015556, RMSE (Train): 0.08034737408161163\n",
      "Test Loss: 0.6558746993541718, R2 Score (Test): 0.7347739200462247, RMSE (Test): 0.8078098893165588\n",
      "Epoch [2576/5000], Train Loss: 0.00861888937652111, R2 Score (Train): 0.9964917179646711, RMSE (Train): 0.09407995641231537\n",
      "Test Loss: 0.6600567996501923, R2 Score (Test): 0.7333079284393432, RMSE (Test): 0.8100393414497375\n",
      "Epoch [2581/5000], Train Loss: 0.01158464296410481, R2 Score (Train): 0.9982336412610483, RMSE (Train): 0.06888101994991302\n",
      "Test Loss: 0.6596775650978088, R2 Score (Test): 0.7315085135549683, RMSE (Test): 0.8127674460411072\n",
      "Epoch [2586/5000], Train Loss: 0.009195012118046483, R2 Score (Train): 0.9970217322051078, RMSE (Train): 0.09431444853544235\n",
      "Test Loss: 0.6295447051525116, R2 Score (Test): 0.7434112672909685, RMSE (Test): 0.7945474982261658\n",
      "Epoch [2591/5000], Train Loss: 0.012487612664699554, R2 Score (Train): 0.9978975556234871, RMSE (Train): 0.08546516299247742\n",
      "Test Loss: 0.6540804505348206, R2 Score (Test): 0.733243270904239, RMSE (Test): 0.8101375102996826\n",
      "Epoch [2596/5000], Train Loss: 0.008455595001578331, R2 Score (Train): 0.9982302627082345, RMSE (Train): 0.06713952869176865\n",
      "Test Loss: 0.6544935405254364, R2 Score (Test): 0.7376366005991328, RMSE (Test): 0.8034386038780212\n",
      "Epoch [2601/5000], Train Loss: 0.007012962984542052, R2 Score (Train): 0.9959574530759551, RMSE (Train): 0.10282071679830551\n",
      "Test Loss: 0.6585318148136139, R2 Score (Test): 0.7366704596964919, RMSE (Test): 0.8049165606498718\n",
      "Epoch [2606/5000], Train Loss: 0.005583128814275066, R2 Score (Train): 0.9963868531135759, RMSE (Train): 0.11069915443658829\n",
      "Test Loss: 0.6571795046329498, R2 Score (Test): 0.7378124345202719, RMSE (Test): 0.8031692504882812\n",
      "Epoch [2611/5000], Train Loss: 0.004550607409328222, R2 Score (Train): 0.9983241014970893, RMSE (Train): 0.06590470671653748\n",
      "Test Loss: 0.662592351436615, R2 Score (Test): 0.7340966180014987, RMSE (Test): 0.8088406324386597\n",
      "Epoch [2616/5000], Train Loss: 0.00828100647777319, R2 Score (Train): 0.9950287895705362, RMSE (Train): 0.11620121449232101\n",
      "Test Loss: 0.6454457938671112, R2 Score (Test): 0.7376539339461754, RMSE (Test): 0.8034120202064514\n",
      "Epoch [2621/5000], Train Loss: 0.006504858300710718, R2 Score (Train): 0.9979034637500371, RMSE (Train): 0.0729544460773468\n",
      "Test Loss: 0.66162970662117, R2 Score (Test): 0.7315886768597352, RMSE (Test): 0.8126460909843445\n",
      "Epoch [2626/5000], Train Loss: 0.0073881182276333375, R2 Score (Train): 0.9973632385279458, RMSE (Train): 0.08398613333702087\n",
      "Test Loss: 0.651709645986557, R2 Score (Test): 0.7366753407442028, RMSE (Test): 0.8049090504646301\n",
      "Epoch [2631/5000], Train Loss: 0.008212261988470951, R2 Score (Train): 0.9970386690123298, RMSE (Train): 0.08975137025117874\n",
      "Test Loss: 0.6612557768821716, R2 Score (Test): 0.7334194278502484, RMSE (Test): 0.8098699450492859\n",
      "Epoch [2636/5000], Train Loss: 0.008504656298706928, R2 Score (Train): 0.9971745350638933, RMSE (Train): 0.09324459731578827\n",
      "Test Loss: 0.6583777368068695, R2 Score (Test): 0.7365569110273873, RMSE (Test): 0.805090069770813\n",
      "Epoch [2641/5000], Train Loss: 0.0070138610899448395, R2 Score (Train): 0.9964669220796688, RMSE (Train): 0.10217641294002533\n",
      "Test Loss: 0.6500254571437836, R2 Score (Test): 0.7371940304670969, RMSE (Test): 0.804115891456604\n",
      "Epoch [2646/5000], Train Loss: 0.006814699620008469, R2 Score (Train): 0.9955110187140968, RMSE (Train): 0.11253941059112549\n",
      "Test Loss: 0.6823978126049042, R2 Score (Test): 0.7246467846285103, RMSE (Test): 0.823087751865387\n",
      "Epoch [2651/5000], Train Loss: 0.0064593964877227945, R2 Score (Train): 0.997281105482907, RMSE (Train): 0.08416371047496796\n",
      "Test Loss: 0.6833905577659607, R2 Score (Test): 0.7240007345910271, RMSE (Test): 0.8240527510643005\n",
      "Epoch [2656/5000], Train Loss: 0.005351613159291446, R2 Score (Train): 0.9977277299810675, RMSE (Train): 0.07644206285476685\n",
      "Test Loss: 0.6506905257701874, R2 Score (Test): 0.736573410739205, RMSE (Test): 0.8050647974014282\n",
      "Epoch [2661/5000], Train Loss: 0.008530506514944136, R2 Score (Train): 0.9971245021287684, RMSE (Train): 0.08201520889997482\n",
      "Test Loss: 0.6513291001319885, R2 Score (Test): 0.7382234454401609, RMSE (Test): 0.8025395274162292\n",
      "Epoch [2666/5000], Train Loss: 0.007562266585106651, R2 Score (Train): 0.9970830361359951, RMSE (Train): 0.10006998479366302\n",
      "Test Loss: 0.6612037718296051, R2 Score (Test): 0.735872015840164, RMSE (Test): 0.8061358332633972\n",
      "Epoch [2671/5000], Train Loss: 0.007334717083722353, R2 Score (Train): 0.9970137348428982, RMSE (Train): 0.09168846905231476\n",
      "Test Loss: 0.6665704250335693, R2 Score (Test): 0.7332354837489599, RMSE (Test): 0.8101493120193481\n",
      "Epoch [2676/5000], Train Loss: 0.008249244885519147, R2 Score (Train): 0.9981429225270868, RMSE (Train): 0.07700824737548828\n",
      "Test Loss: 0.7022108137607574, R2 Score (Test): 0.7203385740549233, RMSE (Test): 0.8295018672943115\n",
      "Epoch [2681/5000], Train Loss: 0.008140725083649158, R2 Score (Train): 0.997095307859029, RMSE (Train): 0.08962347358465195\n",
      "Test Loss: 0.6462827622890472, R2 Score (Test): 0.7381609722393081, RMSE (Test): 0.8026352524757385\n",
      "Epoch [2686/5000], Train Loss: 0.006061549453685681, R2 Score (Train): 0.9961472242555818, RMSE (Train): 0.1060013398528099\n",
      "Test Loss: 0.6464864909648895, R2 Score (Test): 0.7398145624001683, RMSE (Test): 0.8000968098640442\n",
      "Epoch [2691/5000], Train Loss: 0.007432995131239295, R2 Score (Train): 0.9989725307209673, RMSE (Train): 0.0580904558300972\n",
      "Test Loss: 0.6636838018894196, R2 Score (Test): 0.7334897167692802, RMSE (Test): 0.8097631931304932\n",
      "Epoch [2696/5000], Train Loss: 0.00604080397170037, R2 Score (Train): 0.9984592379623952, RMSE (Train): 0.06992591172456741\n",
      "Test Loss: 0.6648772060871124, R2 Score (Test): 0.7343937477425326, RMSE (Test): 0.8083885908126831\n",
      "Epoch [2701/5000], Train Loss: 0.004750010053006311, R2 Score (Train): 0.9963341373635886, RMSE (Train): 0.09397761523723602\n",
      "Test Loss: 0.6668771803379059, R2 Score (Test): 0.7307703608672804, RMSE (Test): 0.813883900642395\n",
      "Epoch [2706/5000], Train Loss: 0.004603058138551812, R2 Score (Train): 0.9990643317081558, RMSE (Train): 0.05619724094867706\n",
      "Test Loss: 0.6730745136737823, R2 Score (Test): 0.7308842692205584, RMSE (Test): 0.8137117028236389\n",
      "Epoch [2711/5000], Train Loss: 0.0059243382420390844, R2 Score (Train): 0.9949740601671376, RMSE (Train): 0.10153903067111969\n",
      "Test Loss: 0.6452169418334961, R2 Score (Test): 0.738982268403489, RMSE (Test): 0.8013755083084106\n",
      "Epoch [2716/5000], Train Loss: 0.006644991148884098, R2 Score (Train): 0.9948221752825621, RMSE (Train): 0.11234690994024277\n",
      "Test Loss: 0.6694922149181366, R2 Score (Test): 0.7295953335123077, RMSE (Test): 0.8156580924987793\n",
      "Epoch [2721/5000], Train Loss: 0.005446231535946329, R2 Score (Train): 0.9985530792686994, RMSE (Train): 0.06506188958883286\n",
      "Test Loss: 0.6757952868938446, R2 Score (Test): 0.7302699642351524, RMSE (Test): 0.8146399855613708\n",
      "Epoch [2726/5000], Train Loss: 0.006517495959997177, R2 Score (Train): 0.9964872149368249, RMSE (Train): 0.09293077886104584\n",
      "Test Loss: 0.6525311470031738, R2 Score (Test): 0.7364421232655374, RMSE (Test): 0.8052654266357422\n",
      "Epoch [2731/5000], Train Loss: 0.007392053647587697, R2 Score (Train): 0.9970518908978546, RMSE (Train): 0.09262549132108688\n",
      "Test Loss: 0.6747671663761139, R2 Score (Test): 0.7296845964160328, RMSE (Test): 0.8155233860015869\n",
      "Epoch [2736/5000], Train Loss: 0.0063619987340644, R2 Score (Train): 0.9960969325766488, RMSE (Train): 0.10754643380641937\n",
      "Test Loss: 0.6535486578941345, R2 Score (Test): 0.7367451842351797, RMSE (Test): 0.8048022389411926\n",
      "Epoch [2741/5000], Train Loss: 0.006825748986254136, R2 Score (Train): 0.9956568448920193, RMSE (Train): 0.10201408714056015\n",
      "Test Loss: 0.6641431748867035, R2 Score (Test): 0.7319890173830652, RMSE (Test): 0.812039852142334\n",
      "Epoch [2746/5000], Train Loss: 0.006993427174165845, R2 Score (Train): 0.9977926250680698, RMSE (Train): 0.0859832614660263\n",
      "Test Loss: 0.6506516635417938, R2 Score (Test): 0.7355815298901651, RMSE (Test): 0.806579053401947\n",
      "Epoch [2751/5000], Train Loss: 0.008410129230469465, R2 Score (Train): 0.9969152503123524, RMSE (Train): 0.08903312683105469\n",
      "Test Loss: 0.6437446475028992, R2 Score (Test): 0.7401890889969752, RMSE (Test): 0.79952073097229\n",
      "Epoch [2756/5000], Train Loss: 0.007482673895234863, R2 Score (Train): 0.9983951807654501, RMSE (Train): 0.06573643535375595\n",
      "Test Loss: 0.6532200276851654, R2 Score (Test): 0.7367659206627134, RMSE (Test): 0.8047706484794617\n",
      "Epoch [2761/5000], Train Loss: 0.009180248249322176, R2 Score (Train): 0.9982120101446824, RMSE (Train): 0.07053881138563156\n",
      "Test Loss: 0.671788901090622, R2 Score (Test): 0.7279628445804411, RMSE (Test): 0.8181164860725403\n",
      "Epoch [2766/5000], Train Loss: 0.007451286539435387, R2 Score (Train): 0.9978382497377324, RMSE (Train): 0.07440397143363953\n",
      "Test Loss: 0.6561127305030823, R2 Score (Test): 0.7320612441668677, RMSE (Test): 0.8119304180145264\n",
      "Epoch [2771/5000], Train Loss: 0.009620401232192913, R2 Score (Train): 0.9979998362532333, RMSE (Train): 0.07249967008829117\n",
      "Test Loss: 0.6495464742183685, R2 Score (Test): 0.7381887119916324, RMSE (Test): 0.8025927543640137\n",
      "Epoch [2776/5000], Train Loss: 0.007514278482024868, R2 Score (Train): 0.9968129533288133, RMSE (Train): 0.09516651928424835\n",
      "Test Loss: 0.6305568218231201, R2 Score (Test): 0.7453897312884435, RMSE (Test): 0.7914782762527466\n",
      "Epoch [2781/5000], Train Loss: 0.005440186786775787, R2 Score (Train): 0.9963270670874048, RMSE (Train): 0.10106923431158066\n",
      "Test Loss: 0.6466981172561646, R2 Score (Test): 0.739044920450874, RMSE (Test): 0.8012793064117432\n",
      "Epoch [2786/5000], Train Loss: 0.005527234636247158, R2 Score (Train): 0.9988376171708991, RMSE (Train): 0.06024863198399544\n",
      "Test Loss: 0.6689321398735046, R2 Score (Test): 0.7324969953800278, RMSE (Test): 0.8112698793411255\n",
      "Epoch [2791/5000], Train Loss: 0.006418383525063594, R2 Score (Train): 0.9985042178533031, RMSE (Train): 0.06987721472978592\n",
      "Test Loss: 0.6903164386749268, R2 Score (Test): 0.722667728906256, RMSE (Test): 0.8260403275489807\n",
      "Epoch [2796/5000], Train Loss: 0.008251317776739597, R2 Score (Train): 0.9953090576692154, RMSE (Train): 0.1172081008553505\n",
      "Test Loss: 0.6454454362392426, R2 Score (Test): 0.7393282407340973, RMSE (Test): 0.8008441925048828\n",
      "Epoch [2801/5000], Train Loss: 0.0055004305904731154, R2 Score (Train): 0.9932250702666279, RMSE (Train): 0.12194564938545227\n",
      "Test Loss: 0.6723251640796661, R2 Score (Test): 0.7264089522706323, RMSE (Test): 0.8204497694969177\n",
      "Epoch [2806/5000], Train Loss: 0.006054084903250138, R2 Score (Train): 0.9957779297504367, RMSE (Train): 0.11254126578569412\n",
      "Test Loss: 0.6739359200000763, R2 Score (Test): 0.7285827034168152, RMSE (Test): 0.8171839118003845\n",
      "Epoch [2811/5000], Train Loss: 0.007205145666375756, R2 Score (Train): 0.9954951684638821, RMSE (Train): 0.11348552256822586\n",
      "Test Loss: 0.6757811605930328, R2 Score (Test): 0.7282671497461773, RMSE (Test): 0.8176587820053101\n",
      "Epoch [2816/5000], Train Loss: 0.008607557819535335, R2 Score (Train): 0.9974091097243383, RMSE (Train): 0.08419238775968552\n",
      "Test Loss: 0.6449835002422333, R2 Score (Test): 0.7376251878842252, RMSE (Test): 0.8034560680389404\n",
      "Epoch [2821/5000], Train Loss: 0.00915528453576068, R2 Score (Train): 0.998997108965462, RMSE (Train): 0.0596158467233181\n",
      "Test Loss: 0.6459635496139526, R2 Score (Test): 0.7354824500475825, RMSE (Test): 0.8067301511764526\n",
      "Epoch [2826/5000], Train Loss: 0.01024133681009213, R2 Score (Train): 0.9959648959656744, RMSE (Train): 0.10301892459392548\n",
      "Test Loss: 0.6401698887348175, R2 Score (Test): 0.7373445048009025, RMSE (Test): 0.8038856983184814\n",
      "Epoch [2831/5000], Train Loss: 0.00610161650304993, R2 Score (Train): 0.9985335039392593, RMSE (Train): 0.06343624740839005\n",
      "Test Loss: 0.671446830034256, R2 Score (Test): 0.7290979344450839, RMSE (Test): 0.8164079189300537\n",
      "Epoch [2836/5000], Train Loss: 0.008920246579994759, R2 Score (Train): 0.9979789300822658, RMSE (Train): 0.07819204777479172\n",
      "Test Loss: 0.6549635231494904, R2 Score (Test): 0.7368693634886228, RMSE (Test): 0.8046124577522278\n",
      "Epoch [2841/5000], Train Loss: 0.008293239322180549, R2 Score (Train): 0.9956056242754222, RMSE (Train): 0.10057452321052551\n",
      "Test Loss: 0.6370124518871307, R2 Score (Test): 0.7422158469860876, RMSE (Test): 0.7963961362838745\n",
      "Epoch [2846/5000], Train Loss: 0.004696431336924434, R2 Score (Train): 0.9977091359949392, RMSE (Train): 0.08465691655874252\n",
      "Test Loss: 0.644703209400177, R2 Score (Test): 0.740989410485785, RMSE (Test): 0.7982883453369141\n",
      "Epoch [2851/5000], Train Loss: 0.005098717364793022, R2 Score (Train): 0.9978050819664602, RMSE (Train): 0.07103294879198074\n",
      "Test Loss: 0.6449626088142395, R2 Score (Test): 0.7378291191809389, RMSE (Test): 0.8031437993049622\n",
      "Epoch [2856/5000], Train Loss: 0.004621459869667888, R2 Score (Train): 0.9968038957136247, RMSE (Train): 0.08467758446931839\n",
      "Test Loss: 0.6459379494190216, R2 Score (Test): 0.7384014830193684, RMSE (Test): 0.802266538143158\n",
      "Epoch [2861/5000], Train Loss: 0.008383981534279883, R2 Score (Train): 0.9946939725782048, RMSE (Train): 0.1227816492319107\n",
      "Test Loss: 0.6132768094539642, R2 Score (Test): 0.7516095658641814, RMSE (Test): 0.7817510962486267\n",
      "Epoch [2866/5000], Train Loss: 0.006257022110124429, R2 Score (Train): 0.9977995981041696, RMSE (Train): 0.07801249623298645\n",
      "Test Loss: 0.649095207452774, R2 Score (Test): 0.7395596652867444, RMSE (Test): 0.8004886507987976\n",
      "Epoch [2871/5000], Train Loss: 0.0050230185346057015, R2 Score (Train): 0.998675384578716, RMSE (Train): 0.06294190138578415\n",
      "Test Loss: 0.6587115228176117, R2 Score (Test): 0.7354843018514334, RMSE (Test): 0.8067273497581482\n",
      "Epoch [2876/5000], Train Loss: 0.005670180777087808, R2 Score (Train): 0.9974643751582695, RMSE (Train): 0.09080659598112106\n",
      "Test Loss: 0.6672629714012146, R2 Score (Test): 0.7304347250216917, RMSE (Test): 0.8143910765647888\n",
      "Epoch [2881/5000], Train Loss: 0.008274841122329235, R2 Score (Train): 0.9953037467221574, RMSE (Train): 0.1135236918926239\n",
      "Test Loss: 0.6421915292739868, R2 Score (Test): 0.7420997791976364, RMSE (Test): 0.7965753674507141\n",
      "Epoch [2886/5000], Train Loss: 0.00941037821273009, R2 Score (Train): 0.9969499542530241, RMSE (Train): 0.09709694236516953\n",
      "Test Loss: 0.6365889310836792, R2 Score (Test): 0.7434927072253685, RMSE (Test): 0.794421374797821\n",
      "Epoch [2891/5000], Train Loss: 0.007563161430880427, R2 Score (Train): 0.9975393869430985, RMSE (Train): 0.07872606813907623\n",
      "Test Loss: 0.6560723781585693, R2 Score (Test): 0.7393351861531607, RMSE (Test): 0.800833523273468\n",
      "Epoch [2896/5000], Train Loss: 0.008870962308719754, R2 Score (Train): 0.9935283629857006, RMSE (Train): 0.14441438019275665\n",
      "Test Loss: 0.6400120258331299, R2 Score (Test): 0.7405504045093021, RMSE (Test): 0.7989645600318909\n",
      "Epoch [2901/5000], Train Loss: 0.0088072270154953, R2 Score (Train): 0.9976960699173637, RMSE (Train): 0.09140022844076157\n",
      "Test Loss: 0.629980206489563, R2 Score (Test): 0.7422364217426882, RMSE (Test): 0.7963643670082092\n",
      "Epoch [2906/5000], Train Loss: 0.006574670240903894, R2 Score (Train): 0.9952845055145138, RMSE (Train): 0.11973226070404053\n",
      "Test Loss: 0.6395276486873627, R2 Score (Test): 0.7413649820985223, RMSE (Test): 0.7977094054222107\n",
      "Epoch [2911/5000], Train Loss: 0.0080501571452866, R2 Score (Train): 0.9982376434593428, RMSE (Train): 0.0837273895740509\n",
      "Test Loss: 0.6649273931980133, R2 Score (Test): 0.7302042348472031, RMSE (Test): 0.8147391676902771\n",
      "Epoch [2916/5000], Train Loss: 0.008744501043111086, R2 Score (Train): 0.9976307102982288, RMSE (Train): 0.0877288281917572\n",
      "Test Loss: 0.6294659972190857, R2 Score (Test): 0.7437617328812951, RMSE (Test): 0.7940046787261963\n",
      "Epoch [2921/5000], Train Loss: 0.0075030354006836815, R2 Score (Train): 0.998282521493, RMSE (Train): 0.0748271495103836\n",
      "Test Loss: 0.6492476165294647, R2 Score (Test): 0.7391834578053399, RMSE (Test): 0.8010665774345398\n",
      "Epoch [2926/5000], Train Loss: 0.006642976930985848, R2 Score (Train): 0.9981755027511713, RMSE (Train): 0.06596451997756958\n",
      "Test Loss: 0.657605916261673, R2 Score (Test): 0.734281071164409, RMSE (Test): 0.8085600733757019\n",
      "Epoch [2931/5000], Train Loss: 0.005495746581194301, R2 Score (Train): 0.9980633058742414, RMSE (Train): 0.07046075910329819\n",
      "Test Loss: 0.646269291639328, R2 Score (Test): 0.7374906134794232, RMSE (Test): 0.8036620616912842\n",
      "Epoch [2936/5000], Train Loss: 0.0062391535223772126, R2 Score (Train): 0.9978442457039226, RMSE (Train): 0.08006296306848526\n",
      "Test Loss: 0.6478042900562286, R2 Score (Test): 0.7363738847540293, RMSE (Test): 0.8053696155548096\n",
      "Epoch [2941/5000], Train Loss: 0.004039242824849983, R2 Score (Train): 0.9981993657671463, RMSE (Train): 0.06377507746219635\n",
      "Test Loss: 0.6506028175354004, R2 Score (Test): 0.7343876459705679, RMSE (Test): 0.8083978891372681\n",
      "Epoch [2946/5000], Train Loss: 0.005231706891208887, R2 Score (Train): 0.9975050370951709, RMSE (Train): 0.08275090157985687\n",
      "Test Loss: 0.6436098515987396, R2 Score (Test): 0.7396692375922163, RMSE (Test): 0.8003202080726624\n",
      "Epoch [2951/5000], Train Loss: 0.0059701089436809225, R2 Score (Train): 0.9974375234891082, RMSE (Train): 0.08199556916952133\n",
      "Test Loss: 0.640594094991684, R2 Score (Test): 0.7389063158495219, RMSE (Test): 0.8014920949935913\n",
      "Epoch [2956/5000], Train Loss: 0.009140990519275268, R2 Score (Train): 0.9937969828724137, RMSE (Train): 0.11576002091169357\n",
      "Test Loss: 0.6702397465705872, R2 Score (Test): 0.7295449861128236, RMSE (Test): 0.8157340288162231\n",
      "Epoch [2961/5000], Train Loss: 0.007180051567653815, R2 Score (Train): 0.998597381288646, RMSE (Train): 0.06653560698032379\n",
      "Test Loss: 0.6589322090148926, R2 Score (Test): 0.734159683212537, RMSE (Test): 0.8087447285652161\n",
      "Epoch [2966/5000], Train Loss: 0.0064988157246261835, R2 Score (Train): 0.9973225526917244, RMSE (Train): 0.09135755151510239\n",
      "Test Loss: 0.6490955650806427, R2 Score (Test): 0.7373667447983141, RMSE (Test): 0.8038516640663147\n",
      "Epoch [2971/5000], Train Loss: 0.00772706069983542, R2 Score (Train): 0.9978206281874372, RMSE (Train): 0.07966151833534241\n",
      "Test Loss: 0.6657837629318237, R2 Score (Test): 0.7322068713061785, RMSE (Test): 0.8117097616195679\n",
      "Epoch [2976/5000], Train Loss: 0.006675739539787173, R2 Score (Train): 0.9982436815132834, RMSE (Train): 0.0763619989156723\n",
      "Test Loss: 0.6526846587657928, R2 Score (Test): 0.7367440016678257, RMSE (Test): 0.8048041462898254\n",
      "Epoch [2981/5000], Train Loss: 0.006246103788726032, R2 Score (Train): 0.9968540929891506, RMSE (Train): 0.08869031816720963\n",
      "Test Loss: 0.6457018852233887, R2 Score (Test): 0.7378842009777193, RMSE (Test): 0.8030593395233154\n",
      "Epoch [2986/5000], Train Loss: 0.007068282769372066, R2 Score (Train): 0.9946335669490394, RMSE (Train): 0.11298305541276932\n",
      "Test Loss: 0.6407206058502197, R2 Score (Test): 0.741008224473396, RMSE (Test): 0.7982593774795532\n",
      "Epoch [2991/5000], Train Loss: 0.008449215752383074, R2 Score (Train): 0.9981131781898126, RMSE (Train): 0.07979509979486465\n",
      "Test Loss: 0.658064216375351, R2 Score (Test): 0.7336093206561016, RMSE (Test): 0.809581458568573\n",
      "Epoch [2996/5000], Train Loss: 0.005683963963141044, R2 Score (Train): 0.9949010539874273, RMSE (Train): 0.10728343576192856\n",
      "Test Loss: 0.6468091309070587, R2 Score (Test): 0.7381737336118496, RMSE (Test): 0.802615761756897\n",
      "Epoch [3001/5000], Train Loss: 0.0060491012409329414, R2 Score (Train): 0.9977825841555009, RMSE (Train): 0.0823930948972702\n",
      "Test Loss: 0.6524388492107391, R2 Score (Test): 0.7364848229690759, RMSE (Test): 0.8052001595497131\n",
      "Epoch [3006/5000], Train Loss: 0.008372163632884622, R2 Score (Train): 0.9989127865447708, RMSE (Train): 0.056737206876277924\n",
      "Test Loss: 0.6598767638206482, R2 Score (Test): 0.7353745510494154, RMSE (Test): 0.8068946599960327\n",
      "Epoch [3011/5000], Train Loss: 0.005951545666903257, R2 Score (Train): 0.9983183776222819, RMSE (Train): 0.0606360137462616\n",
      "Test Loss: 0.6623415052890778, R2 Score (Test): 0.7352386815025156, RMSE (Test): 0.8071017861366272\n",
      "Epoch [3016/5000], Train Loss: 0.006127902229006092, R2 Score (Train): 0.9960704219304939, RMSE (Train): 0.09484295547008514\n",
      "Test Loss: 0.646520346403122, R2 Score (Test): 0.7380465815082183, RMSE (Test): 0.802810549736023\n",
      "Epoch [3021/5000], Train Loss: 0.005974368386281033, R2 Score (Train): 0.996870678572858, RMSE (Train): 0.08431307226419449\n",
      "Test Loss: 0.6499542891979218, R2 Score (Test): 0.7371209703230532, RMSE (Test): 0.8042277097702026\n",
      "Epoch [3026/5000], Train Loss: 0.007860417012125254, R2 Score (Train): 0.9980824547125708, RMSE (Train): 0.0783451646566391\n",
      "Test Loss: 0.6570866405963898, R2 Score (Test): 0.737874934244733, RMSE (Test): 0.8030734658241272\n",
      "Epoch [3031/5000], Train Loss: 0.006056481196234624, R2 Score (Train): 0.9979783560864, RMSE (Train): 0.07462329417467117\n",
      "Test Loss: 0.6551846861839294, R2 Score (Test): 0.7365559180612261, RMSE (Test): 0.8050915598869324\n",
      "Epoch [3036/5000], Train Loss: 0.005781585738683741, R2 Score (Train): 0.9987641060855766, RMSE (Train): 0.06492037326097488\n",
      "Test Loss: 0.6538803279399872, R2 Score (Test): 0.7372967025714415, RMSE (Test): 0.8039587736129761\n",
      "Epoch [3041/5000], Train Loss: 0.005838824436068535, R2 Score (Train): 0.997755195686711, RMSE (Train): 0.06814943999052048\n",
      "Test Loss: 0.6526497900485992, R2 Score (Test): 0.737345858051432, RMSE (Test): 0.8038836121559143\n",
      "Epoch [3046/5000], Train Loss: 0.005097141411776344, R2 Score (Train): 0.9974887856695025, RMSE (Train): 0.08706084638834\n",
      "Test Loss: 0.6674342751502991, R2 Score (Test): 0.7344255149046155, RMSE (Test): 0.8083402514457703\n",
      "Epoch [3051/5000], Train Loss: 0.007100524768854181, R2 Score (Train): 0.9978765855037146, RMSE (Train): 0.0695846825838089\n",
      "Test Loss: 0.6573569774627686, R2 Score (Test): 0.7334395849383127, RMSE (Test): 0.8098393082618713\n",
      "Epoch [3056/5000], Train Loss: 0.0054794800623009605, R2 Score (Train): 0.996815013658872, RMSE (Train): 0.0879945307970047\n",
      "Test Loss: 0.6547536253929138, R2 Score (Test): 0.7372589065534625, RMSE (Test): 0.8040165901184082\n",
      "Epoch [3061/5000], Train Loss: 0.005750858962225418, R2 Score (Train): 0.9978823299986015, RMSE (Train): 0.07680374383926392\n",
      "Test Loss: 0.6246533989906311, R2 Score (Test): 0.7451664298758076, RMSE (Test): 0.7918252944946289\n",
      "Epoch [3066/5000], Train Loss: 0.005567508904884259, R2 Score (Train): 0.9961137657785187, RMSE (Train): 0.10838993638753891\n",
      "Test Loss: 0.6632855832576752, R2 Score (Test): 0.736452708179449, RMSE (Test): 0.8052492737770081\n",
      "Epoch [3071/5000], Train Loss: 0.007505304490526517, R2 Score (Train): 0.997976271651477, RMSE (Train): 0.07312051206827164\n",
      "Test Loss: 0.6439332664012909, R2 Score (Test): 0.7412223911497035, RMSE (Test): 0.7979292273521423\n",
      "Epoch [3076/5000], Train Loss: 0.006875105435028672, R2 Score (Train): 0.9961023668239201, RMSE (Train): 0.10972708463668823\n",
      "Test Loss: 0.6548337638378143, R2 Score (Test): 0.7383150038075253, RMSE (Test): 0.8023991584777832\n",
      "Epoch [3081/5000], Train Loss: 0.009235438968365392, R2 Score (Train): 0.9962478237067578, RMSE (Train): 0.10500717163085938\n",
      "Test Loss: 0.6444426476955414, R2 Score (Test): 0.7419535081658167, RMSE (Test): 0.7968013286590576\n",
      "Epoch [3086/5000], Train Loss: 0.005458500313883026, R2 Score (Train): 0.9977493598402681, RMSE (Train): 0.0754375085234642\n",
      "Test Loss: 0.6787067949771881, R2 Score (Test): 0.7283038085358917, RMSE (Test): 0.8176037073135376\n",
      "Epoch [3091/5000], Train Loss: 0.005408366986860831, R2 Score (Train): 0.9986028121776956, RMSE (Train): 0.06640058010816574\n",
      "Test Loss: 0.6520232856273651, R2 Score (Test): 0.7393603399998115, RMSE (Test): 0.8007949590682983\n",
      "Epoch [3096/5000], Train Loss: 0.006516158968831102, R2 Score (Train): 0.9982303946136982, RMSE (Train): 0.06818399578332901\n",
      "Test Loss: 0.65680792927742, R2 Score (Test): 0.7349895136871092, RMSE (Test): 0.8074814677238464\n",
      "Epoch [3101/5000], Train Loss: 0.005520151268380384, R2 Score (Train): 0.9982846045411798, RMSE (Train): 0.07584338635206223\n",
      "Test Loss: 0.6561555862426758, R2 Score (Test): 0.7382221468344308, RMSE (Test): 0.8025414943695068\n",
      "Epoch [3106/5000], Train Loss: 0.005784291851644714, R2 Score (Train): 0.9970763893758203, RMSE (Train): 0.0954347550868988\n",
      "Test Loss: 0.6549380719661713, R2 Score (Test): 0.7348186026997047, RMSE (Test): 0.8077418208122253\n",
      "Epoch [3111/5000], Train Loss: 0.004219612805172801, R2 Score (Train): 0.9979463492238232, RMSE (Train): 0.07461641728878021\n",
      "Test Loss: 0.6533640325069427, R2 Score (Test): 0.7366308371751248, RMSE (Test): 0.8049770593643188\n",
      "Epoch [3116/5000], Train Loss: 0.0060795515310019255, R2 Score (Train): 0.9980053131961422, RMSE (Train): 0.06661161035299301\n",
      "Test Loss: 0.6547807157039642, R2 Score (Test): 0.7371307999560134, RMSE (Test): 0.8042126297950745\n",
      "Epoch [3121/5000], Train Loss: 0.00774126344670852, R2 Score (Train): 0.9980080091021448, RMSE (Train): 0.09241053462028503\n",
      "Test Loss: 0.6642409861087799, R2 Score (Test): 0.735743790562496, RMSE (Test): 0.8063315749168396\n",
      "Epoch [3126/5000], Train Loss: 0.009269335229570666, R2 Score (Train): 0.9972395207560595, RMSE (Train): 0.07805754989385605\n",
      "Test Loss: 0.6641269624233246, R2 Score (Test): 0.7349879648808986, RMSE (Test): 0.8074837923049927\n",
      "Epoch [3131/5000], Train Loss: 0.005892250221222639, R2 Score (Train): 0.9979285801306635, RMSE (Train): 0.07864971458911896\n",
      "Test Loss: 0.6593822538852692, R2 Score (Test): 0.7371671674947137, RMSE (Test): 0.804157018661499\n",
      "Epoch [3136/5000], Train Loss: 0.006281258383144935, R2 Score (Train): 0.9971217961410612, RMSE (Train): 0.1035725474357605\n",
      "Test Loss: 0.6531906425952911, R2 Score (Test): 0.7370587019356543, RMSE (Test): 0.8043228983879089\n",
      "Epoch [3141/5000], Train Loss: 0.0051673090395828085, R2 Score (Train): 0.9977860033870193, RMSE (Train): 0.07305505871772766\n",
      "Test Loss: 0.6327067613601685, R2 Score (Test): 0.7432003527512694, RMSE (Test): 0.7948738932609558\n",
      "Epoch [3146/5000], Train Loss: 0.009636028204113245, R2 Score (Train): 0.995324605787599, RMSE (Train): 0.12259566783905029\n",
      "Test Loss: 0.6449194848537445, R2 Score (Test): 0.7401801642306838, RMSE (Test): 0.7995345592498779\n",
      "Epoch [3151/5000], Train Loss: 0.006298237092172106, R2 Score (Train): 0.9953285966609747, RMSE (Train): 0.09175251424312592\n",
      "Test Loss: 0.6331671476364136, R2 Score (Test): 0.7421764271477272, RMSE (Test): 0.796457052230835\n",
      "Epoch [3156/5000], Train Loss: 0.006953017010043065, R2 Score (Train): 0.997371939490425, RMSE (Train): 0.08634400367736816\n",
      "Test Loss: 0.6484634280204773, R2 Score (Test): 0.7389320119897216, RMSE (Test): 0.80145263671875\n",
      "Epoch [3161/5000], Train Loss: 0.007092806665847699, R2 Score (Train): 0.9987898543992331, RMSE (Train): 0.06535912305116653\n",
      "Test Loss: 0.6284149289131165, R2 Score (Test): 0.7463703425823929, RMSE (Test): 0.7899526953697205\n",
      "Epoch [3166/5000], Train Loss: 0.007000873874252041, R2 Score (Train): 0.9970598741807087, RMSE (Train): 0.09528182446956635\n",
      "Test Loss: 0.6424336433410645, R2 Score (Test): 0.7432839377055132, RMSE (Test): 0.7947445511817932\n",
      "Epoch [3171/5000], Train Loss: 0.011086854618042707, R2 Score (Train): 0.9981846972447158, RMSE (Train): 0.06861411780118942\n",
      "Test Loss: 0.63209667801857, R2 Score (Test): 0.7434115505029771, RMSE (Test): 0.7945470213890076\n",
      "Epoch [3176/5000], Train Loss: 0.008152043990169963, R2 Score (Train): 0.9960890033398913, RMSE (Train): 0.100446417927742\n",
      "Test Loss: 0.6338931620121002, R2 Score (Test): 0.7443203080839038, RMSE (Test): 0.7931387424468994\n",
      "Epoch [3181/5000], Train Loss: 0.0068685433361679316, R2 Score (Train): 0.99756963160198, RMSE (Train): 0.06775519251823425\n",
      "Test Loss: 0.6480827927589417, R2 Score (Test): 0.7405644979845512, RMSE (Test): 0.7989429235458374\n",
      "Epoch [3186/5000], Train Loss: 0.004676654042365651, R2 Score (Train): 0.997997379623231, RMSE (Train): 0.06993374228477478\n",
      "Test Loss: 0.6507832705974579, R2 Score (Test): 0.739062897993828, RMSE (Test): 0.8012517094612122\n",
      "Epoch [3191/5000], Train Loss: 0.0051830024070416885, R2 Score (Train): 0.9980753604347161, RMSE (Train): 0.07806077599525452\n",
      "Test Loss: 0.6422252357006073, R2 Score (Test): 0.7417446314618793, RMSE (Test): 0.7971237301826477\n",
      "Epoch [3196/5000], Train Loss: 0.006111768462384741, R2 Score (Train): 0.9984626311048321, RMSE (Train): 0.07145673781633377\n",
      "Test Loss: 0.6444147229194641, R2 Score (Test): 0.7402524380448591, RMSE (Test): 0.7994232773780823\n",
      "Epoch [3201/5000], Train Loss: 0.0057137881716092425, R2 Score (Train): 0.9984794956363323, RMSE (Train): 0.061607860028743744\n",
      "Test Loss: 0.6415349245071411, R2 Score (Test): 0.7418015534255228, RMSE (Test): 0.7970358729362488\n",
      "Epoch [3206/5000], Train Loss: 0.007859798225884637, R2 Score (Train): 0.9985869832240785, RMSE (Train): 0.06649281084537506\n",
      "Test Loss: 0.6581724286079407, R2 Score (Test): 0.7352959031436372, RMSE (Test): 0.8070145845413208\n",
      "Epoch [3211/5000], Train Loss: 0.004910703593244155, R2 Score (Train): 0.9971760742581754, RMSE (Train): 0.0900772288441658\n",
      "Test Loss: 0.6334789097309113, R2 Score (Test): 0.7465097663714039, RMSE (Test): 0.7897355556488037\n",
      "Epoch [3216/5000], Train Loss: 0.006230304483324289, R2 Score (Train): 0.9982870509786131, RMSE (Train): 0.0733661949634552\n",
      "Test Loss: 0.6532927751541138, R2 Score (Test): 0.7405452906291198, RMSE (Test): 0.7989724278450012\n",
      "Epoch [3221/5000], Train Loss: 0.005882739865531524, R2 Score (Train): 0.9972754614757116, RMSE (Train): 0.07989379018545151\n",
      "Test Loss: 0.6393739581108093, R2 Score (Test): 0.7420220165680067, RMSE (Test): 0.7966955304145813\n",
      "Epoch [3226/5000], Train Loss: 0.003628049783098201, R2 Score (Train): 0.9982744551877931, RMSE (Train): 0.07125929743051529\n",
      "Test Loss: 0.6412603557109833, R2 Score (Test): 0.7415482615718381, RMSE (Test): 0.797426700592041\n",
      "Epoch [3231/5000], Train Loss: 0.007171564269810915, R2 Score (Train): 0.9936300116099455, RMSE (Train): 0.1164851188659668\n",
      "Test Loss: 0.648191511631012, R2 Score (Test): 0.7397012231587088, RMSE (Test): 0.8002710938453674\n",
      "Epoch [3236/5000], Train Loss: 0.006746887695044279, R2 Score (Train): 0.9976097543230025, RMSE (Train): 0.09266980737447739\n",
      "Test Loss: 0.6497940719127655, R2 Score (Test): 0.7378901451686397, RMSE (Test): 0.8030502796173096\n",
      "Epoch [3241/5000], Train Loss: 0.007396405873199304, R2 Score (Train): 0.9986538758227659, RMSE (Train): 0.06225486099720001\n",
      "Test Loss: 0.6483623385429382, R2 Score (Test): 0.7395757817654348, RMSE (Test): 0.800463855266571\n",
      "Epoch [3246/5000], Train Loss: 0.007309815535942714, R2 Score (Train): 0.994141824664585, RMSE (Train): 0.10584862530231476\n",
      "Test Loss: 0.6599371135234833, R2 Score (Test): 0.7352051647682054, RMSE (Test): 0.8071529269218445\n",
      "Epoch [3251/5000], Train Loss: 0.007043660307923953, R2 Score (Train): 0.9982213105932302, RMSE (Train): 0.0677090734243393\n",
      "Test Loss: 0.6419412195682526, R2 Score (Test): 0.7428021634797435, RMSE (Test): 0.7954899072647095\n",
      "Epoch [3256/5000], Train Loss: 0.006672907931109269, R2 Score (Train): 0.9981450242169095, RMSE (Train): 0.07277388870716095\n",
      "Test Loss: 0.6553252935409546, R2 Score (Test): 0.735182532584796, RMSE (Test): 0.8071873188018799\n",
      "Epoch [3261/5000], Train Loss: 0.005861607302601139, R2 Score (Train): 0.9972612008658109, RMSE (Train): 0.08732388913631439\n",
      "Test Loss: 0.6242683231830597, R2 Score (Test): 0.7487544419875765, RMSE (Test): 0.7862311601638794\n",
      "Epoch [3266/5000], Train Loss: 0.007824313128367066, R2 Score (Train): 0.9972412757837992, RMSE (Train): 0.09085438400506973\n",
      "Test Loss: 0.6521738469600677, R2 Score (Test): 0.7379968566812508, RMSE (Test): 0.8028867840766907\n",
      "Epoch [3271/5000], Train Loss: 0.006766719277948141, R2 Score (Train): 0.9973359210857783, RMSE (Train): 0.09035535156726837\n",
      "Test Loss: 0.6210139393806458, R2 Score (Test): 0.750686805858242, RMSE (Test): 0.7832018136978149\n",
      "Epoch [3276/5000], Train Loss: 0.006362453491116564, R2 Score (Train): 0.9977391642832173, RMSE (Train): 0.08099361509084702\n",
      "Test Loss: 0.6591582298278809, R2 Score (Test): 0.7367494412888176, RMSE (Test): 0.8047958016395569\n",
      "Epoch [3281/5000], Train Loss: 0.006290138931944966, R2 Score (Train): 0.9968795680359337, RMSE (Train): 0.09999138861894608\n",
      "Test Loss: 0.6436838209629059, R2 Score (Test): 0.7418649153890671, RMSE (Test): 0.7969380617141724\n",
      "Epoch [3286/5000], Train Loss: 0.004873372265137732, R2 Score (Train): 0.9979220240889016, RMSE (Train): 0.0743643119931221\n",
      "Test Loss: 0.6472499072551727, R2 Score (Test): 0.7407560709974339, RMSE (Test): 0.7986478805541992\n",
      "Epoch [3291/5000], Train Loss: 0.005305336477855842, R2 Score (Train): 0.9972865800120522, RMSE (Train): 0.08728855848312378\n",
      "Test Loss: 0.6439778804779053, R2 Score (Test): 0.7400361916178622, RMSE (Test): 0.7997559309005737\n",
      "Epoch [3296/5000], Train Loss: 0.0037119323193716505, R2 Score (Train): 0.9978910915594966, RMSE (Train): 0.07713479548692703\n",
      "Test Loss: 0.6532089710235596, R2 Score (Test): 0.7363829732168012, RMSE (Test): 0.8053557872772217\n",
      "Epoch [3301/5000], Train Loss: 0.0033290445571765304, R2 Score (Train): 0.9983646764822014, RMSE (Train): 0.07339753955602646\n",
      "Test Loss: 0.6500416100025177, R2 Score (Test): 0.7395074396335082, RMSE (Test): 0.8005688786506653\n",
      "Epoch [3306/5000], Train Loss: 0.0037163650461783013, R2 Score (Train): 0.9973891125656715, RMSE (Train): 0.08320258557796478\n",
      "Test Loss: 0.6608209908008575, R2 Score (Test): 0.7346912292243151, RMSE (Test): 0.8079358339309692\n",
      "Epoch [3311/5000], Train Loss: 0.005596800086398919, R2 Score (Train): 0.9985520697486294, RMSE (Train): 0.06329687684774399\n",
      "Test Loss: 0.6643829643726349, R2 Score (Test): 0.7318512004548101, RMSE (Test): 0.8122485876083374\n",
      "Epoch [3316/5000], Train Loss: 0.004845298477448523, R2 Score (Train): 0.9976979135804006, RMSE (Train): 0.08304505795240402\n",
      "Test Loss: 0.6480522751808167, R2 Score (Test): 0.738319040079282, RMSE (Test): 0.8023929595947266\n",
      "Epoch [3321/5000], Train Loss: 0.004288939448694388, R2 Score (Train): 0.9977765432956628, RMSE (Train): 0.07589986175298691\n",
      "Test Loss: 0.6403425633907318, R2 Score (Test): 0.7423736131933847, RMSE (Test): 0.7961524128913879\n",
      "Epoch [3326/5000], Train Loss: 0.005889524628097813, R2 Score (Train): 0.9982801867951442, RMSE (Train): 0.06966191530227661\n",
      "Test Loss: 0.6455330550670624, R2 Score (Test): 0.7400899097329547, RMSE (Test): 0.7996733784675598\n",
      "Epoch [3331/5000], Train Loss: 0.005988961396118005, R2 Score (Train): 0.9975920100726853, RMSE (Train): 0.07601096481084824\n",
      "Test Loss: 0.6353519558906555, R2 Score (Test): 0.7444230453779592, RMSE (Test): 0.79297935962677\n",
      "Epoch [3336/5000], Train Loss: 0.005402744592477878, R2 Score (Train): 0.9983566479544859, RMSE (Train): 0.0697091594338417\n",
      "Test Loss: 0.6633166968822479, R2 Score (Test): 0.732275858764837, RMSE (Test): 0.8116051554679871\n",
      "Epoch [3341/5000], Train Loss: 0.006833992199972272, R2 Score (Train): 0.9976375438721677, RMSE (Train): 0.07875768840312958\n",
      "Test Loss: 0.6327515244483948, R2 Score (Test): 0.7457580524744081, RMSE (Test): 0.7909055948257446\n",
      "Epoch [3346/5000], Train Loss: 0.007367730528737108, R2 Score (Train): 0.9952016737285605, RMSE (Train): 0.11651434749364853\n",
      "Test Loss: 0.6479350328445435, R2 Score (Test): 0.7387699258782224, RMSE (Test): 0.8017013669013977\n",
      "Epoch [3351/5000], Train Loss: 0.006456553392733137, R2 Score (Train): 0.9974536847877699, RMSE (Train): 0.07770003378391266\n",
      "Test Loss: 0.6504122018814087, R2 Score (Test): 0.7375369066502399, RMSE (Test): 0.8035911321640015\n",
      "Epoch [3356/5000], Train Loss: 0.0050849305698648095, R2 Score (Train): 0.9977833138619583, RMSE (Train): 0.08309492468833923\n",
      "Test Loss: 0.6641715168952942, R2 Score (Test): 0.734775349846427, RMSE (Test): 0.8078077435493469\n",
      "Epoch [3361/5000], Train Loss: 0.005115842912346125, R2 Score (Train): 0.9978557775108662, RMSE (Train): 0.08239874243736267\n",
      "Test Loss: 0.6328732967376709, R2 Score (Test): 0.7444999004485131, RMSE (Test): 0.7928601503372192\n",
      "Epoch [3366/5000], Train Loss: 0.006950240232981741, R2 Score (Train): 0.9983237500876856, RMSE (Train): 0.067940354347229\n",
      "Test Loss: 0.6362578868865967, R2 Score (Test): 0.7433249413662227, RMSE (Test): 0.7946811318397522\n",
      "Epoch [3371/5000], Train Loss: 0.007121428692092498, R2 Score (Train): 0.9953799463778217, RMSE (Train): 0.10478432476520538\n",
      "Test Loss: 0.6337986886501312, R2 Score (Test): 0.7428575234555641, RMSE (Test): 0.7954043745994568\n",
      "Epoch [3376/5000], Train Loss: 0.00963439807916681, R2 Score (Train): 0.9948257588697315, RMSE (Train): 0.10986591875553131\n",
      "Test Loss: 0.6379361748695374, R2 Score (Test): 0.7422858387361599, RMSE (Test): 0.796288013458252\n",
      "Epoch [3381/5000], Train Loss: 0.006149157881736755, R2 Score (Train): 0.9976856395236405, RMSE (Train): 0.07994583249092102\n",
      "Test Loss: 0.628885954618454, R2 Score (Test): 0.7481940792700281, RMSE (Test): 0.7871074676513672\n",
      "Epoch [3386/5000], Train Loss: 0.0055936279240995646, R2 Score (Train): 0.9967281266209936, RMSE (Train): 0.08558543771505356\n",
      "Test Loss: 0.623414546251297, R2 Score (Test): 0.7486342601633524, RMSE (Test): 0.786419153213501\n",
      "Epoch [3391/5000], Train Loss: 0.006743294419720769, R2 Score (Train): 0.9971486924355375, RMSE (Train): 0.08327904343605042\n",
      "Test Loss: 0.6449566185474396, R2 Score (Test): 0.7418216169135986, RMSE (Test): 0.7970048189163208\n",
      "Epoch [3396/5000], Train Loss: 0.0064405643691619234, R2 Score (Train): 0.9978036294800805, RMSE (Train): 0.08563850075006485\n",
      "Test Loss: 0.6533177196979523, R2 Score (Test): 0.7389854917469568, RMSE (Test): 0.8013705015182495\n",
      "Epoch [3401/5000], Train Loss: 0.008644444402307272, R2 Score (Train): 0.9965640446037628, RMSE (Train): 0.10021466016769409\n",
      "Test Loss: 0.6324810981750488, R2 Score (Test): 0.7440417184555073, RMSE (Test): 0.7935706973075867\n",
      "Epoch [3406/5000], Train Loss: 0.0058313154537851615, R2 Score (Train): 0.9977103885177819, RMSE (Train): 0.08399740606546402\n",
      "Test Loss: 0.6553164422512054, R2 Score (Test): 0.7356635379162593, RMSE (Test): 0.8064540028572083\n",
      "Epoch [3411/5000], Train Loss: 0.006349708108852307, R2 Score (Train): 0.9977237471945873, RMSE (Train): 0.09188869595527649\n",
      "Test Loss: 0.652504175901413, R2 Score (Test): 0.7392750054310593, RMSE (Test): 0.8009259104728699\n",
      "Epoch [3416/5000], Train Loss: 0.0043516329023987055, R2 Score (Train): 0.9981561500932734, RMSE (Train): 0.07568538188934326\n",
      "Test Loss: 0.6546655595302582, R2 Score (Test): 0.736658365112328, RMSE (Test): 0.8049350380897522\n",
      "Epoch [3421/5000], Train Loss: 0.006785016041249037, R2 Score (Train): 0.9986559769981758, RMSE (Train): 0.060868680477142334\n",
      "Test Loss: 0.6527267694473267, R2 Score (Test): 0.7372922439814549, RMSE (Test): 0.8039656281471252\n",
      "Epoch [3426/5000], Train Loss: 0.003075100559120377, R2 Score (Train): 0.9984454925206973, RMSE (Train): 0.07335489988327026\n",
      "Test Loss: 0.6475641429424286, R2 Score (Test): 0.7393524925204991, RMSE (Test): 0.8008069396018982\n",
      "Epoch [3431/5000], Train Loss: 0.006714690010994673, R2 Score (Train): 0.9965020452303148, RMSE (Train): 0.10052865743637085\n",
      "Test Loss: 0.6429556012153625, R2 Score (Test): 0.7411485081650774, RMSE (Test): 0.7980431318283081\n",
      "Epoch [3436/5000], Train Loss: 0.005473442142829299, R2 Score (Train): 0.9974636986458513, RMSE (Train): 0.07807371020317078\n",
      "Test Loss: 0.6383086740970612, R2 Score (Test): 0.7415695973005709, RMSE (Test): 0.797393798828125\n",
      "Epoch [3441/5000], Train Loss: 0.0055802828865125775, R2 Score (Train): 0.997546568780875, RMSE (Train): 0.08779900521039963\n",
      "Test Loss: 0.6365760862827301, R2 Score (Test): 0.7437242032353018, RMSE (Test): 0.7940627932548523\n",
      "Epoch [3446/5000], Train Loss: 0.0058302961600323515, R2 Score (Train): 0.9970123086956768, RMSE (Train): 0.08908194303512573\n",
      "Test Loss: 0.6487817764282227, R2 Score (Test): 0.7375464366262652, RMSE (Test): 0.803576648235321\n",
      "Epoch [3451/5000], Train Loss: 0.005304580709586541, R2 Score (Train): 0.9987804624651798, RMSE (Train): 0.06529550999403\n",
      "Test Loss: 0.6498437821865082, R2 Score (Test): 0.7390173469549666, RMSE (Test): 0.8013216257095337\n",
      "Epoch [3456/5000], Train Loss: 0.005265509632105629, R2 Score (Train): 0.9979542062837863, RMSE (Train): 0.07327631115913391\n",
      "Test Loss: 0.6591764390468597, R2 Score (Test): 0.7349880708210205, RMSE (Test): 0.8074837327003479\n",
      "Epoch [3461/5000], Train Loss: 0.004792122868821025, R2 Score (Train): 0.9962581860238706, RMSE (Train): 0.1026049256324768\n",
      "Test Loss: 0.657459020614624, R2 Score (Test): 0.736025884901734, RMSE (Test): 0.805901050567627\n",
      "Epoch [3466/5000], Train Loss: 0.006668302036511402, R2 Score (Train): 0.994392214110959, RMSE (Train): 0.11477085947990417\n",
      "Test Loss: 0.6587365865707397, R2 Score (Test): 0.7374069821796893, RMSE (Test): 0.8037900328636169\n",
      "Epoch [3471/5000], Train Loss: 0.006702510562414925, R2 Score (Train): 0.9942429370987245, RMSE (Train): 0.12597759068012238\n",
      "Test Loss: 0.6407486200332642, R2 Score (Test): 0.7413823221445675, RMSE (Test): 0.7976826429367065\n",
      "Epoch [3476/5000], Train Loss: 0.0062902073841542006, R2 Score (Train): 0.9981700885772868, RMSE (Train): 0.069138303399086\n",
      "Test Loss: 0.660992294549942, R2 Score (Test): 0.7349323966052621, RMSE (Test): 0.8075684905052185\n",
      "Epoch [3481/5000], Train Loss: 0.004812669241800904, R2 Score (Train): 0.9977731697472763, RMSE (Train): 0.06920938938856125\n",
      "Test Loss: 0.6462176144123077, R2 Score (Test): 0.7438838648624579, RMSE (Test): 0.7938153743743896\n",
      "Epoch [3486/5000], Train Loss: 0.006606668389091889, R2 Score (Train): 0.9984994938912354, RMSE (Train): 0.07066905498504639\n",
      "Test Loss: 0.6419697999954224, R2 Score (Test): 0.7449425645739187, RMSE (Test): 0.7921730279922485\n",
      "Epoch [3491/5000], Train Loss: 0.00654997971529762, R2 Score (Train): 0.9958259161388316, RMSE (Train): 0.10317149013280869\n",
      "Test Loss: 0.6458187103271484, R2 Score (Test): 0.739700164291746, RMSE (Test): 0.8002727627754211\n",
      "Epoch [3496/5000], Train Loss: 0.007940254096562663, R2 Score (Train): 0.9961681682179226, RMSE (Train): 0.09733869135379791\n",
      "Test Loss: 0.6445827484130859, R2 Score (Test): 0.7414592194918246, RMSE (Test): 0.7975640296936035\n",
      "Epoch [3501/5000], Train Loss: 0.010699335485696793, R2 Score (Train): 0.9961437799876542, RMSE (Train): 0.08315523713827133\n",
      "Test Loss: 0.6345449090003967, R2 Score (Test): 0.7439584818522798, RMSE (Test): 0.7936997413635254\n",
      "Epoch [3506/5000], Train Loss: 0.006807773917292555, R2 Score (Train): 0.9982201037375015, RMSE (Train): 0.07776778191328049\n",
      "Test Loss: 0.6280966401100159, R2 Score (Test): 0.7457755942927606, RMSE (Test): 0.7908782958984375\n",
      "Epoch [3511/5000], Train Loss: 0.005550944129936397, R2 Score (Train): 0.9983883770471219, RMSE (Train): 0.06934016197919846\n",
      "Test Loss: 0.64430370926857, R2 Score (Test): 0.7443314386904165, RMSE (Test): 0.7931215167045593\n",
      "Epoch [3516/5000], Train Loss: 0.006128980778157711, R2 Score (Train): 0.9958726514151771, RMSE (Train): 0.10678760707378387\n",
      "Test Loss: 0.6261197328567505, R2 Score (Test): 0.7487646828943968, RMSE (Test): 0.7862151265144348\n",
      "Epoch [3521/5000], Train Loss: 0.0043152162882809835, R2 Score (Train): 0.9972187909128892, RMSE (Train): 0.08295663446187973\n",
      "Test Loss: 0.6495771408081055, R2 Score (Test): 0.7417989124427644, RMSE (Test): 0.7970399260520935\n",
      "Epoch [3526/5000], Train Loss: 0.004299291176721454, R2 Score (Train): 0.9977717055589955, RMSE (Train): 0.07688898593187332\n",
      "Test Loss: 0.6509864628314972, R2 Score (Test): 0.7410962862783318, RMSE (Test): 0.7981236577033997\n",
      "Epoch [3531/5000], Train Loss: 0.005249781029609342, R2 Score (Train): 0.9983906900373292, RMSE (Train): 0.0690123438835144\n",
      "Test Loss: 0.6488492786884308, R2 Score (Test): 0.7401728966502319, RMSE (Test): 0.7995456457138062\n",
      "Epoch [3536/5000], Train Loss: 0.00371067652789255, R2 Score (Train): 0.9981353726634918, RMSE (Train): 0.08042526245117188\n",
      "Test Loss: 0.6511971354484558, R2 Score (Test): 0.7383680396360472, RMSE (Test): 0.8023177981376648\n",
      "Epoch [3541/5000], Train Loss: 0.004598859076698621, R2 Score (Train): 0.9980591683690415, RMSE (Train): 0.07487649470567703\n",
      "Test Loss: 0.6406205892562866, R2 Score (Test): 0.7452517898555038, RMSE (Test): 0.7916926145553589\n",
      "Epoch [3546/5000], Train Loss: 0.00587454733128349, R2 Score (Train): 0.9987957954644917, RMSE (Train): 0.05591202899813652\n",
      "Test Loss: 0.6675564348697662, R2 Score (Test): 0.7350316606042255, RMSE (Test): 0.8074172735214233\n",
      "Epoch [3551/5000], Train Loss: 0.005655275657773018, R2 Score (Train): 0.9984279063331998, RMSE (Train): 0.07747688889503479\n",
      "Test Loss: 0.6406689882278442, R2 Score (Test): 0.7456152473909379, RMSE (Test): 0.7911276817321777\n",
      "Epoch [3556/5000], Train Loss: 0.007444026065059006, R2 Score (Train): 0.9967870747016905, RMSE (Train): 0.10483133047819138\n",
      "Test Loss: 0.6412132680416107, R2 Score (Test): 0.7412271053580171, RMSE (Test): 0.7979220151901245\n",
      "Epoch [3561/5000], Train Loss: 0.006444613992546995, R2 Score (Train): 0.9976301412982358, RMSE (Train): 0.0770774632692337\n",
      "Test Loss: 0.6466735303401947, R2 Score (Test): 0.7396471592181229, RMSE (Test): 0.8003541231155396\n",
      "Epoch [3566/5000], Train Loss: 0.005188965083410342, R2 Score (Train): 0.99796994218704, RMSE (Train): 0.08053205907344818\n",
      "Test Loss: 0.6492281556129456, R2 Score (Test): 0.7393281845922755, RMSE (Test): 0.8008443117141724\n",
      "Epoch [3571/5000], Train Loss: 0.003706235632610818, R2 Score (Train): 0.998430613857227, RMSE (Train): 0.07708533853292465\n",
      "Test Loss: 0.6454991102218628, R2 Score (Test): 0.7397549210258239, RMSE (Test): 0.8001884818077087\n",
      "Epoch [3576/5000], Train Loss: 0.004664012735399107, R2 Score (Train): 0.9985684694976125, RMSE (Train): 0.0693761482834816\n",
      "Test Loss: 0.6315649151802063, R2 Score (Test): 0.7465429751015118, RMSE (Test): 0.7896837592124939\n",
      "Epoch [3581/5000], Train Loss: 0.0031960639947404466, R2 Score (Train): 0.9979313524675647, RMSE (Train): 0.07378556579351425\n",
      "Test Loss: 0.6455439627170563, R2 Score (Test): 0.7412056415965225, RMSE (Test): 0.7979551553726196\n",
      "Epoch [3586/5000], Train Loss: 0.00384252219616125, R2 Score (Train): 0.9980632278077501, RMSE (Train): 0.06157483160495758\n",
      "Test Loss: 0.6695687770843506, R2 Score (Test): 0.7312423273249116, RMSE (Test): 0.8131702542304993\n",
      "Epoch [3591/5000], Train Loss: 0.004825487771692376, R2 Score (Train): 0.9980759211399257, RMSE (Train): 0.07663342356681824\n",
      "Test Loss: 0.6469118595123291, R2 Score (Test): 0.7401273984907162, RMSE (Test): 0.7996156811714172\n",
      "Epoch [3596/5000], Train Loss: 0.0056588345517714815, R2 Score (Train): 0.9979139934235424, RMSE (Train): 0.07395058125257492\n",
      "Test Loss: 0.6597272157669067, R2 Score (Test): 0.7358936248379422, RMSE (Test): 0.8061028718948364\n",
      "Epoch [3601/5000], Train Loss: 0.006208401561404268, R2 Score (Train): 0.9983801382393225, RMSE (Train): 0.07229860872030258\n",
      "Test Loss: 0.6394464075565338, R2 Score (Test): 0.7443820562840978, RMSE (Test): 0.7930429577827454\n",
      "Epoch [3606/5000], Train Loss: 0.005407580717777212, R2 Score (Train): 0.9962802324894546, RMSE (Train): 0.09279191493988037\n",
      "Test Loss: 0.6458578109741211, R2 Score (Test): 0.7407135398852265, RMSE (Test): 0.7987133860588074\n",
      "Epoch [3611/5000], Train Loss: 0.005867770950620373, R2 Score (Train): 0.9977769007657563, RMSE (Train): 0.06774958223104477\n",
      "Test Loss: 0.6696859002113342, R2 Score (Test): 0.7323273168933424, RMSE (Test): 0.8115271329879761\n",
      "Epoch [3616/5000], Train Loss: 0.003937038515383999, R2 Score (Train): 0.9977763048830047, RMSE (Train): 0.07756588608026505\n",
      "Test Loss: 0.6625371277332306, R2 Score (Test): 0.7330720366296775, RMSE (Test): 0.8103974461555481\n",
      "Epoch [3621/5000], Train Loss: 0.004306122854662438, R2 Score (Train): 0.9978986713735563, RMSE (Train): 0.06861036270856857\n",
      "Test Loss: 0.6537930369377136, R2 Score (Test): 0.7374206682425233, RMSE (Test): 0.803769052028656\n",
      "Epoch [3626/5000], Train Loss: 0.0045661263866350055, R2 Score (Train): 0.9978395600961003, RMSE (Train): 0.08169349282979965\n",
      "Test Loss: 0.641465425491333, R2 Score (Test): 0.7432407347512668, RMSE (Test): 0.7948114275932312\n",
      "Epoch [3631/5000], Train Loss: 0.00488388747908175, R2 Score (Train): 0.9974279673454494, RMSE (Train): 0.07501894980669022\n",
      "Test Loss: 0.6670997440814972, R2 Score (Test): 0.736830382203167, RMSE (Test): 0.8046720623970032\n",
      "Epoch [3636/5000], Train Loss: 0.005364626335601012, R2 Score (Train): 0.9981399870387457, RMSE (Train): 0.07410906255245209\n",
      "Test Loss: 0.6526668965816498, R2 Score (Test): 0.7381802942350284, RMSE (Test): 0.8026056885719299\n",
      "Epoch [3641/5000], Train Loss: 0.006109645240940154, R2 Score (Train): 0.9977291410714546, RMSE (Train): 0.08036737143993378\n",
      "Test Loss: 0.6609470248222351, R2 Score (Test): 0.7370454545239431, RMSE (Test): 0.8043432235717773\n",
      "Epoch [3646/5000], Train Loss: 0.0053667775355279446, R2 Score (Train): 0.997372193184803, RMSE (Train): 0.08327571302652359\n",
      "Test Loss: 0.6323631703853607, R2 Score (Test): 0.7438890627218918, RMSE (Test): 0.793807327747345\n",
      "Epoch [3651/5000], Train Loss: 0.0067966442632799344, R2 Score (Train): 0.9973483012892602, RMSE (Train): 0.08207263052463531\n",
      "Test Loss: 0.6469510495662689, R2 Score (Test): 0.742234913283233, RMSE (Test): 0.7963667511940002\n",
      "Epoch [3656/5000], Train Loss: 0.008585477247834206, R2 Score (Train): 0.9973013151101885, RMSE (Train): 0.083863765001297\n",
      "Test Loss: 0.64263916015625, R2 Score (Test): 0.7400814216762845, RMSE (Test): 0.7996863722801208\n",
      "Epoch [3661/5000], Train Loss: 0.009369995988284549, R2 Score (Train): 0.9984059578044245, RMSE (Train): 0.06778658926486969\n",
      "Test Loss: 0.6521725356578827, R2 Score (Test): 0.7389824463188963, RMSE (Test): 0.8013752102851868\n",
      "Epoch [3666/5000], Train Loss: 0.005732467126411696, R2 Score (Train): 0.9983963831530266, RMSE (Train): 0.0707300454378128\n",
      "Test Loss: 0.6418946385383606, R2 Score (Test): 0.7421988571215927, RMSE (Test): 0.7964223623275757\n",
      "Epoch [3671/5000], Train Loss: 0.0068709928697596, R2 Score (Train): 0.9951106819797074, RMSE (Train): 0.11361386626958847\n",
      "Test Loss: 0.64705029129982, R2 Score (Test): 0.7422107867169816, RMSE (Test): 0.7964040040969849\n",
      "Epoch [3676/5000], Train Loss: 0.007472769978145759, R2 Score (Train): 0.9951964005774913, RMSE (Train): 0.11075720936059952\n",
      "Test Loss: 0.6702567338943481, R2 Score (Test): 0.732398396095556, RMSE (Test): 0.8114194869995117\n",
      "Epoch [3681/5000], Train Loss: 0.004775519482791424, R2 Score (Train): 0.997996171073226, RMSE (Train): 0.06876137852668762\n",
      "Test Loss: 0.6538037657737732, R2 Score (Test): 0.7373616655964048, RMSE (Test): 0.8038594126701355\n",
      "Epoch [3686/5000], Train Loss: 0.006159203437467416, R2 Score (Train): 0.9980341311887673, RMSE (Train): 0.0738079622387886\n",
      "Test Loss: 0.6519795656204224, R2 Score (Test): 0.7415925710263411, RMSE (Test): 0.7973583936691284\n",
      "Epoch [3691/5000], Train Loss: 0.008771895663812757, R2 Score (Train): 0.9980472278722845, RMSE (Train): 0.0792190209031105\n",
      "Test Loss: 0.6575274467468262, R2 Score (Test): 0.7395867318487086, RMSE (Test): 0.8004469871520996\n",
      "Epoch [3696/5000], Train Loss: 0.005535205438112219, R2 Score (Train): 0.9965019520258366, RMSE (Train): 0.11353606730699539\n",
      "Test Loss: 0.6374006867408752, R2 Score (Test): 0.7455544289646527, RMSE (Test): 0.7912222743034363\n",
      "Epoch [3701/5000], Train Loss: 0.005903898195053141, R2 Score (Train): 0.9984828286148425, RMSE (Train): 0.06652908027172089\n",
      "Test Loss: 0.6633870899677277, R2 Score (Test): 0.7352878561951874, RMSE (Test): 0.8070268630981445\n",
      "Epoch [3706/5000], Train Loss: 0.004788386131015916, R2 Score (Train): 0.998196441448277, RMSE (Train): 0.06850011646747589\n",
      "Test Loss: 0.6491542756557465, R2 Score (Test): 0.738683086783786, RMSE (Test): 0.8018346428871155\n",
      "Epoch [3711/5000], Train Loss: 0.006506562155360977, R2 Score (Train): 0.9971456205658784, RMSE (Train): 0.09014904499053955\n",
      "Test Loss: 0.6388958096504211, R2 Score (Test): 0.7458408790519158, RMSE (Test): 0.790776789188385\n",
      "Epoch [3716/5000], Train Loss: 0.006957249172652761, R2 Score (Train): 0.9985486559970613, RMSE (Train): 0.06422468274831772\n",
      "Test Loss: 0.640322208404541, R2 Score (Test): 0.7433622116983312, RMSE (Test): 0.7946234345436096\n",
      "Epoch [3721/5000], Train Loss: 0.004896918466935555, R2 Score (Train): 0.9985559375051132, RMSE (Train): 0.0579703114926815\n",
      "Test Loss: 0.6498309969902039, R2 Score (Test): 0.740044527571035, RMSE (Test): 0.799743115901947\n",
      "Epoch [3726/5000], Train Loss: 0.004373943937631945, R2 Score (Train): 0.9984058049613864, RMSE (Train): 0.06762908399105072\n",
      "Test Loss: 0.633763313293457, R2 Score (Test): 0.7467910048020918, RMSE (Test): 0.789297342300415\n",
      "Epoch [3731/5000], Train Loss: 0.006097169515366356, R2 Score (Train): 0.997471768041913, RMSE (Train): 0.09162206202745438\n",
      "Test Loss: 0.6506204605102539, R2 Score (Test): 0.7415301873552429, RMSE (Test): 0.7974545359611511\n",
      "Epoch [3736/5000], Train Loss: 0.005890604419012864, R2 Score (Train): 0.9977617797078346, RMSE (Train): 0.07103558629751205\n",
      "Test Loss: 0.6551021635532379, R2 Score (Test): 0.7389091396758428, RMSE (Test): 0.8014878034591675\n",
      "Epoch [3741/5000], Train Loss: 0.009374679764732718, R2 Score (Train): 0.9970275476774748, RMSE (Train): 0.08585726469755173\n",
      "Test Loss: 0.6583910584449768, R2 Score (Test): 0.7396613057492873, RMSE (Test): 0.8003324270248413\n",
      "Epoch [3746/5000], Train Loss: 0.005187606710630159, R2 Score (Train): 0.997465712080511, RMSE (Train): 0.08204744011163712\n",
      "Test Loss: 0.6398846805095673, R2 Score (Test): 0.7416176818971951, RMSE (Test): 0.7973195910453796\n",
      "Epoch [3751/5000], Train Loss: 0.005914420587942004, R2 Score (Train): 0.9975719372260168, RMSE (Train): 0.08047006279230118\n",
      "Test Loss: 0.651416152715683, R2 Score (Test): 0.7388932977874358, RMSE (Test): 0.8015120625495911\n",
      "Epoch [3756/5000], Train Loss: 0.005972557972806196, R2 Score (Train): 0.9975485470467411, RMSE (Train): 0.08286780118942261\n",
      "Test Loss: 0.6687808036804199, R2 Score (Test): 0.7335304538816887, RMSE (Test): 0.8097012639045715\n",
      "Epoch [3761/5000], Train Loss: 0.004301370975250999, R2 Score (Train): 0.9976726020124347, RMSE (Train): 0.08117858320474625\n",
      "Test Loss: 0.6615177094936371, R2 Score (Test): 0.7363637035451243, RMSE (Test): 0.8053852319717407\n",
      "Epoch [3766/5000], Train Loss: 0.007031719433143735, R2 Score (Train): 0.9961843790338571, RMSE (Train): 0.09704410284757614\n",
      "Test Loss: 0.6642845869064331, R2 Score (Test): 0.7355485355354452, RMSE (Test): 0.8066293001174927\n",
      "Epoch [3771/5000], Train Loss: 0.008112610938648382, R2 Score (Train): 0.9963667387869621, RMSE (Train): 0.09824153780937195\n",
      "Test Loss: 0.625944048166275, R2 Score (Test): 0.7472508109328997, RMSE (Test): 0.7885802984237671\n",
      "Epoch [3776/5000], Train Loss: 0.0052941964628795786, R2 Score (Train): 0.9980646089104648, RMSE (Train): 0.06302279233932495\n",
      "Test Loss: 0.6519849598407745, R2 Score (Test): 0.740710665557779, RMSE (Test): 0.7987178564071655\n",
      "Epoch [3781/5000], Train Loss: 0.0054877787673225, R2 Score (Train): 0.9973441149629819, RMSE (Train): 0.07676716893911362\n",
      "Test Loss: 0.6433286070823669, R2 Score (Test): 0.7455947763935656, RMSE (Test): 0.7911595106124878\n",
      "Epoch [3786/5000], Train Loss: 0.005311811498055856, R2 Score (Train): 0.9970247842376349, RMSE (Train): 0.09374947845935822\n",
      "Test Loss: 0.6467658281326294, R2 Score (Test): 0.7450810626425075, RMSE (Test): 0.7919579148292542\n",
      "Epoch [3791/5000], Train Loss: 0.006516223618139823, R2 Score (Train): 0.997310994779363, RMSE (Train): 0.08082712441682816\n",
      "Test Loss: 0.6301704049110413, R2 Score (Test): 0.7496779663564892, RMSE (Test): 0.784784734249115\n",
      "Epoch [3796/5000], Train Loss: 0.006358119038244088, R2 Score (Train): 0.9983348863931231, RMSE (Train): 0.06451842188835144\n",
      "Test Loss: 0.6392905414104462, R2 Score (Test): 0.7454904598210791, RMSE (Test): 0.7913216948509216\n",
      "Epoch [3801/5000], Train Loss: 0.005593574761102597, R2 Score (Train): 0.9982464260192214, RMSE (Train): 0.07294749468564987\n",
      "Test Loss: 0.625732421875, R2 Score (Test): 0.7520206677150937, RMSE (Test): 0.7811039090156555\n",
      "Epoch [3806/5000], Train Loss: 0.005018416365298132, R2 Score (Train): 0.9976733282629225, RMSE (Train): 0.08909922093153\n",
      "Test Loss: 0.6522380113601685, R2 Score (Test): 0.7417717240437833, RMSE (Test): 0.7970818877220154\n",
      "Epoch [3811/5000], Train Loss: 0.003992282591449718, R2 Score (Train): 0.9978632307906781, RMSE (Train): 0.07400604337453842\n",
      "Test Loss: 0.6453340649604797, R2 Score (Test): 0.7441458037058277, RMSE (Test): 0.7934093475341797\n",
      "Epoch [3816/5000], Train Loss: 0.0038286239723674953, R2 Score (Train): 0.9976665194193886, RMSE (Train): 0.06847335398197174\n",
      "Test Loss: 0.6417155861854553, R2 Score (Test): 0.7445462558255436, RMSE (Test): 0.7927882075309753\n",
      "Epoch [3821/5000], Train Loss: 0.003281597358485063, R2 Score (Train): 0.9978438599329859, RMSE (Train): 0.07958877831697464\n",
      "Test Loss: 0.6398649513721466, R2 Score (Test): 0.7438221705778044, RMSE (Test): 0.7939109802246094\n",
      "Epoch [3826/5000], Train Loss: 0.004865801272292932, R2 Score (Train): 0.9972393095429243, RMSE (Train): 0.081842802464962\n",
      "Test Loss: 0.6530871689319611, R2 Score (Test): 0.7389206301567721, RMSE (Test): 0.8014701008796692\n",
      "Epoch [3831/5000], Train Loss: 0.0057234906901915865, R2 Score (Train): 0.9981661088365744, RMSE (Train): 0.07447919249534607\n",
      "Test Loss: 0.6581766307353973, R2 Score (Test): 0.7367183776038552, RMSE (Test): 0.8048432469367981\n",
      "Epoch [3836/5000], Train Loss: 0.004631874112722774, R2 Score (Train): 0.9987659517124076, RMSE (Train): 0.06857778877019882\n",
      "Test Loss: 0.6629516780376434, R2 Score (Test): 0.7365215245052952, RMSE (Test): 0.8051440715789795\n",
      "Epoch [3841/5000], Train Loss: 0.004648567837042113, R2 Score (Train): 0.9987674562315176, RMSE (Train): 0.06511975079774857\n",
      "Test Loss: 0.6624034345149994, R2 Score (Test): 0.7358324477616236, RMSE (Test): 0.8061962127685547\n",
      "Epoch [3846/5000], Train Loss: 0.004192789356845121, R2 Score (Train): 0.9985665900525804, RMSE (Train): 0.06596597284078598\n",
      "Test Loss: 0.660557359457016, R2 Score (Test): 0.7350939966157883, RMSE (Test): 0.8073222637176514\n",
      "Epoch [3851/5000], Train Loss: 0.006252088816836476, R2 Score (Train): 0.9949965961664904, RMSE (Train): 0.10780160129070282\n",
      "Test Loss: 0.6421582698822021, R2 Score (Test): 0.7424402981553393, RMSE (Test): 0.7960492968559265\n",
      "Epoch [3856/5000], Train Loss: 0.00492475046000133, R2 Score (Train): 0.998584520343334, RMSE (Train): 0.06672555953264236\n",
      "Test Loss: 0.6585924923419952, R2 Score (Test): 0.7373565891313432, RMSE (Test): 0.8038672208786011\n",
      "Epoch [3861/5000], Train Loss: 0.005683788758081694, R2 Score (Train): 0.9973868925315564, RMSE (Train): 0.09010015428066254\n",
      "Test Loss: 0.6418034136295319, R2 Score (Test): 0.7429426696161774, RMSE (Test): 0.7952726483345032\n",
      "Epoch [3866/5000], Train Loss: 0.003964755334891379, R2 Score (Train): 0.997847343038475, RMSE (Train): 0.06619912385940552\n",
      "Test Loss: 0.6527662873268127, R2 Score (Test): 0.7377816434491783, RMSE (Test): 0.8032164573669434\n",
      "Epoch [3871/5000], Train Loss: 0.0049404200399294496, R2 Score (Train): 0.9976716804104563, RMSE (Train): 0.08203936368227005\n",
      "Test Loss: 0.6437488198280334, R2 Score (Test): 0.7435304295369345, RMSE (Test): 0.7943629622459412\n",
      "Epoch [3876/5000], Train Loss: 0.0046355171749989195, R2 Score (Train): 0.9985324287336915, RMSE (Train): 0.06456608325242996\n",
      "Test Loss: 0.6448202729225159, R2 Score (Test): 0.7421009454245706, RMSE (Test): 0.7965736389160156\n",
      "Epoch [3881/5000], Train Loss: 0.0038381334161385894, R2 Score (Train): 0.9972956074593889, RMSE (Train): 0.08546240627765656\n",
      "Test Loss: 0.6474749743938446, R2 Score (Test): 0.7418530234653216, RMSE (Test): 0.7969564199447632\n",
      "Epoch [3886/5000], Train Loss: 0.0060526754629487796, R2 Score (Train): 0.9971273587790251, RMSE (Train): 0.08900430053472519\n",
      "Test Loss: 0.6390605866909027, R2 Score (Test): 0.743619827409098, RMSE (Test): 0.7942244410514832\n",
      "Epoch [3891/5000], Train Loss: 0.007056132657453418, R2 Score (Train): 0.9977632699425322, RMSE (Train): 0.07742291688919067\n",
      "Test Loss: 0.6651671826839447, R2 Score (Test): 0.7343147360668556, RMSE (Test): 0.8085088133811951\n",
      "Epoch [3896/5000], Train Loss: 0.004809261959356566, R2 Score (Train): 0.9984238344984121, RMSE (Train): 0.07002289593219757\n",
      "Test Loss: 0.6648040115833282, R2 Score (Test): 0.7336222828541823, RMSE (Test): 0.8095617890357971\n",
      "Epoch [3901/5000], Train Loss: 0.0036175671266391873, R2 Score (Train): 0.9982674664497251, RMSE (Train): 0.06119231879711151\n",
      "Test Loss: 0.6700756549835205, R2 Score (Test): 0.734922649649687, RMSE (Test): 0.8075833320617676\n",
      "Epoch [3906/5000], Train Loss: 0.004437106855524083, R2 Score (Train): 0.997977571421913, RMSE (Train): 0.06933446228504181\n",
      "Test Loss: 0.6370695531368256, R2 Score (Test): 0.7448095906866221, RMSE (Test): 0.7923794388771057\n",
      "Epoch [3911/5000], Train Loss: 0.003860815311782062, R2 Score (Train): 0.9982825832219316, RMSE (Train): 0.0634109377861023\n",
      "Test Loss: 0.647638350725174, R2 Score (Test): 0.7414024147874493, RMSE (Test): 0.7976517081260681\n",
      "Epoch [3916/5000], Train Loss: 0.004618117624583344, R2 Score (Train): 0.9977355651898311, RMSE (Train): 0.07871066778898239\n",
      "Test Loss: 0.6581650972366333, R2 Score (Test): 0.739837442360568, RMSE (Test): 0.8000616431236267\n",
      "Epoch [3921/5000], Train Loss: 0.0048497710765029, R2 Score (Train): 0.9979163488164683, RMSE (Train): 0.0772862583398819\n",
      "Test Loss: 0.6605519950389862, R2 Score (Test): 0.7366886091625037, RMSE (Test): 0.8048887848854065\n",
      "Epoch [3926/5000], Train Loss: 0.004439962872614463, R2 Score (Train): 0.9973522973467785, RMSE (Train): 0.07578523457050323\n",
      "Test Loss: 0.6672450006008148, R2 Score (Test): 0.7378988860729381, RMSE (Test): 0.8030368685722351\n",
      "Epoch [3931/5000], Train Loss: 0.006718468774730961, R2 Score (Train): 0.9973658933634015, RMSE (Train): 0.07387493550777435\n",
      "Test Loss: 0.6580886542797089, R2 Score (Test): 0.7376139136279248, RMSE (Test): 0.8034732341766357\n",
      "Epoch [3936/5000], Train Loss: 0.005025330814532936, R2 Score (Train): 0.9982728784135043, RMSE (Train): 0.07652636617422104\n",
      "Test Loss: 0.6518254280090332, R2 Score (Test): 0.7416746532769022, RMSE (Test): 0.7972317337989807\n",
      "Epoch [3941/5000], Train Loss: 0.0076497924358894425, R2 Score (Train): 0.9978552547836953, RMSE (Train): 0.08160177618265152\n",
      "Test Loss: 0.65165975689888, R2 Score (Test): 0.7392192460256335, RMSE (Test): 0.8010115623474121\n",
      "Epoch [3946/5000], Train Loss: 0.0048636292728285, R2 Score (Train): 0.996011584609477, RMSE (Train): 0.09577952325344086\n",
      "Test Loss: 0.6580884456634521, R2 Score (Test): 0.7378388986552471, RMSE (Test): 0.803128719329834\n",
      "Epoch [3951/5000], Train Loss: 0.005792772785450022, R2 Score (Train): 0.9987354903088167, RMSE (Train): 0.054229333996772766\n",
      "Test Loss: 0.6557933688163757, R2 Score (Test): 0.7385730377841103, RMSE (Test): 0.8020034432411194\n",
      "Epoch [3956/5000], Train Loss: 0.011044358174937466, R2 Score (Train): 0.997943878426988, RMSE (Train): 0.0794752910733223\n",
      "Test Loss: 0.6704034507274628, R2 Score (Test): 0.7326015134875259, RMSE (Test): 0.8111114501953125\n",
      "Epoch [3961/5000], Train Loss: 0.010062104556709528, R2 Score (Train): 0.9961403322238629, RMSE (Train): 0.10464973002672195\n",
      "Test Loss: 0.6529457271099091, R2 Score (Test): 0.7373530789290001, RMSE (Test): 0.8038725852966309\n",
      "Epoch [3966/5000], Train Loss: 0.007649166199068229, R2 Score (Train): 0.9946225623689554, RMSE (Train): 0.11280031502246857\n",
      "Test Loss: 0.6078396737575531, R2 Score (Test): 0.7533510572716625, RMSE (Test): 0.779005765914917\n",
      "Epoch [3971/5000], Train Loss: 0.004870316130109131, R2 Score (Train): 0.997842716482222, RMSE (Train): 0.079325370490551\n",
      "Test Loss: 0.6572825610637665, R2 Score (Test): 0.7376174714227669, RMSE (Test): 0.803467869758606\n",
      "Epoch [3976/5000], Train Loss: 0.007732017353797953, R2 Score (Train): 0.995685698884109, RMSE (Train): 0.10923859477043152\n",
      "Test Loss: 0.6261226236820221, R2 Score (Test): 0.7466635177589513, RMSE (Test): 0.7894959449768066\n",
      "Epoch [3981/5000], Train Loss: 0.0050478968769311905, R2 Score (Train): 0.9980777551362908, RMSE (Train): 0.07988952845335007\n",
      "Test Loss: 0.644065648317337, R2 Score (Test): 0.7448409154566882, RMSE (Test): 0.792330801486969\n",
      "Epoch [3986/5000], Train Loss: 0.00978966266848147, R2 Score (Train): 0.9960039379790043, RMSE (Train): 0.09581733494997025\n",
      "Test Loss: 0.6419482231140137, R2 Score (Test): 0.7409992250952968, RMSE (Test): 0.7982732653617859\n",
      "Epoch [3991/5000], Train Loss: 0.005380982571902375, R2 Score (Train): 0.9981342240451587, RMSE (Train): 0.07743588835000992\n",
      "Test Loss: 0.627583384513855, R2 Score (Test): 0.7466734884232202, RMSE (Test): 0.789480447769165\n",
      "Epoch [3996/5000], Train Loss: 0.0041449402924627066, R2 Score (Train): 0.9972913380398948, RMSE (Train): 0.0788426548242569\n",
      "Test Loss: 0.6356644630432129, R2 Score (Test): 0.7436596048941857, RMSE (Test): 0.7941628694534302\n",
      "Epoch [4001/5000], Train Loss: 0.004375513565416138, R2 Score (Train): 0.9982762730034052, RMSE (Train): 0.07086299359798431\n",
      "Test Loss: 0.6305816173553467, R2 Score (Test): 0.7466476053481497, RMSE (Test): 0.789520800113678\n",
      "Epoch [4006/5000], Train Loss: 0.0049326207566385465, R2 Score (Train): 0.9977716124322537, RMSE (Train): 0.07151717692613602\n",
      "Test Loss: 0.6437077820301056, R2 Score (Test): 0.7428483888477344, RMSE (Test): 0.7954184412956238\n",
      "Epoch [4011/5000], Train Loss: 0.00464539978808413, R2 Score (Train): 0.9974023151305819, RMSE (Train): 0.06969806551933289\n",
      "Test Loss: 0.6306841671466827, R2 Score (Test): 0.7472734987776093, RMSE (Test): 0.7885449528694153\n",
      "Epoch [4016/5000], Train Loss: 0.004040910784776012, R2 Score (Train): 0.9966615464565689, RMSE (Train): 0.08887232840061188\n",
      "Test Loss: 0.6574684381484985, R2 Score (Test): 0.7370456850697955, RMSE (Test): 0.8043428063392639\n",
      "Epoch [4021/5000], Train Loss: 0.004407789907418191, R2 Score (Train): 0.9979752535816996, RMSE (Train): 0.07613251358270645\n",
      "Test Loss: 0.6549349129199982, R2 Score (Test): 0.7371665096919133, RMSE (Test): 0.8041580319404602\n",
      "Epoch [4026/5000], Train Loss: 0.0050674879069750505, R2 Score (Train): 0.9959273108167486, RMSE (Train): 0.09327585250139236\n",
      "Test Loss: 0.6512623131275177, R2 Score (Test): 0.742527644185992, RMSE (Test): 0.7959144115447998\n",
      "Epoch [4031/5000], Train Loss: 0.004017173972291251, R2 Score (Train): 0.9981032160708114, RMSE (Train): 0.06473027169704437\n",
      "Test Loss: 0.6473475694656372, R2 Score (Test): 0.74223280805253, RMSE (Test): 0.7963699698448181\n",
      "Epoch [4036/5000], Train Loss: 0.003627296226720015, R2 Score (Train): 0.9979442492905535, RMSE (Train): 0.07374179363250732\n",
      "Test Loss: 0.6620537042617798, R2 Score (Test): 0.7350476891990952, RMSE (Test): 0.8073928356170654\n",
      "Epoch [4041/5000], Train Loss: 0.004259498595880966, R2 Score (Train): 0.9981432795507675, RMSE (Train): 0.07039695233106613\n",
      "Test Loss: 0.6397811770439148, R2 Score (Test): 0.7439377997793979, RMSE (Test): 0.7937318086624146\n",
      "Epoch [4046/5000], Train Loss: 0.003516835432189206, R2 Score (Train): 0.9985970502831002, RMSE (Train): 0.06717481464147568\n",
      "Test Loss: 0.6569241881370544, R2 Score (Test): 0.7378334836780198, RMSE (Test): 0.8031370639801025\n",
      "Epoch [4051/5000], Train Loss: 0.00714066616880397, R2 Score (Train): 0.9940822951364021, RMSE (Train): 0.09968787431716919\n",
      "Test Loss: 0.6502013504505157, R2 Score (Test): 0.7414475531897716, RMSE (Test): 0.7975820302963257\n",
      "Epoch [4056/5000], Train Loss: 0.0046195540732393665, R2 Score (Train): 0.9976802115624028, RMSE (Train): 0.08291787654161453\n",
      "Test Loss: 0.6591609418392181, R2 Score (Test): 0.7364673462599738, RMSE (Test): 0.8052268624305725\n",
      "Epoch [4061/5000], Train Loss: 0.0040610758975769086, R2 Score (Train): 0.9967476120094776, RMSE (Train): 0.09306876361370087\n",
      "Test Loss: 0.6702257692813873, R2 Score (Test): 0.734027144659111, RMSE (Test): 0.8089463114738464\n",
      "Epoch [4066/5000], Train Loss: 0.006636791474496325, R2 Score (Train): 0.9983036072641432, RMSE (Train): 0.06571326404809952\n",
      "Test Loss: 0.6828009784221649, R2 Score (Test): 0.7275800109391588, RMSE (Test): 0.8186919689178467\n",
      "Epoch [4071/5000], Train Loss: 0.005673140365009506, R2 Score (Train): 0.9984007729212412, RMSE (Train): 0.06122032552957535\n",
      "Test Loss: 0.6807557344436646, R2 Score (Test): 0.7299230934341452, RMSE (Test): 0.8151636123657227\n",
      "Epoch [4076/5000], Train Loss: 0.005133637071897586, R2 Score (Train): 0.9974857967014414, RMSE (Train): 0.07891793549060822\n",
      "Test Loss: 0.6578832268714905, R2 Score (Test): 0.7384322351331187, RMSE (Test): 0.8022193908691406\n",
      "Epoch [4081/5000], Train Loss: 0.0051600820540140075, R2 Score (Train): 0.9968968952227378, RMSE (Train): 0.08980421721935272\n",
      "Test Loss: 0.6535547971725464, R2 Score (Test): 0.7381308023293913, RMSE (Test): 0.8026815056800842\n",
      "Epoch [4086/5000], Train Loss: 0.006209080883612235, R2 Score (Train): 0.9979419373566799, RMSE (Train): 0.07095614820718765\n",
      "Test Loss: 0.6504656672477722, R2 Score (Test): 0.7399971352885334, RMSE (Test): 0.7998160719871521\n",
      "Epoch [4091/5000], Train Loss: 0.006635791117635866, R2 Score (Train): 0.9982806982315923, RMSE (Train): 0.0687490776181221\n",
      "Test Loss: 0.6630734801292419, R2 Score (Test): 0.7392362242442253, RMSE (Test): 0.8009855151176453\n",
      "Epoch [4096/5000], Train Loss: 0.005227080080658197, R2 Score (Train): 0.9988383234258501, RMSE (Train): 0.05311794579029083\n",
      "Test Loss: 0.6658895015716553, R2 Score (Test): 0.73826402907377, RMSE (Test): 0.8024773001670837\n",
      "Epoch [4101/5000], Train Loss: 0.004171405530845125, R2 Score (Train): 0.9960137451653659, RMSE (Train): 0.09662390500307083\n",
      "Test Loss: 0.6640869677066803, R2 Score (Test): 0.7371791382558903, RMSE (Test): 0.8041386604309082\n",
      "Epoch [4106/5000], Train Loss: 0.006324613156418006, R2 Score (Train): 0.9976024482550859, RMSE (Train): 0.08811905235052109\n",
      "Test Loss: 0.6637769937515259, R2 Score (Test): 0.7375956151079296, RMSE (Test): 0.8035013675689697\n",
      "Epoch [4111/5000], Train Loss: 0.00462769561757644, R2 Score (Train): 0.9977091846272624, RMSE (Train): 0.08207909762859344\n",
      "Test Loss: 0.6881148517131805, R2 Score (Test): 0.7288950094564399, RMSE (Test): 0.8167136907577515\n",
      "Epoch [4116/5000], Train Loss: 0.004509420832619071, R2 Score (Train): 0.9990761086344142, RMSE (Train): 0.04775134474039078\n",
      "Test Loss: 0.6763623356819153, R2 Score (Test): 0.7325770911096301, RMSE (Test): 0.811148464679718\n",
      "Epoch [4121/5000], Train Loss: 0.003989997979563971, R2 Score (Train): 0.9962179345983089, RMSE (Train): 0.10305725038051605\n",
      "Test Loss: 0.648869663476944, R2 Score (Test): 0.7430727365499082, RMSE (Test): 0.7950714230537415\n",
      "Epoch [4126/5000], Train Loss: 0.003492684724430243, R2 Score (Train): 0.9968965981444405, RMSE (Train): 0.10862680524587631\n",
      "Test Loss: 0.6586209535598755, R2 Score (Test): 0.7392272391364667, RMSE (Test): 0.8009993433952332\n",
      "Epoch [4131/5000], Train Loss: 0.00530966369357581, R2 Score (Train): 0.9982738914215024, RMSE (Train): 0.07021279633045197\n",
      "Test Loss: 0.659484475851059, R2 Score (Test): 0.7378226413701405, RMSE (Test): 0.8031535744667053\n",
      "Epoch [4136/5000], Train Loss: 0.004097293751935164, R2 Score (Train): 0.9978409244934178, RMSE (Train): 0.08112764358520508\n",
      "Test Loss: 0.6596267223358154, R2 Score (Test): 0.7351937130584267, RMSE (Test): 0.8071703314781189\n",
      "Epoch [4141/5000], Train Loss: 0.005254947774422665, R2 Score (Train): 0.9974576627155466, RMSE (Train): 0.07170189917087555\n",
      "Test Loss: 0.6551784574985504, R2 Score (Test): 0.7395675058251951, RMSE (Test): 0.800476610660553\n",
      "Epoch [4146/5000], Train Loss: 0.004697390055904786, R2 Score (Train): 0.9978502227641123, RMSE (Train): 0.07134585827589035\n",
      "Test Loss: 0.6716905534267426, R2 Score (Test): 0.7336637638322026, RMSE (Test): 0.8094987273216248\n",
      "Epoch [4151/5000], Train Loss: 0.0060828217926124735, R2 Score (Train): 0.9984239337479064, RMSE (Train): 0.06279782205820084\n",
      "Test Loss: 0.6547079980373383, R2 Score (Test): 0.7384482676153331, RMSE (Test): 0.8021947741508484\n",
      "Epoch [4156/5000], Train Loss: 0.005386872372279565, R2 Score (Train): 0.9976897655738528, RMSE (Train): 0.08122295141220093\n",
      "Test Loss: 0.6370637118816376, R2 Score (Test): 0.7446856183211281, RMSE (Test): 0.7925719022750854\n",
      "Epoch [4161/5000], Train Loss: 0.004551766828323404, R2 Score (Train): 0.9982485051144623, RMSE (Train): 0.0714588612318039\n",
      "Test Loss: 0.6613886952400208, R2 Score (Test): 0.7369095300123918, RMSE (Test): 0.8045510649681091\n",
      "Epoch [4166/5000], Train Loss: 0.003978132309081654, R2 Score (Train): 0.9977377473799244, RMSE (Train): 0.0771527886390686\n",
      "Test Loss: 0.6448014676570892, R2 Score (Test): 0.7437391768253626, RMSE (Test): 0.7940395474433899\n",
      "Epoch [4171/5000], Train Loss: 0.004489884672996898, R2 Score (Train): 0.9984696259434873, RMSE (Train): 0.06579826772212982\n",
      "Test Loss: 0.6518899202346802, R2 Score (Test): 0.741116388850181, RMSE (Test): 0.7980927228927612\n",
      "Epoch [4176/5000], Train Loss: 0.005072890315204859, R2 Score (Train): 0.9975905733275707, RMSE (Train): 0.0832957997918129\n",
      "Test Loss: 0.6427739262580872, R2 Score (Test): 0.7461157630357426, RMSE (Test): 0.790349006652832\n",
      "Epoch [4181/5000], Train Loss: 0.004122974971930186, R2 Score (Train): 0.9971159581690802, RMSE (Train): 0.09328842163085938\n",
      "Test Loss: 0.6679037809371948, R2 Score (Test): 0.7385850960608867, RMSE (Test): 0.801984965801239\n",
      "Epoch [4186/5000], Train Loss: 0.005651678967600067, R2 Score (Train): 0.9980139247961104, RMSE (Train): 0.07831519097089767\n",
      "Test Loss: 0.649708479642868, R2 Score (Test): 0.7420461711942523, RMSE (Test): 0.7966582179069519\n",
      "Epoch [4191/5000], Train Loss: 0.0033328840121005974, R2 Score (Train): 0.997677051711477, RMSE (Train): 0.08147912472486496\n",
      "Test Loss: 0.6479850709438324, R2 Score (Test): 0.7419412755500536, RMSE (Test): 0.7968201041221619\n",
      "Epoch [4196/5000], Train Loss: 0.004928474741367002, R2 Score (Train): 0.996704694971998, RMSE (Train): 0.08476994931697845\n",
      "Test Loss: 0.6300768852233887, R2 Score (Test): 0.7475291140904816, RMSE (Test): 0.7881460189819336\n",
      "Epoch [4201/5000], Train Loss: 0.004020597669295967, R2 Score (Train): 0.9986445188416272, RMSE (Train): 0.06007331982254982\n",
      "Test Loss: 0.6580281257629395, R2 Score (Test): 0.7374612302641459, RMSE (Test): 0.8037070631980896\n",
      "Epoch [4206/5000], Train Loss: 0.004566752701066434, R2 Score (Train): 0.9966854135409458, RMSE (Train): 0.09595299512147903\n",
      "Test Loss: 0.6422586739063263, R2 Score (Test): 0.7452838468333278, RMSE (Test): 0.7916428446769714\n",
      "Epoch [4211/5000], Train Loss: 0.00459092886497577, R2 Score (Train): 0.9985242272071773, RMSE (Train): 0.07001759856939316\n",
      "Test Loss: 0.6567273139953613, R2 Score (Test): 0.7366709600439075, RMSE (Test): 0.804915726184845\n",
      "Epoch [4216/5000], Train Loss: 0.0050186485750600696, R2 Score (Train): 0.9984080755501102, RMSE (Train): 0.07317737489938736\n",
      "Test Loss: 0.6321725845336914, R2 Score (Test): 0.7484483902340449, RMSE (Test): 0.7867099046707153\n",
      "Epoch [4221/5000], Train Loss: 0.0044787715111548705, R2 Score (Train): 0.9965371373315525, RMSE (Train): 0.09514567255973816\n",
      "Test Loss: 0.6558021008968353, R2 Score (Test): 0.7386935882151932, RMSE (Test): 0.8018184900283813\n",
      "Epoch [4226/5000], Train Loss: 0.0064533053276439505, R2 Score (Train): 0.9979659369647255, RMSE (Train): 0.06537730246782303\n",
      "Test Loss: 0.6443833410739899, R2 Score (Test): 0.7408945989942795, RMSE (Test): 0.7984344959259033\n",
      "Epoch [4231/5000], Train Loss: 0.005156903915728132, R2 Score (Train): 0.997150309423125, RMSE (Train): 0.08645119518041611\n",
      "Test Loss: 0.6499520540237427, R2 Score (Test): 0.7417409894627358, RMSE (Test): 0.7971293330192566\n",
      "Epoch [4236/5000], Train Loss: 0.004982783691957593, R2 Score (Train): 0.9984556485127959, RMSE (Train): 0.06768814474344254\n",
      "Test Loss: 0.647607833147049, R2 Score (Test): 0.7425978990370455, RMSE (Test): 0.7958057522773743\n",
      "Epoch [4241/5000], Train Loss: 0.004484225840618213, R2 Score (Train): 0.9988486207119501, RMSE (Train): 0.06244724616408348\n",
      "Test Loss: 0.6539731621742249, R2 Score (Test): 0.73985382882062, RMSE (Test): 0.8000364899635315\n",
      "Epoch [4246/5000], Train Loss: 0.0038022343845417104, R2 Score (Train): 0.9972944901703322, RMSE (Train): 0.09011612832546234\n",
      "Test Loss: 0.6536433398723602, R2 Score (Test): 0.7408471131925618, RMSE (Test): 0.7985076904296875\n",
      "Epoch [4251/5000], Train Loss: 0.003800127422437072, R2 Score (Train): 0.9970786281834113, RMSE (Train): 0.08983655273914337\n",
      "Test Loss: 0.6414244771003723, R2 Score (Test): 0.7439221563245313, RMSE (Test): 0.7937560677528381\n",
      "Epoch [4256/5000], Train Loss: 0.005350552499294281, R2 Score (Train): 0.9976373212754001, RMSE (Train): 0.08053968846797943\n",
      "Test Loss: 0.6803647875785828, R2 Score (Test): 0.7324284311260192, RMSE (Test): 0.8113738298416138\n",
      "Epoch [4261/5000], Train Loss: 0.006915543305998047, R2 Score (Train): 0.997284462924796, RMSE (Train): 0.08711641281843185\n",
      "Test Loss: 0.6536519527435303, R2 Score (Test): 0.7403916584600032, RMSE (Test): 0.7992089986801147\n",
      "Epoch [4266/5000], Train Loss: 0.006134351637835304, R2 Score (Train): 0.9969746483256287, RMSE (Train): 0.09921852499246597\n",
      "Test Loss: 0.6604453921318054, R2 Score (Test): 0.7347542878671118, RMSE (Test): 0.8078398108482361\n",
      "Epoch [4271/5000], Train Loss: 0.00497586924272279, R2 Score (Train): 0.9958346261998434, RMSE (Train): 0.0990377813577652\n",
      "Test Loss: 0.6500519514083862, R2 Score (Test): 0.7402687262074249, RMSE (Test): 0.7993981838226318\n",
      "Epoch [4276/5000], Train Loss: 0.00527141437244912, R2 Score (Train): 0.997334018670257, RMSE (Train): 0.07997629046440125\n",
      "Test Loss: 0.6429623067378998, R2 Score (Test): 0.7437983706139124, RMSE (Test): 0.7939478754997253\n",
      "Epoch [4281/5000], Train Loss: 0.003683434372457365, R2 Score (Train): 0.9979106586000106, RMSE (Train): 0.07933047413825989\n",
      "Test Loss: 0.6649948954582214, R2 Score (Test): 0.7366685633021677, RMSE (Test): 0.8049193620681763\n",
      "Epoch [4286/5000], Train Loss: 0.0036967357931037745, R2 Score (Train): 0.9987548618919206, RMSE (Train): 0.0586644783616066\n",
      "Test Loss: 0.6565732955932617, R2 Score (Test): 0.7389729006945029, RMSE (Test): 0.8013898730278015\n",
      "Epoch [4291/5000], Train Loss: 0.004696143286613126, R2 Score (Train): 0.9980852009596604, RMSE (Train): 0.07164110243320465\n",
      "Test Loss: 0.6678406298160553, R2 Score (Test): 0.7335928442203599, RMSE (Test): 0.8096064925193787\n",
      "Epoch [4296/5000], Train Loss: 0.004667461073646943, R2 Score (Train): 0.9975037622505459, RMSE (Train): 0.08185121417045593\n",
      "Test Loss: 0.661486953496933, R2 Score (Test): 0.7354907025706519, RMSE (Test): 0.8067175149917603\n",
      "Epoch [4301/5000], Train Loss: 0.003012381804486116, R2 Score (Train): 0.9980408959491472, RMSE (Train): 0.08043396472930908\n",
      "Test Loss: 0.6513059139251709, R2 Score (Test): 0.7377131161145389, RMSE (Test): 0.803321361541748\n",
      "Epoch [4306/5000], Train Loss: 0.003376667504198849, R2 Score (Train): 0.9987036828466156, RMSE (Train): 0.0682152509689331\n",
      "Test Loss: 0.654716819524765, R2 Score (Test): 0.739025503315759, RMSE (Test): 0.8013091087341309\n",
      "Epoch [4311/5000], Train Loss: 0.00345951741716514, R2 Score (Train): 0.9989679771452502, RMSE (Train): 0.05330347642302513\n",
      "Test Loss: 0.6617994904518127, R2 Score (Test): 0.7358599798655135, RMSE (Test): 0.8061542510986328\n",
      "Epoch [4316/5000], Train Loss: 0.006032742016638319, R2 Score (Train): 0.9981398462138534, RMSE (Train): 0.07061916589736938\n",
      "Test Loss: 0.6610890626907349, R2 Score (Test): 0.7338316936627705, RMSE (Test): 0.8092434406280518\n",
      "Epoch [4321/5000], Train Loss: 0.0032627908512949944, R2 Score (Train): 0.9970691670316527, RMSE (Train): 0.08354776352643967\n",
      "Test Loss: 0.6655164659023285, R2 Score (Test): 0.7345835236534415, RMSE (Test): 0.8080998063087463\n",
      "Epoch [4326/5000], Train Loss: 0.004461259231902659, R2 Score (Train): 0.9973230040178188, RMSE (Train): 0.08063652366399765\n",
      "Test Loss: 0.6742314100265503, R2 Score (Test): 0.7336197485986619, RMSE (Test): 0.8095656633377075\n",
      "Epoch [4331/5000], Train Loss: 0.005819856965293487, R2 Score (Train): 0.9955358094004318, RMSE (Train): 0.11502350121736526\n",
      "Test Loss: 0.668349027633667, R2 Score (Test): 0.7334568975597949, RMSE (Test): 0.8098130822181702\n",
      "Epoch [4336/5000], Train Loss: 0.004519207946335276, R2 Score (Train): 0.9980944704292322, RMSE (Train): 0.07818996906280518\n",
      "Test Loss: 0.6553933620452881, R2 Score (Test): 0.7391279370828527, RMSE (Test): 0.8011518120765686\n",
      "Epoch [4341/5000], Train Loss: 0.004377502715215087, R2 Score (Train): 0.9986520255871546, RMSE (Train): 0.06383354961872101\n",
      "Test Loss: 0.6683141589164734, R2 Score (Test): 0.7363102858038009, RMSE (Test): 0.8054668307304382\n",
      "Epoch [4346/5000], Train Loss: 0.005034484978144367, R2 Score (Train): 0.9973423399407872, RMSE (Train): 0.07807032018899918\n",
      "Test Loss: 0.6850898563861847, R2 Score (Test): 0.7280168508423317, RMSE (Test): 0.8180352449417114\n",
      "Epoch [4351/5000], Train Loss: 0.006233839706207315, R2 Score (Train): 0.9988434513130074, RMSE (Train): 0.056342169642448425\n",
      "Test Loss: 0.6758910715579987, R2 Score (Test): 0.7310852570213943, RMSE (Test): 0.8134077787399292\n",
      "Epoch [4356/5000], Train Loss: 0.007017256536831458, R2 Score (Train): 0.9963410195763264, RMSE (Train): 0.0888734832406044\n",
      "Test Loss: 0.655720055103302, R2 Score (Test): 0.7393741241006869, RMSE (Test): 0.8007737398147583\n",
      "Epoch [4361/5000], Train Loss: 0.006333742057904601, R2 Score (Train): 0.9980357879110496, RMSE (Train): 0.07296103239059448\n",
      "Test Loss: 0.6944457590579987, R2 Score (Test): 0.7248194851415355, RMSE (Test): 0.82282954454422\n",
      "Epoch [4366/5000], Train Loss: 0.005624529207125306, R2 Score (Train): 0.996231593210257, RMSE (Train): 0.09217705577611923\n",
      "Test Loss: 0.6645875871181488, R2 Score (Test): 0.7375803081913044, RMSE (Test): 0.8035247325897217\n",
      "Epoch [4371/5000], Train Loss: 0.003934741059007744, R2 Score (Train): 0.9982883558253668, RMSE (Train): 0.07371091842651367\n",
      "Test Loss: 0.6740764677524567, R2 Score (Test): 0.7318109855418551, RMSE (Test): 0.8123095631599426\n",
      "Epoch [4376/5000], Train Loss: 0.004209104925394058, R2 Score (Train): 0.997848486557059, RMSE (Train): 0.07331959903240204\n",
      "Test Loss: 0.6724995076656342, R2 Score (Test): 0.7332839925544212, RMSE (Test): 0.8100756406784058\n",
      "Epoch [4381/5000], Train Loss: 0.005041241568202774, R2 Score (Train): 0.9949353738896728, RMSE (Train): 0.10440877079963684\n",
      "Test Loss: 0.6761022806167603, R2 Score (Test): 0.7331151288329184, RMSE (Test): 0.8103320598602295\n",
      "Epoch [4386/5000], Train Loss: 0.005139645732318361, R2 Score (Train): 0.9957907507579518, RMSE (Train): 0.11225597560405731\n",
      "Test Loss: 0.653445839881897, R2 Score (Test): 0.7404166443772524, RMSE (Test): 0.7991704940795898\n",
      "Epoch [4391/5000], Train Loss: 0.0033910147612914443, R2 Score (Train): 0.9979785033774567, RMSE (Train): 0.07072454690933228\n",
      "Test Loss: 0.6638175547122955, R2 Score (Test): 0.7368122190483104, RMSE (Test): 0.8046997785568237\n",
      "Epoch [4396/5000], Train Loss: 0.0030194881837815046, R2 Score (Train): 0.9988188708995221, RMSE (Train): 0.05666224658489227\n",
      "Test Loss: 0.6601956784725189, R2 Score (Test): 0.7373385606665632, RMSE (Test): 0.8038947582244873\n",
      "Epoch [4401/5000], Train Loss: 0.0033704979772058627, R2 Score (Train): 0.9984273998967909, RMSE (Train): 0.06635906547307968\n",
      "Test Loss: 0.6874905526638031, R2 Score (Test): 0.7289605303227962, RMSE (Test): 0.8166149258613586\n",
      "Epoch [4406/5000], Train Loss: 0.0037909912255903087, R2 Score (Train): 0.9970472253943171, RMSE (Train): 0.08504003286361694\n",
      "Test Loss: 0.6795476675033569, R2 Score (Test): 0.7302679808165113, RMSE (Test): 0.8146429657936096\n",
      "Epoch [4411/5000], Train Loss: 0.0036575558866995075, R2 Score (Train): 0.9982887767516998, RMSE (Train): 0.07096771895885468\n",
      "Test Loss: 0.6862348020076752, R2 Score (Test): 0.7304740910055855, RMSE (Test): 0.8143316507339478\n",
      "Epoch [4416/5000], Train Loss: 0.0040033720433712006, R2 Score (Train): 0.9979206679465754, RMSE (Train): 0.0778917670249939\n",
      "Test Loss: 0.6820448637008667, R2 Score (Test): 0.730785851759914, RMSE (Test): 0.8138604760169983\n",
      "Epoch [4421/5000], Train Loss: 0.004437744423436622, R2 Score (Train): 0.9982541637078018, RMSE (Train): 0.07414397597312927\n",
      "Test Loss: 0.6764481961727142, R2 Score (Test): 0.7325077381903672, RMSE (Test): 0.8112536668777466\n",
      "Epoch [4426/5000], Train Loss: 0.0036058520199730992, R2 Score (Train): 0.998449790624287, RMSE (Train): 0.0685378685593605\n",
      "Test Loss: 0.6620750427246094, R2 Score (Test): 0.7374155901483559, RMSE (Test): 0.8037768602371216\n",
      "Epoch [4431/5000], Train Loss: 0.004604900721460581, R2 Score (Train): 0.9983754802603887, RMSE (Train): 0.067024827003479\n",
      "Test Loss: 0.6609347760677338, R2 Score (Test): 0.7343426921318619, RMSE (Test): 0.8084663152694702\n",
      "Epoch [4436/5000], Train Loss: 0.004063131753355265, R2 Score (Train): 0.9974798086145132, RMSE (Train): 0.08169300109148026\n",
      "Test Loss: 0.6603679358959198, R2 Score (Test): 0.7365305006202763, RMSE (Test): 0.8051304221153259\n",
      "Epoch [4441/5000], Train Loss: 0.0037353628237421312, R2 Score (Train): 0.997513477810772, RMSE (Train): 0.07464100420475006\n",
      "Test Loss: 0.6598760783672333, R2 Score (Test): 0.7351830137977402, RMSE (Test): 0.8071866631507874\n",
      "Epoch [4446/5000], Train Loss: 0.003372618773331245, R2 Score (Train): 0.9978352448232344, RMSE (Train): 0.07507789134979248\n",
      "Test Loss: 0.6502995789051056, R2 Score (Test): 0.7401212700762279, RMSE (Test): 0.7996250987052917\n",
      "Epoch [4451/5000], Train Loss: 0.0036976386327296495, R2 Score (Train): 0.9972831865791663, RMSE (Train): 0.07975243777036667\n",
      "Test Loss: 0.6615539789199829, R2 Score (Test): 0.7365926153267406, RMSE (Test): 0.8050354719161987\n",
      "Epoch [4456/5000], Train Loss: 0.0053963395378862815, R2 Score (Train): 0.9973975474749839, RMSE (Train): 0.08727426081895828\n",
      "Test Loss: 0.6353387236595154, R2 Score (Test): 0.7441000571784302, RMSE (Test): 0.7934802770614624\n",
      "Epoch [4461/5000], Train Loss: 0.006745218260524173, R2 Score (Train): 0.9984825496839712, RMSE (Train): 0.07292813062667847\n",
      "Test Loss: 0.661887526512146, R2 Score (Test): 0.7365874314350797, RMSE (Test): 0.8050433993339539\n",
      "Epoch [4466/5000], Train Loss: 0.00664579061170419, R2 Score (Train): 0.9985922160882492, RMSE (Train): 0.059480078518390656\n",
      "Test Loss: 0.6655155122280121, R2 Score (Test): 0.7334274775056517, RMSE (Test): 0.8098577857017517\n",
      "Epoch [4471/5000], Train Loss: 0.004383792596248289, R2 Score (Train): 0.9976855398716153, RMSE (Train): 0.09472291171550751\n",
      "Test Loss: 0.6637607514858246, R2 Score (Test): 0.7361390479178234, RMSE (Test): 0.8057282567024231\n",
      "Epoch [4476/5000], Train Loss: 0.0051913653733208776, R2 Score (Train): 0.9961964275632689, RMSE (Train): 0.08673330396413803\n",
      "Test Loss: 0.6698852181434631, R2 Score (Test): 0.7357909080707856, RMSE (Test): 0.8062596321105957\n",
      "Epoch [4481/5000], Train Loss: 0.005102648593795796, R2 Score (Train): 0.9971864616383895, RMSE (Train): 0.10049840807914734\n",
      "Test Loss: 0.6530955135822296, R2 Score (Test): 0.7403368757316646, RMSE (Test): 0.7992933988571167\n",
      "Epoch [4486/5000], Train Loss: 0.0050999584297339124, R2 Score (Train): 0.9982572179202552, RMSE (Train): 0.07351461797952652\n",
      "Test Loss: 0.6766781806945801, R2 Score (Test): 0.7335517718646789, RMSE (Test): 0.8096688389778137\n",
      "Epoch [4491/5000], Train Loss: 0.006193934163699548, R2 Score (Train): 0.9965613227348833, RMSE (Train): 0.09188347309827805\n",
      "Test Loss: 0.6620942652225494, R2 Score (Test): 0.737380367270894, RMSE (Test): 0.8038308024406433\n",
      "Epoch [4496/5000], Train Loss: 0.005016937192218999, R2 Score (Train): 0.9980744955388444, RMSE (Train): 0.07800790667533875\n",
      "Test Loss: 0.6786301136016846, R2 Score (Test): 0.7316693842088443, RMSE (Test): 0.8125239014625549\n",
      "Epoch [4501/5000], Train Loss: 0.004394180723465979, R2 Score (Train): 0.9976786585429335, RMSE (Train): 0.08929910510778427\n",
      "Test Loss: 0.6367036700248718, R2 Score (Test): 0.7468388930153285, RMSE (Test): 0.7892225980758667\n",
      "Epoch [4506/5000], Train Loss: 0.005019573184351127, R2 Score (Train): 0.9966046532698756, RMSE (Train): 0.09666029363870621\n",
      "Test Loss: 0.6603971421718597, R2 Score (Test): 0.7369833552884079, RMSE (Test): 0.8044381141662598\n",
      "Epoch [4511/5000], Train Loss: 0.003180206559288005, R2 Score (Train): 0.9972473594486482, RMSE (Train): 0.07055286318063736\n",
      "Test Loss: 0.6644775867462158, R2 Score (Test): 0.737666910781623, RMSE (Test): 0.8033921718597412\n",
      "Epoch [4516/5000], Train Loss: 0.003097980283200741, R2 Score (Train): 0.9987897057778198, RMSE (Train): 0.06803810596466064\n",
      "Test Loss: 0.664962112903595, R2 Score (Test): 0.7387714215265552, RMSE (Test): 0.8016989827156067\n",
      "Epoch [4521/5000], Train Loss: 0.004297263803891838, R2 Score (Train): 0.9976475976613995, RMSE (Train): 0.0876251608133316\n",
      "Test Loss: 0.6636408567428589, R2 Score (Test): 0.7385752465296755, RMSE (Test): 0.8020000457763672\n",
      "Epoch [4526/5000], Train Loss: 0.0042616660163427396, R2 Score (Train): 0.9972431090929705, RMSE (Train): 0.087825708091259\n",
      "Test Loss: 0.6820929944515228, R2 Score (Test): 0.7320947299684981, RMSE (Test): 0.8118796348571777\n",
      "Epoch [4531/5000], Train Loss: 0.006448001135140657, R2 Score (Train): 0.9969098631350386, RMSE (Train): 0.10189660638570786\n",
      "Test Loss: 0.6839415729045868, R2 Score (Test): 0.7305373202422043, RMSE (Test): 0.8142361044883728\n",
      "Epoch [4536/5000], Train Loss: 0.006383179609353344, R2 Score (Train): 0.9984063991136204, RMSE (Train): 0.06062128767371178\n",
      "Test Loss: 0.6960622370243073, R2 Score (Test): 0.7267980880979319, RMSE (Test): 0.8198660612106323\n",
      "Epoch [4541/5000], Train Loss: 0.005475174635648727, R2 Score (Train): 0.998258833302321, RMSE (Train): 0.07275918871164322\n",
      "Test Loss: 0.6658329367637634, R2 Score (Test): 0.7381116793494988, RMSE (Test): 0.802710771560669\n",
      "Epoch [4546/5000], Train Loss: 0.006782814161852002, R2 Score (Train): 0.9960991634003947, RMSE (Train): 0.08716057986021042\n",
      "Test Loss: 0.6632938086986542, R2 Score (Test): 0.7365140217434462, RMSE (Test): 0.8051556348800659\n",
      "Epoch [4551/5000], Train Loss: 0.005122094415128231, R2 Score (Train): 0.9979700662956815, RMSE (Train): 0.0693841204047203\n",
      "Test Loss: 0.6650651395320892, R2 Score (Test): 0.7366691885003969, RMSE (Test): 0.8049184679985046\n",
      "Epoch [4556/5000], Train Loss: 0.0041829619246224565, R2 Score (Train): 0.9969060322865642, RMSE (Train): 0.0745382010936737\n",
      "Test Loss: 0.6629610359668732, R2 Score (Test): 0.7352529373048926, RMSE (Test): 0.8070800304412842\n",
      "Epoch [4561/5000], Train Loss: 0.004130273320091267, R2 Score (Train): 0.9971118098917798, RMSE (Train): 0.09316223114728928\n",
      "Test Loss: 0.6709080636501312, R2 Score (Test): 0.733298750613693, RMSE (Test): 0.810053288936615\n",
      "Epoch [4566/5000], Train Loss: 0.004628818482160568, R2 Score (Train): 0.9978393202193359, RMSE (Train): 0.07522639632225037\n",
      "Test Loss: 0.6832398474216461, R2 Score (Test): 0.7287049507912591, RMSE (Test): 0.8169998526573181\n",
      "Epoch [4571/5000], Train Loss: 0.00439859243730704, R2 Score (Train): 0.9975676061430573, RMSE (Train): 0.09096097201108932\n",
      "Test Loss: 0.6769678592681885, R2 Score (Test): 0.7292081597318467, RMSE (Test): 0.8162418007850647\n",
      "Epoch [4576/5000], Train Loss: 0.003961536722878615, R2 Score (Train): 0.9978761855015144, RMSE (Train): 0.07675182819366455\n",
      "Test Loss: 0.6659253835678101, R2 Score (Test): 0.7349096848197638, RMSE (Test): 0.8076030611991882\n",
      "Epoch [4581/5000], Train Loss: 0.004276831517927349, R2 Score (Train): 0.9985732401532821, RMSE (Train): 0.06750282645225525\n",
      "Test Loss: 0.6693393886089325, R2 Score (Test): 0.731845452403308, RMSE (Test): 0.8122572898864746\n",
      "Epoch [4586/5000], Train Loss: 0.004309726374534269, R2 Score (Train): 0.997709964600606, RMSE (Train): 0.08203405141830444\n",
      "Test Loss: 0.6766578257083893, R2 Score (Test): 0.7302069069245665, RMSE (Test): 0.8147351145744324\n",
      "Epoch [4591/5000], Train Loss: 0.004115615816166003, R2 Score (Train): 0.996555511704306, RMSE (Train): 0.10285117477178574\n",
      "Test Loss: 0.6586599946022034, R2 Score (Test): 0.7373216112801422, RMSE (Test): 0.8039206862449646\n",
      "Epoch [4596/5000], Train Loss: 0.004577246184150378, R2 Score (Train): 0.9971186099950641, RMSE (Train): 0.08826975524425507\n",
      "Test Loss: 0.6769990622997284, R2 Score (Test): 0.7318679592227393, RMSE (Test): 0.8122232556343079\n",
      "Epoch [4601/5000], Train Loss: 0.005153150879777968, R2 Score (Train): 0.9979841960462084, RMSE (Train): 0.07669461518526077\n",
      "Test Loss: 0.6741297841072083, R2 Score (Test): 0.7323379189516297, RMSE (Test): 0.8115111589431763\n",
      "Epoch [4606/5000], Train Loss: 0.0041743285255506635, R2 Score (Train): 0.9979490647976142, RMSE (Train): 0.078226737678051\n",
      "Test Loss: 0.6658534705638885, R2 Score (Test): 0.73290626979161, RMSE (Test): 0.810649037361145\n",
      "Epoch [4611/5000], Train Loss: 0.005096685024909675, R2 Score (Train): 0.9979004700750533, RMSE (Train): 0.08026399463415146\n",
      "Test Loss: 0.6781412363052368, R2 Score (Test): 0.73034999490931, RMSE (Test): 0.8145191073417664\n",
      "Epoch [4616/5000], Train Loss: 0.005297756792667012, R2 Score (Train): 0.9984172139052852, RMSE (Train): 0.0709228441119194\n",
      "Test Loss: 0.6715015172958374, R2 Score (Test): 0.734475707854976, RMSE (Test): 0.808263897895813\n",
      "Epoch [4621/5000], Train Loss: 0.004443351334581773, R2 Score (Train): 0.9973063544953154, RMSE (Train): 0.08333108574151993\n",
      "Test Loss: 0.6582337319850922, R2 Score (Test): 0.7373806184459456, RMSE (Test): 0.8038303852081299\n",
      "Epoch [4626/5000], Train Loss: 0.004347632871940732, R2 Score (Train): 0.9962529704098679, RMSE (Train): 0.10190264135599136\n",
      "Test Loss: 0.6723958551883698, R2 Score (Test): 0.7333395253144819, RMSE (Test): 0.8099913597106934\n",
      "Epoch [4631/5000], Train Loss: 0.005057130862648289, R2 Score (Train): 0.9987682557418264, RMSE (Train): 0.06058657169342041\n",
      "Test Loss: 0.669434666633606, R2 Score (Test): 0.7335843568228733, RMSE (Test): 0.8096193671226501\n",
      "Epoch [4636/5000], Train Loss: 0.005663871726331611, R2 Score (Train): 0.9981925947645892, RMSE (Train): 0.0703023299574852\n",
      "Test Loss: 0.6804687082767487, R2 Score (Test): 0.7305943068300542, RMSE (Test): 0.8141499757766724\n",
      "Epoch [4641/5000], Train Loss: 0.007189661652470629, R2 Score (Train): 0.9977700135433001, RMSE (Train): 0.07264167815446854\n",
      "Test Loss: 0.6515729129314423, R2 Score (Test): 0.7378374738847532, RMSE (Test): 0.8031309247016907\n",
      "Epoch [4646/5000], Train Loss: 0.0048130387828374905, R2 Score (Train): 0.9982323343920421, RMSE (Train): 0.07153790444135666\n",
      "Test Loss: 0.6727269291877747, R2 Score (Test): 0.731822597635357, RMSE (Test): 0.8122919201850891\n",
      "Epoch [4651/5000], Train Loss: 0.007741457472244899, R2 Score (Train): 0.9955840137448051, RMSE (Train): 0.10382837802171707\n",
      "Test Loss: 0.6423572897911072, R2 Score (Test): 0.7417392965348704, RMSE (Test): 0.7971319556236267\n",
      "Epoch [4656/5000], Train Loss: 0.005655709615287681, R2 Score (Train): 0.9973911919660009, RMSE (Train): 0.0793318897485733\n",
      "Test Loss: 0.667289525270462, R2 Score (Test): 0.7337338472418515, RMSE (Test): 0.8093922734260559\n",
      "Epoch [4661/5000], Train Loss: 0.009324108405659596, R2 Score (Train): 0.9963533967867868, RMSE (Train): 0.09612612426280975\n",
      "Test Loss: 0.6790888905525208, R2 Score (Test): 0.7274377692922382, RMSE (Test): 0.8189057111740112\n",
      "Epoch [4666/5000], Train Loss: 0.006509777585354944, R2 Score (Train): 0.9978556385978025, RMSE (Train): 0.08041389286518097\n",
      "Test Loss: 0.7094444930553436, R2 Score (Test): 0.7187030311960323, RMSE (Test): 0.8319238424301147\n",
      "Epoch [4671/5000], Train Loss: 0.006597997387871146, R2 Score (Train): 0.9968609880839955, RMSE (Train): 0.09402209520339966\n",
      "Test Loss: 0.680367112159729, R2 Score (Test): 0.7300880278411264, RMSE (Test): 0.8149146437644958\n",
      "Epoch [4676/5000], Train Loss: 0.004644208277265231, R2 Score (Train): 0.995428132817421, RMSE (Train): 0.11340486258268356\n",
      "Test Loss: 0.6808069348335266, R2 Score (Test): 0.7319757005429571, RMSE (Test): 0.8120600581169128\n",
      "Epoch [4681/5000], Train Loss: 0.004492513951845467, R2 Score (Train): 0.9954792560811655, RMSE (Train): 0.10511574894189835\n",
      "Test Loss: 0.6938861012458801, R2 Score (Test): 0.726472273207647, RMSE (Test): 0.8203547596931458\n",
      "Epoch [4686/5000], Train Loss: 0.004883127364640434, R2 Score (Train): 0.9985156672081517, RMSE (Train): 0.06997354328632355\n",
      "Test Loss: 0.7004172503948212, R2 Score (Test): 0.7221839818920223, RMSE (Test): 0.826760470867157\n",
      "Epoch [4691/5000], Train Loss: 0.003872569650411606, R2 Score (Train): 0.9985237396921846, RMSE (Train): 0.06417127698659897\n",
      "Test Loss: 0.6908265352249146, R2 Score (Test): 0.7265876030730256, RMSE (Test): 0.8201818466186523\n",
      "Epoch [4696/5000], Train Loss: 0.003823110368102789, R2 Score (Train): 0.997916249766267, RMSE (Train): 0.0859609916806221\n",
      "Test Loss: 0.6694724857807159, R2 Score (Test): 0.7340762207669753, RMSE (Test): 0.8088716864585876\n",
      "Epoch [4701/5000], Train Loss: 0.004319701072139044, R2 Score (Train): 0.9980374757891324, RMSE (Train): 0.07632061839103699\n",
      "Test Loss: 0.6784996688365936, R2 Score (Test): 0.7303735226966865, RMSE (Test): 0.8144835829734802\n",
      "Epoch [4706/5000], Train Loss: 0.0041343892614046735, R2 Score (Train): 0.9984145778948411, RMSE (Train): 0.07143548876047134\n",
      "Test Loss: 0.669159322977066, R2 Score (Test): 0.7344600865146387, RMSE (Test): 0.8082876801490784\n",
      "Epoch [4711/5000], Train Loss: 0.004030746252586444, R2 Score (Train): 0.99766949406525, RMSE (Train): 0.08158667385578156\n",
      "Test Loss: 0.6696796417236328, R2 Score (Test): 0.7323731933022648, RMSE (Test): 0.811457633972168\n",
      "Epoch [4716/5000], Train Loss: 0.004058531136251986, R2 Score (Train): 0.9964044563852924, RMSE (Train): 0.10456768423318863\n",
      "Test Loss: 0.6832500994205475, R2 Score (Test): 0.7288079013696611, RMSE (Test): 0.8168448209762573\n",
      "Epoch [4721/5000], Train Loss: 0.0036101807296896973, R2 Score (Train): 0.9982508571461183, RMSE (Train): 0.06531479209661484\n",
      "Test Loss: 0.675383597612381, R2 Score (Test): 0.7315379970598714, RMSE (Test): 0.8127228617668152\n",
      "Epoch [4726/5000], Train Loss: 0.005463514477014542, R2 Score (Train): 0.9943090564382332, RMSE (Train): 0.09695426374673843\n",
      "Test Loss: 0.6850973069667816, R2 Score (Test): 0.7298356233726443, RMSE (Test): 0.8152956366539001\n",
      "Epoch [4731/5000], Train Loss: 0.0032299563754349947, R2 Score (Train): 0.9969548942305427, RMSE (Train): 0.09114152938127518\n",
      "Test Loss: 0.6804466843605042, R2 Score (Test): 0.7317043030968111, RMSE (Test): 0.8124710321426392\n",
      "Epoch [4736/5000], Train Loss: 0.004946003435179591, R2 Score (Train): 0.9985097252420198, RMSE (Train): 0.05944160744547844\n",
      "Test Loss: 0.6875225901603699, R2 Score (Test): 0.7295149125757265, RMSE (Test): 0.8157793283462524\n",
      "Epoch [4741/5000], Train Loss: 0.005254712460252146, R2 Score (Train): 0.9985530396896287, RMSE (Train): 0.06778284907341003\n",
      "Test Loss: 0.6975106000900269, R2 Score (Test): 0.7248877139434748, RMSE (Test): 0.8227275609970093\n",
      "Epoch [4746/5000], Train Loss: 0.007976126973517239, R2 Score (Train): 0.9964679990815488, RMSE (Train): 0.087093785405159\n",
      "Test Loss: 0.6715124547481537, R2 Score (Test): 0.7331684255481015, RMSE (Test): 0.8102511167526245\n",
      "Epoch [4751/5000], Train Loss: 0.0047868639618779225, R2 Score (Train): 0.997887641907011, RMSE (Train): 0.07188475131988525\n",
      "Test Loss: 0.660110741853714, R2 Score (Test): 0.7361345890930813, RMSE (Test): 0.8057351112365723\n",
      "Epoch [4756/5000], Train Loss: 0.004679777504255374, R2 Score (Train): 0.9952085819842446, RMSE (Train): 0.10131221264600754\n",
      "Test Loss: 0.6519492566585541, R2 Score (Test): 0.7389045066972175, RMSE (Test): 0.801494836807251\n",
      "Epoch [4761/5000], Train Loss: 0.004087435198016465, R2 Score (Train): 0.9962441484729051, RMSE (Train): 0.09157490730285645\n",
      "Test Loss: 0.6673693358898163, R2 Score (Test): 0.7335845039868611, RMSE (Test): 0.8096191883087158\n",
      "Epoch [4766/5000], Train Loss: 0.0038065169161806502, R2 Score (Train): 0.9986035417968524, RMSE (Train): 0.06502507627010345\n",
      "Test Loss: 0.6637722253799438, R2 Score (Test): 0.7331784095302836, RMSE (Test): 0.8102360367774963\n",
      "Epoch [4771/5000], Train Loss: 0.004911293430874745, R2 Score (Train): 0.997204600038318, RMSE (Train): 0.09179112315177917\n",
      "Test Loss: 0.6701524257659912, R2 Score (Test): 0.7336017143358853, RMSE (Test): 0.8095930814743042\n",
      "Epoch [4776/5000], Train Loss: 0.005168433883227408, R2 Score (Train): 0.9974168520773922, RMSE (Train): 0.07229724526405334\n",
      "Test Loss: 0.6646068394184113, R2 Score (Test): 0.7350731682611886, RMSE (Test): 0.8073540925979614\n",
      "Epoch [4781/5000], Train Loss: 0.004260733102758725, R2 Score (Train): 0.9970873915032612, RMSE (Train): 0.09672635048627853\n",
      "Test Loss: 0.6447586417198181, R2 Score (Test): 0.7440962870450494, RMSE (Test): 0.7934861779212952\n",
      "Epoch [4786/5000], Train Loss: 0.004137174575589597, R2 Score (Train): 0.9982080605992076, RMSE (Train): 0.07271366566419601\n",
      "Test Loss: 0.6622074842453003, R2 Score (Test): 0.737668999674211, RMSE (Test): 0.8033889532089233\n",
      "Epoch [4791/5000], Train Loss: 0.004830812841343383, R2 Score (Train): 0.9968873365275934, RMSE (Train): 0.09040214866399765\n",
      "Test Loss: 0.6578976213932037, R2 Score (Test): 0.7394627121918056, RMSE (Test): 0.8006376028060913\n",
      "Epoch [4796/5000], Train Loss: 0.004845380550250411, R2 Score (Train): 0.9990210084597159, RMSE (Train): 0.05478282645344734\n",
      "Test Loss: 0.6645712852478027, R2 Score (Test): 0.7352480752364359, RMSE (Test): 0.8070874214172363\n",
      "Epoch [4801/5000], Train Loss: 0.0036224245947475233, R2 Score (Train): 0.9982874618307894, RMSE (Train): 0.06738246977329254\n",
      "Test Loss: 0.6837554275989532, R2 Score (Test): 0.7289887725154199, RMSE (Test): 0.816572368144989\n",
      "Epoch [4806/5000], Train Loss: 0.005759211101879676, R2 Score (Train): 0.9969114188264507, RMSE (Train): 0.09618494659662247\n",
      "Test Loss: 0.6613620519638062, R2 Score (Test): 0.7390267964976878, RMSE (Test): 0.8013070821762085\n",
      "Epoch [4811/5000], Train Loss: 0.0037787796075766287, R2 Score (Train): 0.9973904180666889, RMSE (Train): 0.08959601074457169\n",
      "Test Loss: 0.6676242351531982, R2 Score (Test): 0.7344343273668497, RMSE (Test): 0.8083269000053406\n",
      "Epoch [4816/5000], Train Loss: 0.00439052376896143, R2 Score (Train): 0.9970949032475106, RMSE (Train): 0.08010931313037872\n",
      "Test Loss: 0.6794149577617645, R2 Score (Test): 0.7302269877339802, RMSE (Test): 0.8147048354148865\n",
      "Epoch [4821/5000], Train Loss: 0.0049964267915735645, R2 Score (Train): 0.9961866096460197, RMSE (Train): 0.10776276886463165\n",
      "Test Loss: 0.6477700769901276, R2 Score (Test): 0.740837204092651, RMSE (Test): 0.7985228896141052\n",
      "Epoch [4826/5000], Train Loss: 0.004903611998694639, R2 Score (Train): 0.996966524308515, RMSE (Train): 0.0798398107290268\n",
      "Test Loss: 0.6799079775810242, R2 Score (Test): 0.7322865231568216, RMSE (Test): 0.8115890622138977\n",
      "Epoch [4831/5000], Train Loss: 0.0067911522928625345, R2 Score (Train): 0.9976118149814592, RMSE (Train): 0.07717990130186081\n",
      "Test Loss: 0.6844384968280792, R2 Score (Test): 0.7296405138002093, RMSE (Test): 0.815589964389801\n",
      "Epoch [4836/5000], Train Loss: 0.004538079801325996, R2 Score (Train): 0.9966730556619133, RMSE (Train): 0.09496917575597763\n",
      "Test Loss: 0.6719212234020233, R2 Score (Test): 0.7353216251993402, RMSE (Test): 0.8069753050804138\n",
      "Epoch [4841/5000], Train Loss: 0.004537056782282889, R2 Score (Train): 0.9983565190925215, RMSE (Train): 0.06867997348308563\n",
      "Test Loss: 0.6764110624790192, R2 Score (Test): 0.730903304726344, RMSE (Test): 0.8136829733848572\n",
      "Epoch [4846/5000], Train Loss: 0.0036038102504486838, R2 Score (Train): 0.9957841721102892, RMSE (Train): 0.10871671140193939\n",
      "Test Loss: 0.6646826565265656, R2 Score (Test): 0.7348558302006452, RMSE (Test): 0.807685136795044\n",
      "Epoch [4851/5000], Train Loss: 0.004757173592224717, R2 Score (Train): 0.9972499189690217, RMSE (Train): 0.08884833008050919\n",
      "Test Loss: 0.6763409972190857, R2 Score (Test): 0.7314537131250876, RMSE (Test): 0.8128504157066345\n",
      "Epoch [4856/5000], Train Loss: 0.0054348255119596916, R2 Score (Train): 0.9969580737836908, RMSE (Train): 0.09348905831575394\n",
      "Test Loss: 0.6594595313072205, R2 Score (Test): 0.737233722059359, RMSE (Test): 0.8040552139282227\n",
      "Epoch [4861/5000], Train Loss: 0.004358067992143333, R2 Score (Train): 0.9980043189066613, RMSE (Train): 0.07802031934261322\n",
      "Test Loss: 0.6894488036632538, R2 Score (Test): 0.7274229082573699, RMSE (Test): 0.8189280033111572\n",
      "Epoch [4866/5000], Train Loss: 0.004188836746228238, R2 Score (Train): 0.997337913105008, RMSE (Train): 0.08276797831058502\n",
      "Test Loss: 0.6908243894577026, R2 Score (Test): 0.7262353959497331, RMSE (Test): 0.8207099437713623\n",
      "Epoch [4871/5000], Train Loss: 0.00458092603366822, R2 Score (Train): 0.9968664622338113, RMSE (Train): 0.09036596864461899\n",
      "Test Loss: 0.6788929402828217, R2 Score (Test): 0.7316048543026179, RMSE (Test): 0.8126215934753418\n",
      "Epoch [4876/5000], Train Loss: 0.0047442739984641475, R2 Score (Train): 0.9976596182607864, RMSE (Train): 0.0704619437456131\n",
      "Test Loss: 0.6812432706356049, R2 Score (Test): 0.7304344106194365, RMSE (Test): 0.814391553401947\n",
      "Epoch [4881/5000], Train Loss: 0.004108124606621762, R2 Score (Train): 0.9994333037671332, RMSE (Train): 0.0440557561814785\n",
      "Test Loss: 0.694482833147049, R2 Score (Test): 0.7262979692747483, RMSE (Test): 0.8206160664558411\n",
      "Epoch [4886/5000], Train Loss: 0.005074782607456048, R2 Score (Train): 0.9972224213258801, RMSE (Train): 0.0800708532333374\n",
      "Test Loss: 0.651648223400116, R2 Score (Test): 0.7410439659761561, RMSE (Test): 0.7982043027877808\n",
      "Epoch [4891/5000], Train Loss: 0.0033262232318520546, R2 Score (Train): 0.9973213202148881, RMSE (Train): 0.088310606777668\n",
      "Test Loss: 0.6614670157432556, R2 Score (Test): 0.7362189431643614, RMSE (Test): 0.8056063055992126\n",
      "Epoch [4896/5000], Train Loss: 0.005233780791362126, R2 Score (Train): 0.9979387532877617, RMSE (Train): 0.07539635896682739\n",
      "Test Loss: 0.6822146475315094, R2 Score (Test): 0.7272806684297559, RMSE (Test): 0.8191416263580322\n",
      "Epoch [4901/5000], Train Loss: 0.004498169369374712, R2 Score (Train): 0.9981286247068609, RMSE (Train): 0.07371289283037186\n",
      "Test Loss: 0.6891453564167023, R2 Score (Test): 0.7270577453498556, RMSE (Test): 0.8194763660430908\n",
      "Epoch [4906/5000], Train Loss: 0.005596392399941881, R2 Score (Train): 0.996830111247744, RMSE (Train): 0.08534765243530273\n",
      "Test Loss: 0.6744981706142426, R2 Score (Test): 0.7318302181367561, RMSE (Test): 0.8122804164886475\n",
      "Epoch [4911/5000], Train Loss: 0.004819455634181698, R2 Score (Train): 0.9974980310000714, RMSE (Train): 0.08563266694545746\n",
      "Test Loss: 0.6802636384963989, R2 Score (Test): 0.7339241332777142, RMSE (Test): 0.8091028928756714\n",
      "Epoch [4916/5000], Train Loss: 0.0030789043521508574, R2 Score (Train): 0.998520764085969, RMSE (Train): 0.06718429923057556\n",
      "Test Loss: 0.6968535482883453, R2 Score (Test): 0.7239574848493437, RMSE (Test): 0.8241173028945923\n",
      "Epoch [4921/5000], Train Loss: 0.004023049802829822, R2 Score (Train): 0.9975868736635934, RMSE (Train): 0.09022049605846405\n",
      "Test Loss: 0.6655101180076599, R2 Score (Test): 0.7345016546748329, RMSE (Test): 0.8082243800163269\n",
      "Epoch [4926/5000], Train Loss: 0.00478410511277616, R2 Score (Train): 0.9984279601615763, RMSE (Train): 0.06612445414066315\n",
      "Test Loss: 0.6947275102138519, R2 Score (Test): 0.7246128584506891, RMSE (Test): 0.823138415813446\n",
      "Epoch [4931/5000], Train Loss: 0.004526035627350211, R2 Score (Train): 0.9983478223001258, RMSE (Train): 0.06274676322937012\n",
      "Test Loss: 0.6705682873725891, R2 Score (Test): 0.7327964338511926, RMSE (Test): 0.810815691947937\n",
      "Epoch [4936/5000], Train Loss: 0.00570648016097645, R2 Score (Train): 0.9973145229913555, RMSE (Train): 0.08603180944919586\n",
      "Test Loss: 0.6654723286628723, R2 Score (Test): 0.7362029683097354, RMSE (Test): 0.8056307435035706\n",
      "Epoch [4941/5000], Train Loss: 0.0053116098667184515, R2 Score (Train): 0.9974720074525575, RMSE (Train): 0.07712147384881973\n",
      "Test Loss: 0.665386974811554, R2 Score (Test): 0.7340581030172271, RMSE (Test): 0.8088992834091187\n",
      "Epoch [4946/5000], Train Loss: 0.0057549315970391035, R2 Score (Train): 0.9977531986495254, RMSE (Train): 0.08480627834796906\n",
      "Test Loss: 0.6677667200565338, R2 Score (Test): 0.734153220541145, RMSE (Test): 0.808754563331604\n",
      "Epoch [4951/5000], Train Loss: 0.00556756486184895, R2 Score (Train): 0.9967416217502401, RMSE (Train): 0.09377147257328033\n",
      "Test Loss: 0.6647703647613525, R2 Score (Test): 0.7342906707598374, RMSE (Test): 0.8085455298423767\n",
      "Epoch [4956/5000], Train Loss: 0.005503384706874688, R2 Score (Train): 0.9968126849966983, RMSE (Train): 0.09037197381258011\n",
      "Test Loss: 0.6955863535404205, R2 Score (Test): 0.7211905473785643, RMSE (Test): 0.8282372951507568\n",
      "Epoch [4961/5000], Train Loss: 0.0042709166106457514, R2 Score (Train): 0.9985472365922995, RMSE (Train): 0.061613768339157104\n",
      "Test Loss: 0.6911168992519379, R2 Score (Test): 0.7249588103843366, RMSE (Test): 0.82262122631073\n",
      "Epoch [4966/5000], Train Loss: 0.004541056579910219, R2 Score (Train): 0.9957829329527983, RMSE (Train): 0.09387585520744324\n",
      "Test Loss: 0.6687509417533875, R2 Score (Test): 0.7330738725010049, RMSE (Test): 0.8103946447372437\n",
      "Epoch [4971/5000], Train Loss: 0.004146160247425239, R2 Score (Train): 0.9979779706408879, RMSE (Train): 0.07995156943798065\n",
      "Test Loss: 0.6719760596752167, R2 Score (Test): 0.731590526132935, RMSE (Test): 0.8126433491706848\n",
      "Epoch [4976/5000], Train Loss: 0.0030102358432486653, R2 Score (Train): 0.9984953499211294, RMSE (Train): 0.06392857432365417\n",
      "Test Loss: 0.6733049750328064, R2 Score (Test): 0.7316267012909803, RMSE (Test): 0.8125885128974915\n",
      "Epoch [4981/5000], Train Loss: 0.00782567805921038, R2 Score (Train): 0.9944907606388472, RMSE (Train): 0.12748335301876068\n",
      "Test Loss: 0.6709700524806976, R2 Score (Test): 0.7369098529968436, RMSE (Test): 0.8045505881309509\n",
      "Epoch [4986/5000], Train Loss: 0.003877783427014947, R2 Score (Train): 0.9976307070256808, RMSE (Train): 0.0821806937456131\n",
      "Test Loss: 0.6647078692913055, R2 Score (Test): 0.7365139419269214, RMSE (Test): 0.8051556944847107\n",
      "Epoch [4991/5000], Train Loss: 0.004304618341848254, R2 Score (Train): 0.9979553636684246, RMSE (Train): 0.07469454407691956\n",
      "Test Loss: 0.6932691335678101, R2 Score (Test): 0.7277422919182188, RMSE (Test): 0.8184480667114258\n",
      "Epoch [4996/5000], Train Loss: 0.00362621049862355, R2 Score (Train): 0.9977016487357964, RMSE (Train): 0.0763283222913742\n",
      "Test Loss: 0.6627368330955505, R2 Score (Test): 0.7366329529533011, RMSE (Test): 0.8049737811088562\n",
      "=========================================\n",
      "average R2 on training dataset:0.9968718660815636, average R2 on testing dataset:0.7111938807901618\n",
      "average rmse on training dataset:0.09025829285383224, average rmse on testing dataset:0.8398036360740662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device', device)\n",
    "model = prediction_model(x_data.shape[1], 1024, 512, 1)\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00015, weight_decay = 1e-5)\n",
    "\n",
    "model = model.to(device)\n",
    "loss = loss_func.to(device)\n",
    "epochs = 5000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 64)\n",
    "\n",
    "\n",
    "final_R2_test, final_R2_train, final_rmse_test, final_rmse_train = train(model, x_data, y_data, optimizer, loss, epochs, device)\n",
    "\n",
    "# for calculation\n",
    "print('=========================================')\n",
    "final_R2_train_sub = final_R2_train[20:]\n",
    "final_R2_test_sub = final_R2_test[20:]\n",
    "final_rmse_train_sub = final_rmse_train[20:]\n",
    "final_rmse_test_sub = final_rmse_test[20:]\n",
    "print(f'average R2 on training dataset:{np.mean(final_R2_train_sub)}, average R2 on testing dataset:{np.mean(final_R2_test_sub)}')\n",
    "print(f'average rmse on training dataset:{np.mean(final_rmse_train_sub)}, average rmse on testing dataset:{np.mean(final_rmse_test_sub)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(990)]\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "# for plot\n",
    "final_R2_train = final_R2_train[10:]\n",
    "final_R2_test = final_R2_test[10:]\n",
    "final_rmse_train = final_rmse_train[10:]\n",
    "final_rmse_test = final_rmse_test[10:]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, final_R2_train, label = 'r2_train')\n",
    "plt.plot(x, final_R2_test, label = 'r2_test')\n",
    "plt.legend()\n",
    "plt.title('R2 score')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('R2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, final_rmse_train, label = 'rmse_train')\n",
    "plt.plot(x, final_rmse_test, label = 'rmse_test')\n",
    "plt.legend()\n",
    "plt.title('RMSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, './model/ANN_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Drug_Discovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
