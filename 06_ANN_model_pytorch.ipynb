{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden1_size)\n",
    "        # self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.batch1 = nn.BatchNorm1d(hidden1_size)\n",
    "\n",
    "        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        # self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.batch2 = nn.BatchNorm1d(hidden2_size)\n",
    "\n",
    "        # self.hidden3 = nn.Linear(hidden2_size, hidden3_size)\n",
    "        # self.batch3 = nn.BatchNorm1d(hidden3_size)\n",
    "\n",
    "        self.predict = nn.Linear(hidden2_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        result = self.hidden1(input)\n",
    "        # result = self.dropout1(result)\n",
    "        result = self.batch1(result)\n",
    "\n",
    "        result = F.leaky_relu(result)\n",
    "\n",
    "        result = self.hidden2(result)\n",
    "        # result = self.dropout2(result)\n",
    "        result = self.batch2(result)\n",
    "        result = F.leaky_relu(result)\n",
    "\n",
    "        # result = self.hidden3(result)\n",
    "        # result = self.batch3(result)\n",
    "        # result = F.leaky_relu(result)\n",
    "\n",
    "        result = self.predict(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_data, y_data, optimizer, loss, epochs, device):\n",
    "\n",
    "    final_R2_train = []\n",
    "    final_R2_test = []\n",
    "    final_rmse_train = []\n",
    "    final_rmse_test = []\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 64)\n",
    "\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1) # convert 1D to 2D, from (N,) to (N,1)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "    test_load = DataLoader(test_dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        mse_loss = []\n",
    "        # define batch: batch_sizee = 100\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()   # initialize gradient, gradient computed from batch 1 does not interfere batch2 and model update\n",
    "\n",
    "            output = model(batch_x)\n",
    "            l = loss(output, batch_y)\n",
    "\n",
    "            mse_loss.append(l.item())   # l.item() 返回tensor中的每一个值，节约内存\n",
    "\n",
    "            l.backward()     # back propagation\n",
    "            optimizer.step()    # update model\n",
    "        \n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                train_pred = model(x_train_tensor.to(device)).cpu().numpy().flatten()\n",
    "                r2_train = r2_score(y_train, train_pred)\n",
    "                rmse_train = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "\n",
    "                final_R2_train.append(r2_train)\n",
    "                final_rmse_train.append(rmse_train)\n",
    "\n",
    "                # final_R2_test.append()\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {np.mean(mse_loss)}, R2 Score (Train): {r2_train}, RMSE (Train): {rmse_train}\")\n",
    "\n",
    "                test_loss = []\n",
    "                test_preds_list = []\n",
    "                test_target_list = []\n",
    "                for batch_x, batch_y in test_load:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    test_output = model(batch_x)\n",
    "                    l = loss(test_output, batch_y)\n",
    "                    test_loss.append(l.item())\n",
    "\n",
    "                    test_preds_list.append(test_output.cpu().numpy())\n",
    "                    test_target_list.append(batch_y.cpu().numpy())\n",
    "                    \n",
    "                test_preds = np.concatenate(test_preds_list).flatten()\n",
    "                test_target = np.concatenate(test_target_list).flatten()\n",
    "\n",
    "                r2_test = r2_score(test_target, test_preds)\n",
    "                rmse_test = np.sqrt(mean_squared_error(test_target, test_preds))\n",
    "\n",
    "                final_R2_test.append(r2_test)\n",
    "                final_rmse_test.append(rmse_test)\n",
    "\n",
    "                print(f\"Test Loss: {np.mean(test_loss)}, R2 Score (Test): {r2_test}, RMSE (Test): {rmse_test}\")\n",
    "\n",
    "    return final_R2_test, final_R2_train, final_rmse_test, final_rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = pd.read_csv('./Data/x_value_train.csv')\n",
    "x_data = x_data.values\n",
    "# x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.read_csv('./Data/y_value_train.csv')\n",
    "y_data = y_data.values\n",
    "# y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Epoch [1/5000], Train Loss: 45.104501724243164, R2 Score (Train): -15.24716978404771, RMSE (Train): 6.770422172249811\n",
      "Test Loss: 43.261404037475586, R2 Score (Test): -16.560870441947777, RMSE (Test): 6.573158264160156\n",
      "Epoch [6/5000], Train Loss: 23.89957316716512, R2 Score (Train): -6.778414107793171, RMSE (Train): 4.684598994915529\n",
      "Test Loss: 24.102577209472656, R2 Score (Test): -8.824601276849997, RMSE (Test): 4.91652774810791\n",
      "Epoch [11/5000], Train Loss: 13.253333648045858, R2 Score (Train): -3.083334262224823, RMSE (Train): 3.3941759906041162\n",
      "Test Loss: 14.512732028961182, R2 Score (Test): -4.880657669671311, RMSE (Test): 3.8037679195404053\n",
      "Epoch [16/5000], Train Loss: 6.814841588338216, R2 Score (Train): -1.0074149274729414, RMSE (Train): 2.379827426806435\n",
      "Test Loss: 8.441783905029297, R2 Score (Test): -2.48075392702749, RMSE (Test): 2.926427125930786\n",
      "Epoch [21/5000], Train Loss: 3.278698960940043, R2 Score (Train): 0.05934591180346527, RMSE (Train): 1.6290779459242593\n",
      "Test Loss: 5.418120861053467, R2 Score (Test): -1.2337398920386748, RMSE (Test): 2.344322443008423\n",
      "Epoch [26/5000], Train Loss: 1.6123568564653397, R2 Score (Train): 0.6016511646955873, RMSE (Train): 1.0601288820083279\n",
      "Test Loss: 3.6680771112442017, R2 Score (Test): -0.4501361921819338, RMSE (Test): 1.8888846635818481\n",
      "Epoch [31/5000], Train Loss: 0.67403953243047, R2 Score (Train): 0.839237506689338, RMSE (Train): 0.6734715547522101\n",
      "Test Loss: 2.586852788925171, R2 Score (Test): -0.052347336861920946, RMSE (Test): 1.6090909242630005\n",
      "Epoch [36/5000], Train Loss: 0.32695077545940876, R2 Score (Train): 0.9536767973310515, RMSE (Train): 0.36151488162254636\n",
      "Test Loss: 1.7045438289642334, R2 Score (Test): 0.2632672922576996, RMSE (Test): 1.3463438749313354\n",
      "Epoch [41/5000], Train Loss: 0.23161408180991808, R2 Score (Train): 0.9652178886075112, RMSE (Train): 0.31325994077472863\n",
      "Test Loss: 1.900989592075348, R2 Score (Test): 0.280805895542932, RMSE (Test): 1.3302218914031982\n",
      "Epoch [46/5000], Train Loss: 0.19133657030761242, R2 Score (Train): 0.9863251050224374, RMSE (Train): 0.19642143995458233\n",
      "Test Loss: 1.5388884544372559, R2 Score (Test): 0.4229851283942625, RMSE (Test): 1.1915013790130615\n",
      "Epoch [51/5000], Train Loss: 0.10372684119890134, R2 Score (Train): 0.986585286176444, RMSE (Train): 0.1945438905669734\n",
      "Test Loss: 1.221326768398285, R2 Score (Test): 0.4485918560840221, RMSE (Test): 1.1647632122039795\n",
      "Epoch [56/5000], Train Loss: 0.16333335638046265, R2 Score (Train): 0.9885408247536034, RMSE (Train): 0.17980574808227598\n",
      "Test Loss: 1.2864394187927246, R2 Score (Test): 0.47099575268567595, RMSE (Test): 1.1408554315567017\n",
      "Epoch [61/5000], Train Loss: 0.11467980438222487, R2 Score (Train): 0.9843610291446472, RMSE (Train): 0.2100540095401094\n",
      "Test Loss: 1.187581479549408, R2 Score (Test): 0.5114118274743344, RMSE (Test): 1.0964088439941406\n",
      "Epoch [66/5000], Train Loss: 0.07979923393577337, R2 Score (Train): 0.9798571059984053, RMSE (Train): 0.23838988511874493\n",
      "Test Loss: 1.1684356331825256, R2 Score (Test): 0.5105788566326843, RMSE (Test): 1.09734308719635\n",
      "Epoch [71/5000], Train Loss: 0.09055437333881855, R2 Score (Train): 0.9829348193739268, RMSE (Train): 0.21942309440606408\n",
      "Test Loss: 1.0733562409877777, R2 Score (Test): 0.5265071823033, RMSE (Test): 1.0793386697769165\n",
      "Epoch [76/5000], Train Loss: 0.11740277335047722, R2 Score (Train): 0.9831973170961585, RMSE (Train): 0.21772896417333387\n",
      "Test Loss: 1.0766615271568298, R2 Score (Test): 0.5526452323582067, RMSE (Test): 1.0491245985031128\n",
      "Epoch [81/5000], Train Loss: 0.06970704564203818, R2 Score (Train): 0.9853803404394056, RMSE (Train): 0.20309326450501616\n",
      "Test Loss: 0.9959947168827057, R2 Score (Test): 0.5797946726608374, RMSE (Test): 1.0167913436889648\n",
      "Epoch [86/5000], Train Loss: 0.09058970492333174, R2 Score (Train): 0.9876874373153786, RMSE (Train): 0.18638077701193984\n",
      "Test Loss: 1.1207402050495148, R2 Score (Test): 0.5237994707664653, RMSE (Test): 1.0824204683303833\n",
      "Epoch [91/5000], Train Loss: 0.11217426663885514, R2 Score (Train): 0.9866719981675276, RMSE (Train): 0.19391411056074764\n",
      "Test Loss: 1.0412811040878296, R2 Score (Test): 0.5597268252168568, RMSE (Test): 1.040787696838379\n",
      "Epoch [96/5000], Train Loss: 0.10543123818933964, R2 Score (Train): 0.983656940261711, RMSE (Train): 0.2147304205960703\n",
      "Test Loss: 0.8915603756904602, R2 Score (Test): 0.5847542478841513, RMSE (Test): 1.010772943496704\n",
      "Epoch [101/5000], Train Loss: 0.07947240676730871, R2 Score (Train): 0.9899526142679841, RMSE (Train): 0.168365625649969\n",
      "Test Loss: 1.0045872926712036, R2 Score (Test): 0.5551298296280367, RMSE (Test): 1.0462071895599365\n",
      "Epoch [106/5000], Train Loss: 0.08562157861888409, R2 Score (Train): 0.985123356267928, RMSE (Train): 0.2048704738698916\n",
      "Test Loss: 1.1668627262115479, R2 Score (Test): 0.5784869144745786, RMSE (Test): 1.0183724164962769\n",
      "Epoch [111/5000], Train Loss: 0.11198573435346286, R2 Score (Train): 0.9875178894281722, RMSE (Train): 0.18765965060175516\n",
      "Test Loss: 1.0243089199066162, R2 Score (Test): 0.5965387408057724, RMSE (Test): 0.9963271021842957\n",
      "Epoch [116/5000], Train Loss: 0.08583767991513014, R2 Score (Train): 0.9928063880438764, RMSE (Train): 0.14246244518972617\n",
      "Test Loss: 0.9455347955226898, R2 Score (Test): 0.5743858970976439, RMSE (Test): 1.023314356803894\n",
      "Epoch [121/5000], Train Loss: 0.08546149963513017, R2 Score (Train): 0.9877139266691174, RMSE (Train): 0.18618017845449347\n",
      "Test Loss: 0.9346904754638672, R2 Score (Test): 0.5734918532056923, RMSE (Test): 1.0243885517120361\n",
      "Epoch [126/5000], Train Loss: 0.06520973068351547, R2 Score (Train): 0.9906956527810485, RMSE (Train): 0.16202045414620944\n",
      "Test Loss: 1.0197809636592865, R2 Score (Test): 0.6084409610803555, RMSE (Test): 0.9815211296081543\n",
      "Epoch [131/5000], Train Loss: 0.0625404454767704, R2 Score (Train): 0.9865344428084115, RMSE (Train): 0.19491221419299556\n",
      "Test Loss: 0.9676580429077148, R2 Score (Test): 0.6229910820731994, RMSE (Test): 0.9631122350692749\n",
      "Epoch [136/5000], Train Loss: 0.07050807572280367, R2 Score (Train): 0.9945827897151344, RMSE (Train): 0.12362740060305735\n",
      "Test Loss: 1.1421632766723633, R2 Score (Test): 0.5772234383003602, RMSE (Test): 1.0198974609375\n",
      "Epoch [141/5000], Train Loss: 0.06683735394229491, R2 Score (Train): 0.9887873798214739, RMSE (Train): 0.1778608838468671\n",
      "Test Loss: 0.8712998926639557, R2 Score (Test): 0.612164962836026, RMSE (Test): 0.9768425226211548\n",
      "Epoch [146/5000], Train Loss: 0.05987222461650769, R2 Score (Train): 0.9867067649046253, RMSE (Train): 0.19366102824270154\n",
      "Test Loss: 1.004479467868805, R2 Score (Test): 0.6137594266723152, RMSE (Test): 0.9748324751853943\n",
      "Epoch [151/5000], Train Loss: 0.06416825608660777, R2 Score (Train): 0.9900837145466919, RMSE (Train): 0.16726358492223975\n",
      "Test Loss: 0.9988522231578827, R2 Score (Test): 0.5944994477118386, RMSE (Test): 0.9988419413566589\n",
      "Epoch [156/5000], Train Loss: 0.052032770588994026, R2 Score (Train): 0.9885123505534945, RMSE (Train): 0.18002900360470492\n",
      "Test Loss: 0.9227147102355957, R2 Score (Test): 0.6048615571839979, RMSE (Test): 0.9859972596168518\n",
      "Epoch [161/5000], Train Loss: 0.04631423686320583, R2 Score (Train): 0.9920784179511183, RMSE (Train): 0.1494971292442196\n",
      "Test Loss: 0.9844363033771515, R2 Score (Test): 0.6068776028608085, RMSE (Test): 0.9834786057472229\n",
      "Epoch [166/5000], Train Loss: 0.07094316774358352, R2 Score (Train): 0.9893482845807438, RMSE (Train): 0.17335511807495066\n",
      "Test Loss: 0.9567446708679199, R2 Score (Test): 0.6075890489291839, RMSE (Test): 0.9825882911682129\n",
      "Epoch [171/5000], Train Loss: 0.08518758912881215, R2 Score (Train): 0.9902157232917761, RMSE (Train): 0.16614652177747746\n",
      "Test Loss: 1.0251784920692444, R2 Score (Test): 0.6054393410160398, RMSE (Test): 0.9852760434150696\n",
      "Epoch [176/5000], Train Loss: 0.053467066337664924, R2 Score (Train): 0.9927489984509762, RMSE (Train): 0.14302958865296564\n",
      "Test Loss: 0.9357747733592987, R2 Score (Test): 0.6136068020571401, RMSE (Test): 0.9750249981880188\n",
      "Epoch [181/5000], Train Loss: 0.05863197815294067, R2 Score (Train): 0.9900526551748977, RMSE (Train): 0.1675253281064508\n",
      "Test Loss: 0.8141369372606277, R2 Score (Test): 0.6186947809578578, RMSE (Test): 0.9685842990875244\n",
      "Epoch [186/5000], Train Loss: 0.06332542995611827, R2 Score (Train): 0.9903719618913641, RMSE (Train): 0.16481464181822292\n",
      "Test Loss: 0.8295013904571533, R2 Score (Test): 0.6354368407777374, RMSE (Test): 0.9470817446708679\n",
      "Epoch [191/5000], Train Loss: 0.04849947585413853, R2 Score (Train): 0.9905495195112759, RMSE (Train): 0.1632878367778413\n",
      "Test Loss: 0.9003822803497314, R2 Score (Test): 0.6400855666399843, RMSE (Test): 0.9410239458084106\n",
      "Epoch [196/5000], Train Loss: 0.06270444517334302, R2 Score (Train): 0.9895790103300924, RMSE (Train): 0.17146732529119674\n",
      "Test Loss: 0.8581810295581818, R2 Score (Test): 0.6174954211628834, RMSE (Test): 0.9701063632965088\n",
      "Epoch [201/5000], Train Loss: 0.04838549982135495, R2 Score (Train): 0.991273209061469, RMSE (Train): 0.15691128416076697\n",
      "Test Loss: 0.8528412580490112, R2 Score (Test): 0.6481229945852112, RMSE (Test): 0.9304574131965637\n",
      "Epoch [206/5000], Train Loss: 0.05289221446340283, R2 Score (Train): 0.9831094055049738, RMSE (Train): 0.21829779978115388\n",
      "Test Loss: 0.8621152937412262, R2 Score (Test): 0.6409404203224809, RMSE (Test): 0.9399057626724243\n",
      "Epoch [211/5000], Train Loss: 0.05636131980766853, R2 Score (Train): 0.9935976460591128, RMSE (Train): 0.1343992144904982\n",
      "Test Loss: 0.8768908977508545, R2 Score (Test): 0.620893529526545, RMSE (Test): 0.9657876491546631\n",
      "Epoch [216/5000], Train Loss: 0.06931888808806737, R2 Score (Train): 0.9908542684638119, RMSE (Train): 0.1606334973976901\n",
      "Test Loss: 1.057703047990799, R2 Score (Test): 0.6099631377136316, RMSE (Test): 0.9796115159988403\n",
      "Epoch [221/5000], Train Loss: 0.05096004096170267, R2 Score (Train): 0.9895110863331986, RMSE (Train): 0.17202522956274854\n",
      "Test Loss: 0.989007830619812, R2 Score (Test): 0.6271056316457742, RMSE (Test): 0.9578421711921692\n",
      "Epoch [226/5000], Train Loss: 0.06749879692991574, R2 Score (Train): 0.9901495196156285, RMSE (Train): 0.1667076755011637\n",
      "Test Loss: 0.7740774750709534, R2 Score (Test): 0.6480472069183385, RMSE (Test): 0.9305575489997864\n",
      "Epoch [231/5000], Train Loss: 0.06250324752181768, R2 Score (Train): 0.9880260521485008, RMSE (Train): 0.18380002743345578\n",
      "Test Loss: 0.8161140382289886, R2 Score (Test): 0.6511189025052727, RMSE (Test): 0.9264878630638123\n",
      "Epoch [236/5000], Train Loss: 0.0698367403820157, R2 Score (Train): 0.9899449066518895, RMSE (Train): 0.1684301921382821\n",
      "Test Loss: 0.9148964881896973, R2 Score (Test): 0.6411781736315101, RMSE (Test): 0.9395945072174072\n",
      "Epoch [241/5000], Train Loss: 0.0525213242508471, R2 Score (Train): 0.9912307355498741, RMSE (Train): 0.1572926662636772\n",
      "Test Loss: 0.8145225942134857, R2 Score (Test): 0.653150454249162, RMSE (Test): 0.9237865209579468\n",
      "Epoch [246/5000], Train Loss: 0.05685097444802523, R2 Score (Train): 0.9833167739226814, RMSE (Train): 0.2169536236007566\n",
      "Test Loss: 0.9761283993721008, R2 Score (Test): 0.6326907348850161, RMSE (Test): 0.9506420493125916\n",
      "Epoch [251/5000], Train Loss: 0.08841904252767563, R2 Score (Train): 0.9893350993688432, RMSE (Train): 0.1734623785944197\n",
      "Test Loss: 0.8708002865314484, R2 Score (Test): 0.648343399756349, RMSE (Test): 0.9301660060882568\n",
      "Epoch [256/5000], Train Loss: 0.04595299328987797, R2 Score (Train): 0.9906140127150469, RMSE (Train): 0.16272971785509113\n",
      "Test Loss: 0.8821573555469513, R2 Score (Test): 0.6654194861162874, RMSE (Test): 0.9073010087013245\n",
      "Epoch [261/5000], Train Loss: 0.054766937935103975, R2 Score (Train): 0.9890620018008118, RMSE (Train): 0.17566927722871187\n",
      "Test Loss: 0.9187674820423126, R2 Score (Test): 0.6513962130626306, RMSE (Test): 0.9261196851730347\n",
      "Epoch [266/5000], Train Loss: 0.04453697117666403, R2 Score (Train): 0.9878431439583345, RMSE (Train): 0.1851985267849882\n",
      "Test Loss: 0.7960683703422546, R2 Score (Test): 0.6732687022693449, RMSE (Test): 0.8965951204299927\n",
      "Epoch [271/5000], Train Loss: 0.04769016429781914, R2 Score (Train): 0.9922287297791823, RMSE (Train): 0.14807198413413544\n",
      "Test Loss: 0.8024689257144928, R2 Score (Test): 0.6568183884115608, RMSE (Test): 0.9188889861106873\n",
      "Epoch [276/5000], Train Loss: 0.04765131898845235, R2 Score (Train): 0.9879137982150489, RMSE (Train): 0.18465956622298982\n",
      "Test Loss: 0.6676735877990723, R2 Score (Test): 0.6995110700086695, RMSE (Test): 0.8598352670669556\n",
      "Epoch [281/5000], Train Loss: 0.0660424861125648, R2 Score (Train): 0.9888888411409518, RMSE (Train): 0.17705433687953298\n",
      "Test Loss: 0.7664394378662109, R2 Score (Test): 0.6909696043471406, RMSE (Test): 0.8719702959060669\n",
      "Epoch [286/5000], Train Loss: 0.04023385110000769, R2 Score (Train): 0.9939533713299932, RMSE (Train): 0.13061213849024786\n",
      "Test Loss: 0.873598039150238, R2 Score (Test): 0.6636175063393682, RMSE (Test): 0.9097409844398499\n",
      "Epoch [291/5000], Train Loss: 0.03688986521835128, R2 Score (Train): 0.9937908526819687, RMSE (Train): 0.13235576809051863\n",
      "Test Loss: 0.7090313732624054, R2 Score (Test): 0.690749405033063, RMSE (Test): 0.8722807765007019\n",
      "Epoch [296/5000], Train Loss: 0.0714351615558068, R2 Score (Train): 0.9901609488153063, RMSE (Train): 0.16661093461987525\n",
      "Test Loss: 0.8824483752250671, R2 Score (Test): 0.6465851972764667, RMSE (Test): 0.9324883222579956\n",
      "Epoch [301/5000], Train Loss: 0.06832974155743916, R2 Score (Train): 0.9918584111335181, RMSE (Train): 0.15155891044792796\n",
      "Test Loss: 0.7769868075847626, R2 Score (Test): 0.6887961977156083, RMSE (Test): 0.8750311136245728\n",
      "Epoch [306/5000], Train Loss: 0.04642317661394676, R2 Score (Train): 0.9922485566748562, RMSE (Train): 0.14788297496141342\n",
      "Test Loss: 0.7858211100101471, R2 Score (Test): 0.6662160316001166, RMSE (Test): 0.9062202572822571\n",
      "Epoch [311/5000], Train Loss: 0.05254011958216628, R2 Score (Train): 0.9904911132818485, RMSE (Train): 0.16379163851315542\n",
      "Test Loss: 0.7654972076416016, R2 Score (Test): 0.6692790196023961, RMSE (Test): 0.9020527005195618\n",
      "Epoch [316/5000], Train Loss: 0.06846109715600808, R2 Score (Train): 0.9898081153869991, RMSE (Train): 0.16957199895428046\n",
      "Test Loss: 0.8732936680316925, R2 Score (Test): 0.6692822742720681, RMSE (Test): 0.9020482897758484\n",
      "Epoch [321/5000], Train Loss: 0.05479885948201021, R2 Score (Train): 0.9912717231542084, RMSE (Train): 0.15692464220349173\n",
      "Test Loss: 0.8064667880535126, R2 Score (Test): 0.665926769602739, RMSE (Test): 0.9066128730773926\n",
      "Epoch [326/5000], Train Loss: 0.04635499774788817, R2 Score (Train): 0.9930109808538921, RMSE (Train): 0.14042195197685148\n",
      "Test Loss: 0.7530461549758911, R2 Score (Test): 0.681615312805941, RMSE (Test): 0.8850690126419067\n",
      "Epoch [331/5000], Train Loss: 0.04678604782869419, R2 Score (Train): 0.993096140921119, RMSE (Train): 0.13956382053702587\n",
      "Test Loss: 0.7821010649204254, R2 Score (Test): 0.6739440184156003, RMSE (Test): 0.8956681489944458\n",
      "Epoch [336/5000], Train Loss: 0.058123798264811434, R2 Score (Train): 0.9901888439388706, RMSE (Train): 0.16637458401168895\n",
      "Test Loss: 0.8062109649181366, R2 Score (Test): 0.6777717835921524, RMSE (Test): 0.8903952240943909\n",
      "Epoch [341/5000], Train Loss: 0.05102209219088157, R2 Score (Train): 0.9934142920582639, RMSE (Train): 0.13631012755184752\n",
      "Test Loss: 0.7694894075393677, R2 Score (Test): 0.6788505044728643, RMSE (Test): 0.8889036178588867\n",
      "Epoch [346/5000], Train Loss: 0.037613993122552834, R2 Score (Train): 0.9922677064323601, RMSE (Train): 0.14770019130395728\n",
      "Test Loss: 0.7579508125782013, R2 Score (Test): 0.6888727765615201, RMSE (Test): 0.8749234080314636\n",
      "Epoch [351/5000], Train Loss: 0.04513634337733189, R2 Score (Train): 0.991854829021794, RMSE (Train): 0.1515922479976413\n",
      "Test Loss: 0.8627719879150391, R2 Score (Test): 0.6794962373612525, RMSE (Test): 0.8880094289779663\n",
      "Epoch [356/5000], Train Loss: 0.038133096881210804, R2 Score (Train): 0.9912380516484322, RMSE (Train): 0.15722703882286374\n",
      "Test Loss: 0.827331155538559, R2 Score (Test): 0.6697862686090192, RMSE (Test): 0.9013606309890747\n",
      "Epoch [361/5000], Train Loss: 0.051807045781364046, R2 Score (Train): 0.9861723239722271, RMSE (Train): 0.1975156392023181\n",
      "Test Loss: 0.8272150456905365, R2 Score (Test): 0.6969008466361704, RMSE (Test): 0.8635618090629578\n",
      "Epoch [366/5000], Train Loss: 0.03771304897964001, R2 Score (Train): 0.9886545947442275, RMSE (Train): 0.17891093994991225\n",
      "Test Loss: 0.745915412902832, R2 Score (Test): 0.6995134153815881, RMSE (Test): 0.8598318696022034\n",
      "Epoch [371/5000], Train Loss: 0.048949251882731915, R2 Score (Train): 0.9920347869741414, RMSE (Train): 0.14990826863996226\n",
      "Test Loss: 0.8196873366832733, R2 Score (Test): 0.6774608150567875, RMSE (Test): 0.8908247351646423\n",
      "Epoch [376/5000], Train Loss: 0.05902033268163601, R2 Score (Train): 0.9863611487250715, RMSE (Train): 0.19616240954623337\n",
      "Test Loss: 0.8316422998905182, R2 Score (Test): 0.6789156825111005, RMSE (Test): 0.8888134956359863\n",
      "Epoch [381/5000], Train Loss: 0.04055622142429153, R2 Score (Train): 0.9923477121178762, RMSE (Train): 0.146934080884412\n",
      "Test Loss: 0.7336836457252502, R2 Score (Test): 0.690283291695784, RMSE (Test): 0.8729378581047058\n",
      "Epoch [386/5000], Train Loss: 0.03573820018209517, R2 Score (Train): 0.9938014096323604, RMSE (Train): 0.13224320289987762\n",
      "Test Loss: 0.7147002816200256, R2 Score (Test): 0.6967517553561247, RMSE (Test): 0.8637740612030029\n",
      "Epoch [391/5000], Train Loss: 0.02898212382569909, R2 Score (Train): 0.9918412110654479, RMSE (Train): 0.15171891903260487\n",
      "Test Loss: 0.723796546459198, R2 Score (Test): 0.6839414394581933, RMSE (Test): 0.8818299174308777\n",
      "Epoch [396/5000], Train Loss: 0.036275249595443405, R2 Score (Train): 0.9912080655996687, RMSE (Train): 0.1574958483751582\n",
      "Test Loss: 0.754966676235199, R2 Score (Test): 0.6942358465996821, RMSE (Test): 0.8673499226570129\n",
      "Epoch [401/5000], Train Loss: 0.040960253681987524, R2 Score (Train): 0.9905892036473452, RMSE (Train): 0.16294463974973716\n",
      "Test Loss: 0.7628690302371979, R2 Score (Test): 0.6894171948942855, RMSE (Test): 0.8741576671600342\n",
      "Epoch [406/5000], Train Loss: 0.03210831123093764, R2 Score (Train): 0.9945540431196987, RMSE (Train): 0.12395498292632393\n",
      "Test Loss: 0.7445821762084961, R2 Score (Test): 0.6894510330462502, RMSE (Test): 0.8741099238395691\n",
      "Epoch [411/5000], Train Loss: 0.03321540914475918, R2 Score (Train): 0.992801177061233, RMSE (Train): 0.14251403505453553\n",
      "Test Loss: 0.7582566738128662, R2 Score (Test): 0.7008817462829926, RMSE (Test): 0.8578720092773438\n",
      "Epoch [416/5000], Train Loss: 0.04744984582066536, R2 Score (Train): 0.9909623010264283, RMSE (Train): 0.15968194966988508\n",
      "Test Loss: 0.7433193325996399, R2 Score (Test): 0.6723418858411829, RMSE (Test): 0.897865891456604\n",
      "Epoch [421/5000], Train Loss: 0.043104557786136866, R2 Score (Train): 0.9829536638925239, RMSE (Train): 0.21930191008287117\n",
      "Test Loss: 0.7490190863609314, R2 Score (Test): 0.6957645764737423, RMSE (Test): 0.8651790022850037\n",
      "Epoch [426/5000], Train Loss: 0.049122312727073826, R2 Score (Train): 0.9849327201666058, RMSE (Train): 0.20617894723940502\n",
      "Test Loss: 0.8482524156570435, R2 Score (Test): 0.6812637971560571, RMSE (Test): 0.8855574727058411\n",
      "Epoch [431/5000], Train Loss: 0.05009401263669133, R2 Score (Train): 0.9932501073901167, RMSE (Train): 0.13799880394571432\n",
      "Test Loss: 0.6463819593191147, R2 Score (Test): 0.7100330066683938, RMSE (Test): 0.8446471691131592\n",
      "Epoch [436/5000], Train Loss: 0.03527426548923055, R2 Score (Train): 0.9878596042779013, RMSE (Train): 0.18507310539678315\n",
      "Test Loss: 0.8049115836620331, R2 Score (Test): 0.6707642806827206, RMSE (Test): 0.900024950504303\n",
      "Epoch [441/5000], Train Loss: 0.029845753218978643, R2 Score (Train): 0.9923381852732253, RMSE (Train): 0.1470255164637619\n",
      "Test Loss: 0.6973236501216888, R2 Score (Test): 0.7086131894003704, RMSE (Test): 0.8467125296592712\n",
      "Epoch [446/5000], Train Loss: 0.04039545636624098, R2 Score (Train): 0.9901929873086023, RMSE (Train): 0.16633944930345756\n",
      "Test Loss: 0.7269847393035889, R2 Score (Test): 0.6964903743033171, RMSE (Test): 0.8641462326049805\n",
      "Epoch [451/5000], Train Loss: 0.04316945793107152, R2 Score (Train): 0.9913302527607237, RMSE (Train): 0.15639760879266137\n",
      "Test Loss: 0.8350417613983154, R2 Score (Test): 0.689539669154426, RMSE (Test): 0.8739852905273438\n",
      "Epoch [456/5000], Train Loss: 0.030936368741095066, R2 Score (Train): 0.9897521577500321, RMSE (Train): 0.17003687173933654\n",
      "Test Loss: 0.7002180516719818, R2 Score (Test): 0.6834699641233166, RMSE (Test): 0.8824873566627502\n",
      "Epoch [461/5000], Train Loss: 0.03775750178222855, R2 Score (Train): 0.9894869627372286, RMSE (Train): 0.17222293753910753\n",
      "Test Loss: 0.6683562994003296, R2 Score (Test): 0.7056102063129213, RMSE (Test): 0.8510643243789673\n",
      "Epoch [466/5000], Train Loss: 0.03739330358803272, R2 Score (Train): 0.9895417872317825, RMSE (Train): 0.17177328739055342\n",
      "Test Loss: 0.7427549064159393, R2 Score (Test): 0.7029640174597191, RMSE (Test): 0.8548808097839355\n",
      "Epoch [471/5000], Train Loss: 0.035501306566099324, R2 Score (Train): 0.9924844510973244, RMSE (Train): 0.14561537842613173\n",
      "Test Loss: 0.6709407269954681, R2 Score (Test): 0.7168150071548207, RMSE (Test): 0.8347110748291016\n",
      "Epoch [476/5000], Train Loss: 0.04751485958695412, R2 Score (Train): 0.9904594194005584, RMSE (Train): 0.1640643767647327\n",
      "Test Loss: 0.7707730531692505, R2 Score (Test): 0.7016830764886663, RMSE (Test): 0.8567220568656921\n",
      "Epoch [481/5000], Train Loss: 0.03302605915814638, R2 Score (Train): 0.9892268591382639, RMSE (Train): 0.17434040875018864\n",
      "Test Loss: 0.7850404381752014, R2 Score (Test): 0.7031562439131438, RMSE (Test): 0.854604184627533\n",
      "Epoch [486/5000], Train Loss: 0.044976933393627405, R2 Score (Train): 0.9916499324395305, RMSE (Train): 0.15348710169578758\n",
      "Test Loss: 0.7271768152713776, R2 Score (Test): 0.698081837592523, RMSE (Test): 0.8618777394294739\n",
      "Epoch [491/5000], Train Loss: 0.04126471964021524, R2 Score (Train): 0.9897857486389262, RMSE (Train): 0.16975796532522464\n",
      "Test Loss: 0.7261497378349304, R2 Score (Test): 0.7074279971185278, RMSE (Test): 0.848432719707489\n",
      "Epoch [496/5000], Train Loss: 0.04611826657007138, R2 Score (Train): 0.9877428272350331, RMSE (Train): 0.18596107344151752\n",
      "Test Loss: 0.7484593093395233, R2 Score (Test): 0.6996238058322254, RMSE (Test): 0.8596740365028381\n",
      "Epoch [501/5000], Train Loss: 0.03948407511537274, R2 Score (Train): 0.9887288418114428, RMSE (Train): 0.17832456086166537\n",
      "Test Loss: 0.7464273869991302, R2 Score (Test): 0.7047754629703671, RMSE (Test): 0.8522700667381287\n",
      "Epoch [506/5000], Train Loss: 0.03809641363720099, R2 Score (Train): 0.9886616284401322, RMSE (Train): 0.17885547255064782\n",
      "Test Loss: 0.7543032765388489, R2 Score (Test): 0.6834692894062699, RMSE (Test): 0.8824883103370667\n",
      "Epoch [511/5000], Train Loss: 0.03563736084227761, R2 Score (Train): 0.9908037279999852, RMSE (Train): 0.1610767263709428\n",
      "Test Loss: 0.7097578644752502, R2 Score (Test): 0.7106415626318068, RMSE (Test): 0.8437603712081909\n",
      "Epoch [516/5000], Train Loss: 0.0364815864401559, R2 Score (Train): 0.9934833377666077, RMSE (Train): 0.1355936952524183\n",
      "Test Loss: 0.7679260671138763, R2 Score (Test): 0.7034206166833076, RMSE (Test): 0.8542236089706421\n",
      "Epoch [521/5000], Train Loss: 0.0395815959200263, R2 Score (Train): 0.9924592944620301, RMSE (Train): 0.14585888244051973\n",
      "Test Loss: 0.6522924304008484, R2 Score (Test): 0.7100363886998673, RMSE (Test): 0.8446422219276428\n",
      "Epoch [526/5000], Train Loss: 0.041834730499734483, R2 Score (Train): 0.9926586941767488, RMSE (Train): 0.1439174809616433\n",
      "Test Loss: 0.6603306829929352, R2 Score (Test): 0.7227165922321055, RMSE (Test): 0.82596755027771\n",
      "Epoch [531/5000], Train Loss: 0.03772764885798097, R2 Score (Train): 0.9886828264318726, RMSE (Train): 0.17868820203306182\n",
      "Test Loss: 0.7024088203907013, R2 Score (Test): 0.692198575437056, RMSE (Test): 0.8702346086502075\n",
      "Epoch [536/5000], Train Loss: 0.043304707699765764, R2 Score (Train): 0.9862399879869258, RMSE (Train): 0.19703178743870134\n",
      "Test Loss: 0.7212156653404236, R2 Score (Test): 0.7138081053904746, RMSE (Test): 0.8391308784484863\n",
      "Epoch [541/5000], Train Loss: 0.04129473368326823, R2 Score (Train): 0.9904302012207304, RMSE (Train): 0.164315409595287\n",
      "Test Loss: 0.794925719499588, R2 Score (Test): 0.6894347402988451, RMSE (Test): 0.8741329908370972\n",
      "Epoch [546/5000], Train Loss: 0.0365216457284987, R2 Score (Train): 0.9851697518493437, RMSE (Train): 0.20455076105886144\n",
      "Test Loss: 0.6864176690578461, R2 Score (Test): 0.7134422573380277, RMSE (Test): 0.8396670818328857\n",
      "Epoch [551/5000], Train Loss: 0.034466471523046494, R2 Score (Train): 0.9894398657305525, RMSE (Train): 0.17260827436760598\n",
      "Test Loss: 0.661688357591629, R2 Score (Test): 0.7110847140011989, RMSE (Test): 0.8431140184402466\n",
      "Epoch [556/5000], Train Loss: 0.04531967748577396, R2 Score (Train): 0.9899170669642683, RMSE (Train): 0.16866319856605552\n",
      "Test Loss: 0.753399670124054, R2 Score (Test): 0.6910464793635605, RMSE (Test): 0.8718618154525757\n",
      "Epoch [561/5000], Train Loss: 0.03622085942576329, R2 Score (Train): 0.9892010878139863, RMSE (Train): 0.174548811270207\n",
      "Test Loss: 0.7998261749744415, R2 Score (Test): 0.6922908052242275, RMSE (Test): 0.8701042532920837\n",
      "Epoch [566/5000], Train Loss: 0.03268343458573023, R2 Score (Train): 0.9927694719136076, RMSE (Train): 0.14282752133235235\n",
      "Test Loss: 0.7694213688373566, R2 Score (Test): 0.6999348171514119, RMSE (Test): 0.8592288494110107\n",
      "Epoch [571/5000], Train Loss: 0.029751296465595562, R2 Score (Train): 0.9894969378734785, RMSE (Train): 0.17214121258513465\n",
      "Test Loss: 0.6199327409267426, R2 Score (Test): 0.729506063587076, RMSE (Test): 0.8157927393913269\n",
      "Epoch [576/5000], Train Loss: 0.03869272178659836, R2 Score (Train): 0.9911857679686545, RMSE (Train): 0.1576954381567089\n",
      "Test Loss: 0.7259882092475891, R2 Score (Test): 0.7187883027675386, RMSE (Test): 0.8317976593971252\n",
      "Epoch [581/5000], Train Loss: 0.02620189047108094, R2 Score (Train): 0.992468817749994, RMSE (Train): 0.14576674948455787\n",
      "Test Loss: 0.6516939550638199, R2 Score (Test): 0.7109146250306438, RMSE (Test): 0.8433621525764465\n",
      "Epoch [586/5000], Train Loss: 0.03280521393753588, R2 Score (Train): 0.9900229456198261, RMSE (Train): 0.16777531402390633\n",
      "Test Loss: 0.6665705442428589, R2 Score (Test): 0.7166082149382185, RMSE (Test): 0.8350157737731934\n",
      "Epoch [591/5000], Train Loss: 0.0401085102930665, R2 Score (Train): 0.9919304457348836, RMSE (Train): 0.1508869444440677\n",
      "Test Loss: 0.681643933057785, R2 Score (Test): 0.7236240745394553, RMSE (Test): 0.8246148824691772\n",
      "Epoch [596/5000], Train Loss: 0.03229808357233802, R2 Score (Train): 0.9899070536787974, RMSE (Train): 0.16874692686495643\n",
      "Test Loss: 0.6841416358947754, R2 Score (Test): 0.7132372725783193, RMSE (Test): 0.8399673104286194\n",
      "Epoch [601/5000], Train Loss: 0.022915920553108055, R2 Score (Train): 0.9942928993433519, RMSE (Train): 0.12689212104693406\n",
      "Test Loss: 0.7280912697315216, R2 Score (Test): 0.7196149993299334, RMSE (Test): 0.8305742144584656\n",
      "Epoch [606/5000], Train Loss: 0.02574161347001791, R2 Score (Train): 0.9942179718583493, RMSE (Train): 0.12772237663657277\n",
      "Test Loss: 0.6768005788326263, R2 Score (Test): 0.7271566677099854, RMSE (Test): 0.8193278908729553\n",
      "Epoch [611/5000], Train Loss: 0.031137123393515747, R2 Score (Train): 0.9921242791621631, RMSE (Train): 0.1490637517075854\n",
      "Test Loss: 0.6266265958547592, R2 Score (Test): 0.7066849701448676, RMSE (Test): 0.8495094180107117\n",
      "Epoch [616/5000], Train Loss: 0.023370662626499932, R2 Score (Train): 0.9922892725478624, RMSE (Train): 0.14749407241430934\n",
      "Test Loss: 0.7413230538368225, R2 Score (Test): 0.7159202662638537, RMSE (Test): 0.8360286355018616\n",
      "Epoch [621/5000], Train Loss: 0.028674728237092495, R2 Score (Train): 0.990892088277982, RMSE (Train): 0.16030102412593578\n",
      "Test Loss: 0.650052934885025, R2 Score (Test): 0.7172170686482497, RMSE (Test): 0.8341182470321655\n",
      "Epoch [626/5000], Train Loss: 0.031028517832358677, R2 Score (Train): 0.9903675732832076, RMSE (Train): 0.16485220006572968\n",
      "Test Loss: 0.6897247731685638, R2 Score (Test): 0.6959441490458402, RMSE (Test): 0.8649235367774963\n",
      "Epoch [631/5000], Train Loss: 0.024780552213390667, R2 Score (Train): 0.990196907905484, RMSE (Train): 0.16630619681678924\n",
      "Test Loss: 0.6873323619365692, R2 Score (Test): 0.7208770029923804, RMSE (Test): 0.8287029266357422\n",
      "Epoch [636/5000], Train Loss: 0.03557331177095572, R2 Score (Train): 0.9888823122172848, RMSE (Train): 0.17710634785796114\n",
      "Test Loss: 0.7572406232357025, R2 Score (Test): 0.706229050946322, RMSE (Test): 0.8501694202423096\n",
      "Epoch [641/5000], Train Loss: 0.023843038827180862, R2 Score (Train): 0.991634796411865, RMSE (Train): 0.153626150461215\n",
      "Test Loss: 0.6622816324234009, R2 Score (Test): 0.7255617418128868, RMSE (Test): 0.8217190504074097\n",
      "Epoch [646/5000], Train Loss: 0.020917997307454545, R2 Score (Train): 0.9903598098657495, RMSE (Train): 0.16491861939686234\n",
      "Test Loss: 0.6844482719898224, R2 Score (Test): 0.7122339379266146, RMSE (Test): 0.8414354920387268\n",
      "Epoch [651/5000], Train Loss: 0.02084782316039006, R2 Score (Train): 0.9926881760690071, RMSE (Train): 0.14362821171158419\n",
      "Test Loss: 0.7738573253154755, R2 Score (Test): 0.7121674925425963, RMSE (Test): 0.8415325284004211\n",
      "Epoch [656/5000], Train Loss: 0.0290346029214561, R2 Score (Train): 0.990438451083082, RMSE (Train): 0.16424456840934917\n",
      "Test Loss: 0.6377011090517044, R2 Score (Test): 0.7120677255650072, RMSE (Test): 0.8416785001754761\n",
      "Epoch [661/5000], Train Loss: 0.03158432881658276, R2 Score (Train): 0.9928005329595037, RMSE (Train): 0.1425204105053592\n",
      "Test Loss: 0.6697738468647003, R2 Score (Test): 0.7191258533761833, RMSE (Test): 0.8312984108924866\n",
      "Epoch [666/5000], Train Loss: 0.031304989475756884, R2 Score (Train): 0.9914365928583667, RMSE (Train): 0.15543549015996377\n",
      "Test Loss: 0.6919141113758087, R2 Score (Test): 0.7242129284951047, RMSE (Test): 0.8237358331680298\n",
      "Epoch [671/5000], Train Loss: 0.030176503583788872, R2 Score (Train): 0.9901135104630963, RMSE (Train): 0.16701210359920943\n",
      "Test Loss: 0.7319895029067993, R2 Score (Test): 0.7232011340576661, RMSE (Test): 0.8252455592155457\n",
      "Epoch [676/5000], Train Loss: 0.02153937720383207, R2 Score (Train): 0.9943406794123402, RMSE (Train): 0.12635983172868237\n",
      "Test Loss: 0.669407457113266, R2 Score (Test): 0.7316795862881589, RMSE (Test): 0.8125085234642029\n",
      "Epoch [681/5000], Train Loss: 0.03920680571657916, R2 Score (Train): 0.9882133862585948, RMSE (Train): 0.18235657067435923\n",
      "Test Loss: 0.8273864090442657, R2 Score (Test): 0.6982843321425192, RMSE (Test): 0.861588716506958\n",
      "Epoch [686/5000], Train Loss: 0.03102933312766254, R2 Score (Train): 0.989503589869994, RMSE (Train): 0.17208669209881133\n",
      "Test Loss: 0.8421198725700378, R2 Score (Test): 0.6969082004912663, RMSE (Test): 0.8635512590408325\n",
      "Epoch [691/5000], Train Loss: 0.02863100985996425, R2 Score (Train): 0.9908053900465605, RMSE (Train): 0.16106216997685793\n",
      "Test Loss: 0.7439367175102234, R2 Score (Test): 0.7197650930917695, RMSE (Test): 0.8303518891334534\n",
      "Epoch [696/5000], Train Loss: 0.03585794583583871, R2 Score (Train): 0.9897799103218449, RMSE (Train): 0.1698064739828532\n",
      "Test Loss: 0.6868400573730469, R2 Score (Test): 0.7206612046696346, RMSE (Test): 0.8290231823921204\n",
      "Epoch [701/5000], Train Loss: 0.02957500470802188, R2 Score (Train): 0.993381168209704, RMSE (Train): 0.13665249415270322\n",
      "Test Loss: 0.6911525726318359, R2 Score (Test): 0.7157378801610115, RMSE (Test): 0.8362969756126404\n",
      "Epoch [706/5000], Train Loss: 0.03142728881600002, R2 Score (Train): 0.9920076149887271, RMSE (Train): 0.15016374437798002\n",
      "Test Loss: 0.7846368551254272, R2 Score (Test): 0.6914024147210729, RMSE (Test): 0.8713594079017639\n",
      "Epoch [711/5000], Train Loss: 0.03332418055894474, R2 Score (Train): 0.9899156253228932, RMSE (Train): 0.1686752557300721\n",
      "Test Loss: 0.6653779149055481, R2 Score (Test): 0.7195035110317003, RMSE (Test): 0.8307393193244934\n",
      "Epoch [716/5000], Train Loss: 0.029888482531532645, R2 Score (Train): 0.9874187654054581, RMSE (Train): 0.18840330669877778\n",
      "Test Loss: 0.7804071307182312, R2 Score (Test): 0.71679740536221, RMSE (Test): 0.8347369432449341\n",
      "Epoch [721/5000], Train Loss: 0.02081700918885569, R2 Score (Train): 0.9934007679104826, RMSE (Train): 0.13645001629824435\n",
      "Test Loss: 0.6375046670436859, R2 Score (Test): 0.7251000802368368, RMSE (Test): 0.8224099278450012\n",
      "Epoch [726/5000], Train Loss: 0.02710043507007261, R2 Score (Train): 0.9881925270915966, RMSE (Train): 0.18251786062586395\n",
      "Test Loss: 0.7148431539535522, R2 Score (Test): 0.7204262239457071, RMSE (Test): 0.8293717503547668\n",
      "Epoch [731/5000], Train Loss: 0.02832413635527094, R2 Score (Train): 0.9893724528114684, RMSE (Train): 0.17315833917243378\n",
      "Test Loss: 0.6573278903961182, R2 Score (Test): 0.7223095771851932, RMSE (Test): 0.8265734910964966\n",
      "Epoch [736/5000], Train Loss: 0.022234113421291113, R2 Score (Train): 0.9916975778312532, RMSE (Train): 0.15304857740796418\n",
      "Test Loss: 0.6377525627613068, R2 Score (Test): 0.7311845923182647, RMSE (Test): 0.8132575750350952\n",
      "Epoch [741/5000], Train Loss: 0.027030006051063538, R2 Score (Train): 0.9937290793104866, RMSE (Train): 0.13301252711926398\n",
      "Test Loss: 0.639151006937027, R2 Score (Test): 0.7285677877128613, RMSE (Test): 0.8172063827514648\n",
      "Epoch [746/5000], Train Loss: 0.02973546413704753, R2 Score (Train): 0.9926369543463136, RMSE (Train): 0.14413041506651367\n",
      "Test Loss: 0.6976489722728729, R2 Score (Test): 0.7235187508959829, RMSE (Test): 0.8247719407081604\n",
      "Epoch [751/5000], Train Loss: 0.029476055487369496, R2 Score (Train): 0.992489405556922, RMSE (Train): 0.1455673736524943\n",
      "Test Loss: 0.7241674959659576, R2 Score (Test): 0.727278627926294, RMSE (Test): 0.8191447257995605\n",
      "Epoch [756/5000], Train Loss: 0.02782547815392415, R2 Score (Train): 0.9918612192305125, RMSE (Train): 0.1515327712733934\n",
      "Test Loss: 0.6845969557762146, R2 Score (Test): 0.7235712417970619, RMSE (Test): 0.8246936798095703\n",
      "Epoch [761/5000], Train Loss: 0.0243425858207047, R2 Score (Train): 0.9923762276685049, RMSE (Train): 0.14666005716089542\n",
      "Test Loss: 0.709183007478714, R2 Score (Test): 0.7179740233308456, RMSE (Test): 0.8330011367797852\n",
      "Epoch [766/5000], Train Loss: 0.03155859777083, R2 Score (Train): 0.98945671688054, RMSE (Train): 0.17247050106241046\n",
      "Test Loss: 0.74082812666893, R2 Score (Test): 0.7230374974214459, RMSE (Test): 0.8254894614219666\n",
      "Epoch [771/5000], Train Loss: 0.03469611813003818, R2 Score (Train): 0.9930013942647665, RMSE (Train): 0.1405182248746243\n",
      "Test Loss: 0.695706695318222, R2 Score (Test): 0.7391044334190242, RMSE (Test): 0.8011878728866577\n",
      "Epoch [776/5000], Train Loss: 0.025946596482147772, R2 Score (Train): 0.9906809894955072, RMSE (Train): 0.16214807281953594\n",
      "Test Loss: 0.7777677178382874, R2 Score (Test): 0.7090951162964125, RMSE (Test): 0.8460120558738708\n",
      "Epoch [781/5000], Train Loss: 0.02591920007641117, R2 Score (Train): 0.9906182397714096, RMSE (Train): 0.16269307039688932\n",
      "Test Loss: 0.6645385324954987, R2 Score (Test): 0.7198965852597046, RMSE (Test): 0.8301570415496826\n",
      "Epoch [786/5000], Train Loss: 0.023766803710410993, R2 Score (Train): 0.9912217507698428, RMSE (Train): 0.15737322479895938\n",
      "Test Loss: 0.716956228017807, R2 Score (Test): 0.7224187644802224, RMSE (Test): 0.8264110088348389\n",
      "Epoch [791/5000], Train Loss: 0.03283388757457336, R2 Score (Train): 0.9895193398242902, RMSE (Train): 0.1719575348443679\n",
      "Test Loss: 0.6511018872261047, R2 Score (Test): 0.7430365002176507, RMSE (Test): 0.7951275110244751\n",
      "Epoch [796/5000], Train Loss: 0.023190311932315428, R2 Score (Train): 0.9907932447056038, RMSE (Train): 0.16116850996435722\n",
      "Test Loss: 0.6336910724639893, R2 Score (Test): 0.7303859562314934, RMSE (Test): 0.814464807510376\n",
      "Epoch [801/5000], Train Loss: 0.03021445528914531, R2 Score (Train): 0.9897900974340252, RMSE (Train): 0.16972182360634805\n",
      "Test Loss: 0.6969879865646362, R2 Score (Test): 0.73389576210687, RMSE (Test): 0.8091461062431335\n",
      "Epoch [806/5000], Train Loss: 0.027270845447977383, R2 Score (Train): 0.993231432994202, RMSE (Train): 0.13818956730185294\n",
      "Test Loss: 0.719828873872757, R2 Score (Test): 0.7201422383808842, RMSE (Test): 0.8297929167747498\n",
      "Epoch [811/5000], Train Loss: 0.025077184817443293, R2 Score (Train): 0.9943292915356304, RMSE (Train): 0.12648690060592419\n",
      "Test Loss: 0.6313741207122803, R2 Score (Test): 0.7373346947702129, RMSE (Test): 0.8039007186889648\n",
      "Epoch [816/5000], Train Loss: 0.025570109020918608, R2 Score (Train): 0.9920381491644705, RMSE (Train): 0.14987662646488403\n",
      "Test Loss: 0.7187269032001495, R2 Score (Test): 0.7242747639131275, RMSE (Test): 0.8236435055732727\n",
      "Epoch [821/5000], Train Loss: 0.022790328056241076, R2 Score (Train): 0.9920358097352489, RMSE (Train): 0.1498986439589665\n",
      "Test Loss: 0.6224025785923004, R2 Score (Test): 0.7450655986190755, RMSE (Test): 0.7919819355010986\n",
      "Epoch [826/5000], Train Loss: 0.018912763722861808, R2 Score (Train): 0.9914964269355663, RMSE (Train): 0.15489151030723014\n",
      "Test Loss: 0.6370210647583008, R2 Score (Test): 0.7330498042310672, RMSE (Test): 0.8104312419891357\n",
      "Epoch [831/5000], Train Loss: 0.02623945059409986, R2 Score (Train): 0.9856206512036161, RMSE (Train): 0.20141717474840912\n",
      "Test Loss: 0.6196309924125671, R2 Score (Test): 0.7224775187439818, RMSE (Test): 0.8263235688209534\n",
      "Epoch [836/5000], Train Loss: 0.03289446979761124, R2 Score (Train): 0.9868975332346864, RMSE (Train): 0.19226641302663286\n",
      "Test Loss: 0.7031883299350739, R2 Score (Test): 0.7134087813522356, RMSE (Test): 0.8397161960601807\n",
      "Epoch [841/5000], Train Loss: 0.030803050224979717, R2 Score (Train): 0.9915683771567183, RMSE (Train): 0.15423483619834086\n",
      "Test Loss: 0.6385425329208374, R2 Score (Test): 0.723506559717215, RMSE (Test): 0.8247901201248169\n",
      "Epoch [846/5000], Train Loss: 0.03202990194161733, R2 Score (Train): 0.9900466014998189, RMSE (Train): 0.1675762959613763\n",
      "Test Loss: 0.6084272563457489, R2 Score (Test): 0.7209647993957752, RMSE (Test): 0.8285725116729736\n",
      "Epoch [851/5000], Train Loss: 0.02194413550508519, R2 Score (Train): 0.9921846661654625, RMSE (Train): 0.14849117970611245\n",
      "Test Loss: 0.604177787899971, R2 Score (Test): 0.7315159194524828, RMSE (Test): 0.8127562999725342\n",
      "Epoch [856/5000], Train Loss: 0.026619252748787403, R2 Score (Train): 0.9908129930004043, RMSE (Train): 0.1609955656566288\n",
      "Test Loss: 0.6327495872974396, R2 Score (Test): 0.7318731037200722, RMSE (Test): 0.8122154474258423\n",
      "Epoch [861/5000], Train Loss: 0.02550806337967515, R2 Score (Train): 0.9924048122852341, RMSE (Train): 0.14638485491782635\n",
      "Test Loss: 0.6380691826343536, R2 Score (Test): 0.7364829344487447, RMSE (Test): 0.8052030801773071\n",
      "Epoch [866/5000], Train Loss: 0.035772721360748015, R2 Score (Train): 0.9887387756708926, RMSE (Train): 0.17824596017198585\n",
      "Test Loss: 0.6556295454502106, R2 Score (Test): 0.740363830805935, RMSE (Test): 0.7992518544197083\n",
      "Epoch [871/5000], Train Loss: 0.03443011501803994, R2 Score (Train): 0.991911308495196, RMSE (Train): 0.15106575541464634\n",
      "Test Loss: 0.6128049790859222, R2 Score (Test): 0.7414907247072219, RMSE (Test): 0.7975154519081116\n",
      "Epoch [876/5000], Train Loss: 0.02378149105546375, R2 Score (Train): 0.9938717993671778, RMSE (Train): 0.1314901976990424\n",
      "Test Loss: 0.6294494569301605, R2 Score (Test): 0.742912391359517, RMSE (Test): 0.7953194975852966\n",
      "Epoch [881/5000], Train Loss: 0.021588811728482444, R2 Score (Train): 0.9916713996485822, RMSE (Train): 0.15328967454063464\n",
      "Test Loss: 0.6444413363933563, R2 Score (Test): 0.7445574114920676, RMSE (Test): 0.7927708625793457\n",
      "Epoch [886/5000], Train Loss: 0.022048754850402474, R2 Score (Train): 0.9903230630653717, RMSE (Train): 0.16523264159629447\n",
      "Test Loss: 0.6689216196537018, R2 Score (Test): 0.7267641068175636, RMSE (Test): 0.8199170827865601\n",
      "Epoch [891/5000], Train Loss: 0.020801374999185402, R2 Score (Train): 0.9943823778327874, RMSE (Train): 0.12589345534427904\n",
      "Test Loss: 0.636506050825119, R2 Score (Test): 0.7451496060987606, RMSE (Test): 0.7918514609336853\n",
      "Epoch [896/5000], Train Loss: 0.021392981056123972, R2 Score (Train): 0.9909160276984706, RMSE (Train): 0.16009021627515393\n",
      "Test Loss: 0.6000489294528961, R2 Score (Test): 0.7396213478119893, RMSE (Test): 0.80039381980896\n",
      "Epoch [901/5000], Train Loss: 0.024211487888048094, R2 Score (Train): 0.9902209865089455, RMSE (Train): 0.1661018284964518\n",
      "Test Loss: 0.6164775788784027, R2 Score (Test): 0.728472184299152, RMSE (Test): 0.8173502087593079\n",
      "Epoch [906/5000], Train Loss: 0.02217435681571563, R2 Score (Train): 0.9915694634222154, RMSE (Train): 0.15422490066276107\n",
      "Test Loss: 0.5609070062637329, R2 Score (Test): 0.739906441634558, RMSE (Test): 0.7999555468559265\n",
      "Epoch [911/5000], Train Loss: 0.021240081793318193, R2 Score (Train): 0.9948881125769355, RMSE (Train): 0.12009295499512601\n",
      "Test Loss: 0.6366904675960541, R2 Score (Test): 0.7292108670847015, RMSE (Test): 0.81623774766922\n",
      "Epoch [916/5000], Train Loss: 0.01661073020659387, R2 Score (Train): 0.9912766010579744, RMSE (Train): 0.1568807864520234\n",
      "Test Loss: 0.6554519236087799, R2 Score (Test): 0.7339429919459581, RMSE (Test): 0.8090742826461792\n",
      "Epoch [921/5000], Train Loss: 0.018166915358354647, R2 Score (Train): 0.9923934538890181, RMSE (Train): 0.14649427130616388\n",
      "Test Loss: 0.603332981467247, R2 Score (Test): 0.7333675630566394, RMSE (Test): 0.809948742389679\n",
      "Epoch [926/5000], Train Loss: 0.02472223062068224, R2 Score (Train): 0.9936779702671726, RMSE (Train): 0.1335534642485706\n",
      "Test Loss: 0.635140985250473, R2 Score (Test): 0.7405839412692138, RMSE (Test): 0.7989130020141602\n",
      "Epoch [931/5000], Train Loss: 0.024937925006573398, R2 Score (Train): 0.9937930561180636, RMSE (Train): 0.13233228150373508\n",
      "Test Loss: 0.6923873126506805, R2 Score (Test): 0.7417209525852556, RMSE (Test): 0.7971602082252502\n",
      "Epoch [936/5000], Train Loss: 0.020197566753874224, R2 Score (Train): 0.9905469215469798, RMSE (Train): 0.1633102793837976\n",
      "Test Loss: 0.6040878295898438, R2 Score (Test): 0.7410369561675599, RMSE (Test): 0.7982150912284851\n",
      "Epoch [941/5000], Train Loss: 0.021241873929587502, R2 Score (Train): 0.9916431323937198, RMSE (Train): 0.15354958663428322\n",
      "Test Loss: 0.6748115420341492, R2 Score (Test): 0.7280995838718065, RMSE (Test): 0.8179108500480652\n",
      "Epoch [946/5000], Train Loss: 0.020493211612726252, R2 Score (Train): 0.9925470506568366, RMSE (Train): 0.1450076703552434\n",
      "Test Loss: 0.6554884910583496, R2 Score (Test): 0.744099881345203, RMSE (Test): 0.7934805154800415\n",
      "Epoch [951/5000], Train Loss: 0.022797775299598772, R2 Score (Train): 0.9962118547926939, RMSE (Train): 0.10338087405904291\n",
      "Test Loss: 0.614758163690567, R2 Score (Test): 0.7494266881898382, RMSE (Test): 0.7851786017417908\n",
      "Epoch [956/5000], Train Loss: 0.02666735090315342, R2 Score (Train): 0.9860738812278609, RMSE (Train): 0.19821747433056253\n",
      "Test Loss: 0.6389345526695251, R2 Score (Test): 0.742399616787345, RMSE (Test): 0.7961122393608093\n",
      "Epoch [961/5000], Train Loss: 0.029565667423109215, R2 Score (Train): 0.9829327407798064, RMSE (Train): 0.2194364572199831\n",
      "Test Loss: 0.6911924481391907, R2 Score (Test): 0.728220832394715, RMSE (Test): 0.8177285194396973\n",
      "Epoch [966/5000], Train Loss: 0.023951479932293296, R2 Score (Train): 0.9849705141115394, RMSE (Train): 0.20592020085459128\n",
      "Test Loss: 0.598550096154213, R2 Score (Test): 0.7418268414441888, RMSE (Test): 0.7969968318939209\n",
      "Epoch [971/5000], Train Loss: 0.021095721516758204, R2 Score (Train): 0.9924284986781413, RMSE (Train): 0.14615641838334578\n",
      "Test Loss: 0.6170299351215363, R2 Score (Test): 0.7438442627716313, RMSE (Test): 0.7938767075538635\n",
      "Epoch [976/5000], Train Loss: 0.024414505111053586, R2 Score (Train): 0.9922326915751198, RMSE (Train): 0.14803423575106842\n",
      "Test Loss: 0.5843078196048737, R2 Score (Test): 0.7527552563390617, RMSE (Test): 0.7799460887908936\n",
      "Epoch [981/5000], Train Loss: 0.02121363130087654, R2 Score (Train): 0.9937338217072897, RMSE (Train): 0.1329622221066234\n",
      "Test Loss: 0.6240876913070679, R2 Score (Test): 0.7452449997518618, RMSE (Test): 0.7917031645774841\n",
      "Epoch [986/5000], Train Loss: 0.0194058062043041, R2 Score (Train): 0.9948850549958477, RMSE (Train): 0.12012886531810968\n",
      "Test Loss: 0.6014984846115112, R2 Score (Test): 0.7490525322721628, RMSE (Test): 0.7857646346092224\n",
      "Epoch [991/5000], Train Loss: 0.024166419093186658, R2 Score (Train): 0.9880068970096438, RMSE (Train): 0.1839469843170067\n",
      "Test Loss: 0.6334586441516876, R2 Score (Test): 0.7463551237365794, RMSE (Test): 0.7899763584136963\n",
      "Epoch [996/5000], Train Loss: 0.020815279412393767, R2 Score (Train): 0.9910823266098245, RMSE (Train): 0.15861807385491133\n",
      "Test Loss: 0.621006041765213, R2 Score (Test): 0.7450883375124906, RMSE (Test): 0.7919465899467468\n",
      "Epoch [1001/5000], Train Loss: 0.023342439904808998, R2 Score (Train): 0.9915882949629797, RMSE (Train): 0.1540525559880197\n",
      "Test Loss: 0.6355469822883606, R2 Score (Test): 0.7406841761067774, RMSE (Test): 0.7987586259841919\n",
      "Epoch [1006/5000], Train Loss: 0.030232214213659365, R2 Score (Train): 0.9875064209303329, RMSE (Train): 0.18774584116065232\n",
      "Test Loss: 0.5802369862794876, R2 Score (Test): 0.7397537007876215, RMSE (Test): 0.8001903891563416\n",
      "Epoch [1011/5000], Train Loss: 0.02584928972646594, R2 Score (Train): 0.9853948920096482, RMSE (Train): 0.2029921656508099\n",
      "Test Loss: 0.6208477914333344, R2 Score (Test): 0.745326799317634, RMSE (Test): 0.791576087474823\n",
      "Epoch [1016/5000], Train Loss: 0.0255244728953888, R2 Score (Train): 0.9915327324772045, RMSE (Train): 0.15456050619854242\n",
      "Test Loss: 0.6047096848487854, R2 Score (Test): 0.7405839906183992, RMSE (Test): 0.7989128232002258\n",
      "Epoch [1021/5000], Train Loss: 0.021135863071928423, R2 Score (Train): 0.9920808582200235, RMSE (Train): 0.1494741009341263\n",
      "Test Loss: 0.6593019962310791, R2 Score (Test): 0.7427599359427329, RMSE (Test): 0.7955552339553833\n",
      "Epoch [1026/5000], Train Loss: 0.020825248599673312, R2 Score (Train): 0.9931567095448479, RMSE (Train): 0.13895026437262234\n",
      "Test Loss: 0.5797378718852997, R2 Score (Test): 0.765500695919768, RMSE (Test): 0.7595770359039307\n",
      "Epoch [1031/5000], Train Loss: 0.024709913646802306, R2 Score (Train): 0.9912930641683428, RMSE (Train): 0.15673268102700819\n",
      "Test Loss: 0.6236642599105835, R2 Score (Test): 0.73598864737777, RMSE (Test): 0.8059578537940979\n",
      "Epoch [1036/5000], Train Loss: 0.017656279417375725, R2 Score (Train): 0.9851474584076405, RMSE (Train): 0.2047044479007709\n",
      "Test Loss: 0.629581868648529, R2 Score (Test): 0.7329307299835022, RMSE (Test): 0.8106119632720947\n",
      "Epoch [1041/5000], Train Loss: 0.017857930002113182, R2 Score (Train): 0.9923102523892334, RMSE (Train): 0.1472932800771388\n",
      "Test Loss: 0.6740728318691254, R2 Score (Test): 0.7415611728017186, RMSE (Test): 0.797406792640686\n",
      "Epoch [1046/5000], Train Loss: 0.02830547047778964, R2 Score (Train): 0.9891336928142521, RMSE (Train): 0.17509263549403717\n",
      "Test Loss: 0.629764050245285, R2 Score (Test): 0.7672691990771334, RMSE (Test): 0.7567073702812195\n",
      "Epoch [1051/5000], Train Loss: 0.018957248112807672, R2 Score (Train): 0.9884501651497695, RMSE (Train): 0.1805156160595824\n",
      "Test Loss: 0.6633776128292084, R2 Score (Test): 0.7479927463265916, RMSE (Test): 0.7874220013618469\n",
      "Epoch [1056/5000], Train Loss: 0.019883221248164773, R2 Score (Train): 0.9910423861318164, RMSE (Train): 0.15897288635136553\n",
      "Test Loss: 0.6557855010032654, R2 Score (Test): 0.7444973366895782, RMSE (Test): 0.7928640842437744\n",
      "Epoch [1061/5000], Train Loss: 0.01694871811196208, R2 Score (Train): 0.9944706539271992, RMSE (Train): 0.12490038496971187\n",
      "Test Loss: 0.5854002833366394, R2 Score (Test): 0.750152880113058, RMSE (Test): 0.7840400338172913\n",
      "Epoch [1066/5000], Train Loss: 0.01645417232066393, R2 Score (Train): 0.9928142480209726, RMSE (Train): 0.1423845943409698\n",
      "Test Loss: 0.5754830539226532, R2 Score (Test): 0.7512715475002854, RMSE (Test): 0.7822827696800232\n",
      "Epoch [1071/5000], Train Loss: 0.02612159913405776, R2 Score (Train): 0.9954089229078779, RMSE (Train): 0.11381098534607346\n",
      "Test Loss: 0.5734543949365616, R2 Score (Test): 0.7547943857170274, RMSE (Test): 0.7767232060432434\n",
      "Epoch [1076/5000], Train Loss: 0.017352367440859478, R2 Score (Train): 0.9903926933707615, RMSE (Train): 0.164637103433919\n",
      "Test Loss: 0.6079410314559937, R2 Score (Test): 0.7459981578925647, RMSE (Test): 0.7905320525169373\n",
      "Epoch [1081/5000], Train Loss: 0.02046492788940668, R2 Score (Train): 0.9924660989934476, RMSE (Train): 0.1457930580063302\n",
      "Test Loss: 0.6647871136665344, R2 Score (Test): 0.7483502605918744, RMSE (Test): 0.7868632674217224\n",
      "Epoch [1086/5000], Train Loss: 0.017363041173666716, R2 Score (Train): 0.989678937261308, RMSE (Train): 0.17064324440822126\n",
      "Test Loss: 0.5972325205802917, R2 Score (Test): 0.74659184796432, RMSE (Test): 0.789607584476471\n",
      "Epoch [1091/5000], Train Loss: 0.02140820010875662, R2 Score (Train): 0.9933973426490488, RMSE (Train): 0.13648542317142973\n",
      "Test Loss: 0.6645308434963226, R2 Score (Test): 0.7529781677626174, RMSE (Test): 0.779594361782074\n",
      "Epoch [1096/5000], Train Loss: 0.016950400002921622, R2 Score (Train): 0.994426929063549, RMSE (Train): 0.1253932550226455\n",
      "Test Loss: 0.6617507636547089, R2 Score (Test): 0.7463462165602739, RMSE (Test): 0.7899901866912842\n",
      "Epoch [1101/5000], Train Loss: 0.022448854831357796, R2 Score (Train): 0.9937220661366543, RMSE (Train): 0.13308688456634063\n",
      "Test Loss: 0.559337392449379, R2 Score (Test): 0.7507262481412921, RMSE (Test): 0.7831398248672485\n",
      "Epoch [1106/5000], Train Loss: 0.015126490189383427, R2 Score (Train): 0.9921740778934066, RMSE (Train): 0.14859173413368268\n",
      "Test Loss: 0.661400556564331, R2 Score (Test): 0.7550182174628715, RMSE (Test): 0.7763685584068298\n",
      "Epoch [1111/5000], Train Loss: 0.018273450434207916, R2 Score (Train): 0.9925482987494735, RMSE (Train): 0.14499552814314956\n",
      "Test Loss: 0.627412348985672, R2 Score (Test): 0.7538882921221499, RMSE (Test): 0.7781569361686707\n",
      "Epoch [1116/5000], Train Loss: 0.016034920932725072, R2 Score (Train): 0.99303195253887, RMSE (Train): 0.140211114279093\n",
      "Test Loss: 0.5276781320571899, R2 Score (Test): 0.758601296231651, RMSE (Test): 0.7706701159477234\n",
      "Epoch [1121/5000], Train Loss: 0.017862731008790433, R2 Score (Train): 0.9950049499826954, RMSE (Train): 0.1187125984954975\n",
      "Test Loss: 0.55796679854393, R2 Score (Test): 0.7519614872979143, RMSE (Test): 0.7811970710754395\n",
      "Epoch [1126/5000], Train Loss: 0.01751705965337654, R2 Score (Train): 0.9923230492879119, RMSE (Train): 0.14717066993971745\n",
      "Test Loss: 0.594816267490387, R2 Score (Test): 0.7493468308739978, RMSE (Test): 0.7853037118911743\n",
      "Epoch [1131/5000], Train Loss: 0.017659948207437992, R2 Score (Train): 0.9942948097057445, RMSE (Train): 0.12687088169619512\n",
      "Test Loss: 0.5754669010639191, R2 Score (Test): 0.7548958823034646, RMSE (Test): 0.7765623927116394\n",
      "Epoch [1136/5000], Train Loss: 0.016708949813619256, R2 Score (Train): 0.9922016864753236, RMSE (Train): 0.14832939881007753\n",
      "Test Loss: 0.6741093993186951, R2 Score (Test): 0.7418941454271757, RMSE (Test): 0.7968929409980774\n",
      "Epoch [1141/5000], Train Loss: 0.015865166982014973, R2 Score (Train): 0.9905700781192253, RMSE (Train): 0.1631101316176061\n",
      "Test Loss: 0.6056674718856812, R2 Score (Test): 0.7531080060644131, RMSE (Test): 0.779389500617981\n",
      "Epoch [1146/5000], Train Loss: 0.019013350130990148, R2 Score (Train): 0.9915008565038052, RMSE (Train): 0.15485116303900223\n",
      "Test Loss: 0.6357616186141968, R2 Score (Test): 0.7455185994850573, RMSE (Test): 0.7912779450416565\n",
      "Epoch [1151/5000], Train Loss: 0.022475503385066986, R2 Score (Train): 0.9880487601584818, RMSE (Train): 0.18362566081652248\n",
      "Test Loss: 0.7137731909751892, R2 Score (Test): 0.7353128037982717, RMSE (Test): 0.8069887757301331\n",
      "Epoch [1156/5000], Train Loss: 0.020579274899015825, R2 Score (Train): 0.9951198448344176, RMSE (Train): 0.11733935754168967\n",
      "Test Loss: 0.653899222612381, R2 Score (Test): 0.7448137482044407, RMSE (Test): 0.79237300157547\n",
      "Epoch [1161/5000], Train Loss: 0.01872101309709251, R2 Score (Train): 0.9929852610965436, RMSE (Train): 0.14068009277207594\n",
      "Test Loss: 0.5973666310310364, R2 Score (Test): 0.7515237569501657, RMSE (Test): 0.7818860411643982\n",
      "Epoch [1166/5000], Train Loss: 0.016589868580922484, R2 Score (Train): 0.9895280945524496, RMSE (Train): 0.17188569986637825\n",
      "Test Loss: 0.5972641408443451, R2 Score (Test): 0.7531902125112018, RMSE (Test): 0.7792597413063049\n",
      "Epoch [1171/5000], Train Loss: 0.019898655591532588, R2 Score (Train): 0.9941379914062337, RMSE (Train): 0.12860270871113252\n",
      "Test Loss: 0.6554707586765289, R2 Score (Test): 0.7539756292020678, RMSE (Test): 0.7780188322067261\n",
      "Epoch [1176/5000], Train Loss: 0.017921155784279108, R2 Score (Train): 0.9897356410867173, RMSE (Train): 0.17017384257660834\n",
      "Test Loss: 0.6362811326980591, R2 Score (Test): 0.7427336571726908, RMSE (Test): 0.7955958843231201\n",
      "Epoch [1181/5000], Train Loss: 0.019563885987736285, R2 Score (Train): 0.9899674448034692, RMSE (Train): 0.16824132095294064\n",
      "Test Loss: 0.5511041581630707, R2 Score (Test): 0.743959829563464, RMSE (Test): 0.7936976552009583\n",
      "Epoch [1186/5000], Train Loss: 0.024172694655135274, R2 Score (Train): 0.9910715847838376, RMSE (Train): 0.15871357718226548\n",
      "Test Loss: 0.541620671749115, R2 Score (Test): 0.7503923297219368, RMSE (Test): 0.7836642265319824\n",
      "Epoch [1191/5000], Train Loss: 0.025292971016218264, R2 Score (Train): 0.9929954458063233, RMSE (Train): 0.1405779288583319\n",
      "Test Loss: 0.6428552865982056, R2 Score (Test): 0.7422184287784059, RMSE (Test): 0.7963922023773193\n",
      "Epoch [1196/5000], Train Loss: 0.02221744825753073, R2 Score (Train): 0.9898068907627794, RMSE (Train): 0.16958218626247581\n",
      "Test Loss: 0.6653398871421814, R2 Score (Test): 0.7483811337547206, RMSE (Test): 0.7868149876594543\n",
      "Epoch [1201/5000], Train Loss: 0.014506775264938673, R2 Score (Train): 0.993687833523269, RMSE (Train): 0.1334492424827975\n",
      "Test Loss: 0.5894921720027924, R2 Score (Test): 0.7509359533325333, RMSE (Test): 0.7828103303909302\n",
      "Epoch [1206/5000], Train Loss: 0.019579568800205987, R2 Score (Train): 0.9902935376183872, RMSE (Train): 0.16548452150576604\n",
      "Test Loss: 0.6086769700050354, R2 Score (Test): 0.7693916800784781, RMSE (Test): 0.7532489895820618\n",
      "Epoch [1211/5000], Train Loss: 0.020980512024834752, R2 Score (Train): 0.9929048237034817, RMSE (Train): 0.14148437679614267\n",
      "Test Loss: 0.5838277339935303, R2 Score (Test): 0.7488594346938591, RMSE (Test): 0.7860668897628784\n",
      "Epoch [1216/5000], Train Loss: 0.024009801719027262, R2 Score (Train): 0.9913844613666224, RMSE (Train): 0.1559078949480704\n",
      "Test Loss: 0.6643101274967194, R2 Score (Test): 0.7401526675339345, RMSE (Test): 0.7995767593383789\n",
      "Epoch [1221/5000], Train Loss: 0.020030345922956865, R2 Score (Train): 0.9939207705315852, RMSE (Train): 0.13096376716184646\n",
      "Test Loss: 0.5684107542037964, R2 Score (Test): 0.7518195413106628, RMSE (Test): 0.7814205884933472\n",
      "Epoch [1226/5000], Train Loss: 0.014586008891152838, R2 Score (Train): 0.9922057304168685, RMSE (Train): 0.14829093451904568\n",
      "Test Loss: 0.605841189622879, R2 Score (Test): 0.7484437257339289, RMSE (Test): 0.7867171168327332\n",
      "Epoch [1231/5000], Train Loss: 0.015404700223977367, R2 Score (Train): 0.9924539367725342, RMSE (Train): 0.14591068978395721\n",
      "Test Loss: 0.5936979055404663, R2 Score (Test): 0.7468961222183592, RMSE (Test): 0.7891334295272827\n",
      "Epoch [1236/5000], Train Loss: 0.015689357028653223, R2 Score (Train): 0.9921117053042728, RMSE (Train): 0.14918269694238515\n",
      "Test Loss: 0.5967990756034851, R2 Score (Test): 0.7508689704046605, RMSE (Test): 0.7829156517982483\n",
      "Epoch [1241/5000], Train Loss: 0.014904783417781195, R2 Score (Train): 0.9898798808100401, RMSE (Train): 0.1689739297570858\n",
      "Test Loss: 0.6001575291156769, R2 Score (Test): 0.7430656178210283, RMSE (Test): 0.7950824499130249\n",
      "Epoch [1246/5000], Train Loss: 0.026106380159035325, R2 Score (Train): 0.9909375857552932, RMSE (Train): 0.1599001406170331\n",
      "Test Loss: 0.649174839258194, R2 Score (Test): 0.7490971962678958, RMSE (Test): 0.7856946587562561\n",
      "Epoch [1251/5000], Train Loss: 0.01798189423667888, R2 Score (Train): 0.99321534011094, RMSE (Train): 0.13835374889888696\n",
      "Test Loss: 0.6752225756645203, R2 Score (Test): 0.7500117276996235, RMSE (Test): 0.7842614054679871\n",
      "Epoch [1256/5000], Train Loss: 0.01862589626883467, R2 Score (Train): 0.991979835579399, RMSE (Train): 0.15042448267318212\n",
      "Test Loss: 0.6205265522003174, R2 Score (Test): 0.7469792331404395, RMSE (Test): 0.7890039086341858\n",
      "Epoch [1261/5000], Train Loss: 0.019497760649149615, R2 Score (Train): 0.9902927091079136, RMSE (Train): 0.16549158395181085\n",
      "Test Loss: 0.5545967072248459, R2 Score (Test): 0.7619893510760734, RMSE (Test): 0.7652427554130554\n",
      "Epoch [1266/5000], Train Loss: 0.01392106005611519, R2 Score (Train): 0.994100540589744, RMSE (Train): 0.12901285894924328\n",
      "Test Loss: 0.6012423932552338, R2 Score (Test): 0.7540747700217278, RMSE (Test): 0.777862012386322\n",
      "Epoch [1271/5000], Train Loss: 0.015421025066946944, R2 Score (Train): 0.9913408192180864, RMSE (Train): 0.15630227311256184\n",
      "Test Loss: 0.6226341128349304, R2 Score (Test): 0.7507933947981966, RMSE (Test): 0.7830343246459961\n",
      "Epoch [1276/5000], Train Loss: 0.014487717921535173, R2 Score (Train): 0.9918033947996541, RMSE (Train): 0.1520701237313691\n",
      "Test Loss: 0.6151748895645142, R2 Score (Test): 0.7445251938164014, RMSE (Test): 0.7928208708763123\n",
      "Epoch [1281/5000], Train Loss: 0.014350099644313255, R2 Score (Train): 0.9939234773143171, RMSE (Train): 0.130934608044446\n",
      "Test Loss: 0.6365330219268799, R2 Score (Test): 0.752432657753874, RMSE (Test): 0.7804547548294067\n",
      "Epoch [1286/5000], Train Loss: 0.013854603903988997, R2 Score (Train): 0.9938588035106097, RMSE (Train): 0.13162954714666739\n",
      "Test Loss: 0.6212566196918488, R2 Score (Test): 0.7529696588907644, RMSE (Test): 0.779607892036438\n",
      "Epoch [1291/5000], Train Loss: 0.018888698192313313, R2 Score (Train): 0.9888613255795071, RMSE (Train): 0.17727342910871216\n",
      "Test Loss: 0.6082983016967773, R2 Score (Test): 0.7441064284437897, RMSE (Test): 0.7934704422950745\n",
      "Epoch [1296/5000], Train Loss: 0.014350944431498647, R2 Score (Train): 0.9888764103041472, RMSE (Train): 0.17715335077873137\n",
      "Test Loss: 0.5912878811359406, R2 Score (Test): 0.7501259074671724, RMSE (Test): 0.784082293510437\n",
      "Epoch [1301/5000], Train Loss: 0.018024200728784006, R2 Score (Train): 0.9917358230448675, RMSE (Train): 0.15269566039770088\n",
      "Test Loss: 0.538687527179718, R2 Score (Test): 0.7574890103021682, RMSE (Test): 0.7724435925483704\n",
      "Epoch [1306/5000], Train Loss: 0.017921292688697577, R2 Score (Train): 0.9888556434666382, RMSE (Train): 0.17731863912055995\n",
      "Test Loss: 0.6087287664413452, R2 Score (Test): 0.7454994741710399, RMSE (Test): 0.7913076877593994\n",
      "Epoch [1311/5000], Train Loss: 0.02205677446909249, R2 Score (Train): 0.9916089278687252, RMSE (Train): 0.15386350374477142\n",
      "Test Loss: 0.59935262799263, R2 Score (Test): 0.7515955262732398, RMSE (Test): 0.7817731499671936\n",
      "Epoch [1316/5000], Train Loss: 0.016036728862673044, R2 Score (Train): 0.9898979187617366, RMSE (Train): 0.16882327427122276\n",
      "Test Loss: 0.6003983020782471, R2 Score (Test): 0.7593625336984822, RMSE (Test): 0.7694540023803711\n",
      "Epoch [1321/5000], Train Loss: 0.016507919877767563, R2 Score (Train): 0.9928814098813938, RMSE (Train): 0.1417176311544024\n",
      "Test Loss: 0.6092775464057922, R2 Score (Test): 0.7500913006754792, RMSE (Test): 0.7841365933418274\n",
      "Epoch [1326/5000], Train Loss: 0.01363447067948679, R2 Score (Train): 0.9922767825419744, RMSE (Train): 0.1476134811496175\n",
      "Test Loss: 0.5899502635002136, R2 Score (Test): 0.7519916978729712, RMSE (Test): 0.7811494469642639\n",
      "Epoch [1331/5000], Train Loss: 0.015496017644181848, R2 Score (Train): 0.9918196477406784, RMSE (Train): 0.15191927999011148\n",
      "Test Loss: 0.6170784831047058, R2 Score (Test): 0.7389335605673115, RMSE (Test): 0.801450252532959\n",
      "Epoch [1336/5000], Train Loss: 0.018219266086816788, R2 Score (Train): 0.9911497492256603, RMSE (Train): 0.1580173153515771\n",
      "Test Loss: 0.6296201050281525, R2 Score (Test): 0.7504640317097162, RMSE (Test): 0.7835516929626465\n",
      "Epoch [1341/5000], Train Loss: 0.0194390785569946, R2 Score (Train): 0.9893703010680557, RMSE (Train): 0.1731758678388049\n",
      "Test Loss: 0.5996951758861542, R2 Score (Test): 0.7354252148311919, RMSE (Test): 0.8068174123764038\n",
      "Epoch [1346/5000], Train Loss: 0.01579544215928763, R2 Score (Train): 0.9927712056810556, RMSE (Train): 0.14281039640382662\n",
      "Test Loss: 0.6495064496994019, R2 Score (Test): 0.7360653784347875, RMSE (Test): 0.8058407306671143\n",
      "Epoch [1351/5000], Train Loss: 0.01634347578510642, R2 Score (Train): 0.9924765525405757, RMSE (Train): 0.14569187645124057\n",
      "Test Loss: 0.615961104631424, R2 Score (Test): 0.7413346702571759, RMSE (Test): 0.7977561354637146\n",
      "Epoch [1356/5000], Train Loss: 0.021678538682560127, R2 Score (Train): 0.9891162774705458, RMSE (Train): 0.1752328891046704\n",
      "Test Loss: 0.6651028692722321, R2 Score (Test): 0.7498286293505679, RMSE (Test): 0.7845485210418701\n",
      "Epoch [1361/5000], Train Loss: 0.018646137012789648, R2 Score (Train): 0.9924048667955083, RMSE (Train): 0.1463843296185718\n",
      "Test Loss: 0.6013340651988983, R2 Score (Test): 0.7474564187798456, RMSE (Test): 0.7882595062255859\n",
      "Epoch [1366/5000], Train Loss: 0.012789073710640272, R2 Score (Train): 0.9933379363871258, RMSE (Train): 0.1370980501922499\n",
      "Test Loss: 0.6144441068172455, R2 Score (Test): 0.7483806477745243, RMSE (Test): 0.7868158221244812\n",
      "Epoch [1371/5000], Train Loss: 0.012009763934959969, R2 Score (Train): 0.995315118634931, RMSE (Train): 0.11496779170942036\n",
      "Test Loss: 0.5909373760223389, R2 Score (Test): 0.7481338043831156, RMSE (Test): 0.7872016429901123\n",
      "Epoch [1376/5000], Train Loss: 0.017222561019783218, R2 Score (Train): 0.9890670796850372, RMSE (Train): 0.17562849591805152\n",
      "Test Loss: 0.6382135152816772, R2 Score (Test): 0.7421784794535047, RMSE (Test): 0.7964538931846619\n",
      "Epoch [1381/5000], Train Loss: 0.01835413572068016, R2 Score (Train): 0.9901098295393514, RMSE (Train): 0.16704319156044353\n",
      "Test Loss: 0.6310706734657288, R2 Score (Test): 0.7334616631051906, RMSE (Test): 0.8098058104515076\n",
      "Epoch [1386/5000], Train Loss: 0.017434776993468404, R2 Score (Train): 0.9908129981830901, RMSE (Train): 0.16099552024523642\n",
      "Test Loss: 0.6366815268993378, R2 Score (Test): 0.7494009627844344, RMSE (Test): 0.7852188944816589\n",
      "Epoch [1391/5000], Train Loss: 0.014713500315944353, R2 Score (Train): 0.9907868180130832, RMSE (Train): 0.16122475126398889\n",
      "Test Loss: 0.6281494200229645, R2 Score (Test): 0.7369855806691088, RMSE (Test): 0.8044347763061523\n",
      "Epoch [1396/5000], Train Loss: 0.013537983992137015, R2 Score (Train): 0.9933590351072933, RMSE (Train): 0.13688078363601\n",
      "Test Loss: 0.6045692563056946, R2 Score (Test): 0.7466271376246454, RMSE (Test): 0.789552628993988\n",
      "Epoch [1401/5000], Train Loss: 0.018932038607696693, R2 Score (Train): 0.9940832791977596, RMSE (Train): 0.12920146224200812\n",
      "Test Loss: 0.6352829337120056, R2 Score (Test): 0.7414088135403125, RMSE (Test): 0.7976418137550354\n",
      "Epoch [1406/5000], Train Loss: 0.0174910849891603, R2 Score (Train): 0.9895622499640386, RMSE (Train): 0.17160515771557566\n",
      "Test Loss: 0.6194805800914764, R2 Score (Test): 0.7387059866156958, RMSE (Test): 0.801799476146698\n",
      "Epoch [1411/5000], Train Loss: 0.015083175618201494, R2 Score (Train): 0.9931643374303323, RMSE (Train): 0.13887280220509912\n",
      "Test Loss: 0.66549351811409, R2 Score (Test): 0.7446128790783857, RMSE (Test): 0.7926848530769348\n",
      "Epoch [1416/5000], Train Loss: 0.013810569265236458, R2 Score (Train): 0.9895858321134263, RMSE (Train): 0.17141119317366468\n",
      "Test Loss: 0.5608260929584503, R2 Score (Test): 0.7513686459867192, RMSE (Test): 0.7821300625801086\n",
      "Epoch [1421/5000], Train Loss: 0.01557295365879933, R2 Score (Train): 0.9924060211778086, RMSE (Train): 0.1463732047379393\n",
      "Test Loss: 0.586367666721344, R2 Score (Test): 0.7595694917389587, RMSE (Test): 0.7691230773925781\n",
      "Epoch [1426/5000], Train Loss: 0.016511475201696157, R2 Score (Train): 0.9896544411963385, RMSE (Train): 0.17084562718534488\n",
      "Test Loss: 0.5814313888549805, R2 Score (Test): 0.755399537160476, RMSE (Test): 0.7757641077041626\n",
      "Epoch [1431/5000], Train Loss: 0.018394379410892725, R2 Score (Train): 0.9887060227358032, RMSE (Train): 0.1785049834977701\n",
      "Test Loss: 0.6426196396350861, R2 Score (Test): 0.7492279487701246, RMSE (Test): 0.7854899168014526\n",
      "Epoch [1436/5000], Train Loss: 0.017866141628473997, R2 Score (Train): 0.9904280730380625, RMSE (Train): 0.16433367924626685\n",
      "Test Loss: 0.571408748626709, R2 Score (Test): 0.757496495682364, RMSE (Test): 0.7724316716194153\n",
      "Epoch [1441/5000], Train Loss: 0.01707858716448148, R2 Score (Train): 0.992200218098391, RMSE (Train): 0.14834336293290584\n",
      "Test Loss: 0.5960609614849091, R2 Score (Test): 0.7451631558814691, RMSE (Test): 0.7918304204940796\n",
      "Epoch [1446/5000], Train Loss: 0.019256045576184988, R2 Score (Train): 0.994510713839346, RMSE (Train): 0.1244471131087572\n",
      "Test Loss: 0.6260687112808228, R2 Score (Test): 0.7464115733448792, RMSE (Test): 0.7898885011672974\n",
      "Epoch [1451/5000], Train Loss: 0.016517365894590814, R2 Score (Train): 0.994167716819307, RMSE (Train): 0.1282762312688869\n",
      "Test Loss: 0.5612401366233826, R2 Score (Test): 0.7531372675050924, RMSE (Test): 0.77934330701828\n",
      "Epoch [1456/5000], Train Loss: 0.016719848771269124, R2 Score (Train): 0.989784138803393, RMSE (Train): 0.16977134230352753\n",
      "Test Loss: 0.612728476524353, R2 Score (Test): 0.7503609461535516, RMSE (Test): 0.7837134599685669\n",
      "Epoch [1461/5000], Train Loss: 0.01298574967465053, R2 Score (Train): 0.9916945831571445, RMSE (Train): 0.1530761771421411\n",
      "Test Loss: 0.6153807938098907, R2 Score (Test): 0.7510049715897258, RMSE (Test): 0.7827019095420837\n",
      "Epoch [1466/5000], Train Loss: 0.01675804619056483, R2 Score (Train): 0.9907240288574083, RMSE (Train): 0.16177320329917974\n",
      "Test Loss: 0.6741522252559662, R2 Score (Test): 0.7449910558621557, RMSE (Test): 0.7920976877212524\n",
      "Epoch [1471/5000], Train Loss: 0.0108722149549673, R2 Score (Train): 0.9945588020608057, RMSE (Train): 0.12390081215316594\n",
      "Test Loss: 0.5805732905864716, R2 Score (Test): 0.7612273739017296, RMSE (Test): 0.7664667367935181\n",
      "Epoch [1476/5000], Train Loss: 0.016856640965367358, R2 Score (Train): 0.9903722699535177, RMSE (Train): 0.16481200506299434\n",
      "Test Loss: 0.6780466437339783, R2 Score (Test): 0.7416697936016787, RMSE (Test): 0.7972391843795776\n",
      "Epoch [1481/5000], Train Loss: 0.01682435014906029, R2 Score (Train): 0.9926060826402449, RMSE (Train): 0.1444322533112598\n",
      "Test Loss: 0.5792624652385712, R2 Score (Test): 0.7561607252703667, RMSE (Test): 0.7745561599731445\n",
      "Epoch [1486/5000], Train Loss: 0.014041568851098418, R2 Score (Train): 0.9910670861546601, RMSE (Train): 0.15875355648414557\n",
      "Test Loss: 0.6386091709136963, R2 Score (Test): 0.7491357042584096, RMSE (Test): 0.7856343984603882\n",
      "Epoch [1491/5000], Train Loss: 0.014821901529406508, R2 Score (Train): 0.9922423416246695, RMSE (Train): 0.1479422488227127\n",
      "Test Loss: 0.564738467335701, R2 Score (Test): 0.7526449474342909, RMSE (Test): 0.7801200747489929\n",
      "Epoch [1496/5000], Train Loss: 0.01982571747309218, R2 Score (Train): 0.9870196654880532, RMSE (Train): 0.19136822691201905\n",
      "Test Loss: 0.601736456155777, R2 Score (Test): 0.7550738001729327, RMSE (Test): 0.7762804627418518\n",
      "Epoch [1501/5000], Train Loss: 0.019459039593736332, R2 Score (Train): 0.983938880096694, RMSE (Train): 0.2128701679615618\n",
      "Test Loss: 0.6592714488506317, R2 Score (Test): 0.7453432641921627, RMSE (Test): 0.7915504574775696\n",
      "Epoch [1506/5000], Train Loss: 0.01602314435876906, R2 Score (Train): 0.9887253992845254, RMSE (Train): 0.17835179143651472\n",
      "Test Loss: 0.6165318787097931, R2 Score (Test): 0.7524303261735936, RMSE (Test): 0.780458390712738\n",
      "Epoch [1511/5000], Train Loss: 0.013532798504456878, R2 Score (Train): 0.9918646099616661, RMSE (Train): 0.1515012026374482\n",
      "Test Loss: 0.5391069501638412, R2 Score (Test): 0.7507381429045985, RMSE (Test): 0.7831211686134338\n",
      "Epoch [1516/5000], Train Loss: 0.01266503392253071, R2 Score (Train): 0.9922839381752229, RMSE (Train): 0.14754508266716373\n",
      "Test Loss: 0.6237812638282776, R2 Score (Test): 0.7550391331511077, RMSE (Test): 0.7763354182243347\n",
      "Epoch [1521/5000], Train Loss: 0.010256206694369515, R2 Score (Train): 0.9921757298880048, RMSE (Train): 0.14857604999483492\n",
      "Test Loss: 0.5634328126907349, R2 Score (Test): 0.7614269492346756, RMSE (Test): 0.7661463618278503\n",
      "Epoch [1526/5000], Train Loss: 0.015607006731443107, R2 Score (Train): 0.9874434767146577, RMSE (Train): 0.18821819049706048\n",
      "Test Loss: 0.6552546918392181, R2 Score (Test): 0.7532251898771705, RMSE (Test): 0.7792044878005981\n",
      "Epoch [1531/5000], Train Loss: 0.016816133516840637, R2 Score (Train): 0.9905437879923341, RMSE (Train): 0.16333734460310875\n",
      "Test Loss: 0.5846587419509888, R2 Score (Test): 0.7586292981733169, RMSE (Test): 0.7706254124641418\n",
      "Epoch [1536/5000], Train Loss: 0.016981289566804964, R2 Score (Train): 0.9945746085561115, RMSE (Train): 0.12372071743910716\n",
      "Test Loss: 0.6355196833610535, R2 Score (Test): 0.7542217308238468, RMSE (Test): 0.7776296138763428\n",
      "Epoch [1541/5000], Train Loss: 0.016739975777454674, R2 Score (Train): 0.9892136152823745, RMSE (Train): 0.17444753767835633\n",
      "Test Loss: 0.6959222853183746, R2 Score (Test): 0.7505604839459891, RMSE (Test): 0.7834001779556274\n",
      "Epoch [1546/5000], Train Loss: 0.013490729072752098, R2 Score (Train): 0.9921171883660268, RMSE (Train): 0.14913084035216742\n",
      "Test Loss: 0.5961134731769562, R2 Score (Test): 0.763773116635331, RMSE (Test): 0.7623698115348816\n",
      "Epoch [1551/5000], Train Loss: 0.015149546593117217, R2 Score (Train): 0.9911091321890597, RMSE (Train): 0.15837949989487854\n",
      "Test Loss: 0.6116092503070831, R2 Score (Test): 0.7544834363314511, RMSE (Test): 0.7772154808044434\n",
      "Epoch [1556/5000], Train Loss: 0.013329184769342342, R2 Score (Train): 0.9935594117466026, RMSE (Train): 0.13479992745684408\n",
      "Test Loss: 0.6031153500080109, R2 Score (Test): 0.7537388418384791, RMSE (Test): 0.7783931493759155\n",
      "Epoch [1561/5000], Train Loss: 0.011990083381533623, R2 Score (Train): 0.9932390379566203, RMSE (Train): 0.13811191262040529\n",
      "Test Loss: 0.5939968526363373, R2 Score (Test): 0.7511558445041344, RMSE (Test): 0.7824647426605225\n",
      "Epoch [1566/5000], Train Loss: 0.015211287109802166, R2 Score (Train): 0.9933848697298745, RMSE (Train): 0.13661427799022016\n",
      "Test Loss: 0.6417094469070435, R2 Score (Test): 0.751676524961313, RMSE (Test): 0.7816457152366638\n",
      "Epoch [1571/5000], Train Loss: 0.013192060558746258, R2 Score (Train): 0.9914240805951603, RMSE (Train): 0.1555490045491896\n",
      "Test Loss: 0.5800583958625793, R2 Score (Test): 0.7589559945359813, RMSE (Test): 0.7701038122177124\n",
      "Epoch [1576/5000], Train Loss: 0.011965326421583692, R2 Score (Train): 0.9915348382337313, RMSE (Train): 0.15454128588739988\n",
      "Test Loss: 0.6701747477054596, R2 Score (Test): 0.7448195639524076, RMSE (Test): 0.7923640012741089\n",
      "Epoch [1581/5000], Train Loss: 0.012562809977680445, R2 Score (Train): 0.9917216885473203, RMSE (Train): 0.15282618485248675\n",
      "Test Loss: 0.586829662322998, R2 Score (Test): 0.7589880121237201, RMSE (Test): 0.7700526118278503\n",
      "Epoch [1586/5000], Train Loss: 0.01548154306753228, R2 Score (Train): 0.991319356768526, RMSE (Train): 0.1564958568725097\n",
      "Test Loss: 0.6385151743888855, R2 Score (Test): 0.7548311605956162, RMSE (Test): 0.7766649127006531\n",
      "Epoch [1591/5000], Train Loss: 0.014047790008286635, R2 Score (Train): 0.9860310395978771, RMSE (Train): 0.19852213346764255\n",
      "Test Loss: 0.6663456559181213, R2 Score (Test): 0.7422100299864349, RMSE (Test): 0.7964051961898804\n",
      "Epoch [1596/5000], Train Loss: 0.013941459668179354, R2 Score (Train): 0.9924750579162552, RMSE (Train): 0.14570634746397657\n",
      "Test Loss: 0.6003128886222839, R2 Score (Test): 0.753641507649516, RMSE (Test): 0.778546929359436\n",
      "Epoch [1601/5000], Train Loss: 0.014671922428533435, R2 Score (Train): 0.9915999760906589, RMSE (Train): 0.1539455543316972\n",
      "Test Loss: 0.5550891757011414, R2 Score (Test): 0.7532899171525046, RMSE (Test): 0.7791023254394531\n",
      "Epoch [1606/5000], Train Loss: 0.019448800167689722, R2 Score (Train): 0.9883441243930071, RMSE (Train): 0.1813423931047651\n",
      "Test Loss: 0.60639688372612, R2 Score (Test): 0.7473686950664826, RMSE (Test): 0.7883963584899902\n",
      "Epoch [1611/5000], Train Loss: 0.014155823232916495, R2 Score (Train): 0.9928974333530474, RMSE (Train): 0.1415580428294983\n",
      "Test Loss: 0.6175232529640198, R2 Score (Test): 0.7484552272723453, RMSE (Test): 0.7866991758346558\n",
      "Epoch [1616/5000], Train Loss: 0.01836100989021361, R2 Score (Train): 0.9878340762901692, RMSE (Train): 0.18526758270496108\n",
      "Test Loss: 0.6250796318054199, R2 Score (Test): 0.7497157730074149, RMSE (Test): 0.784725546836853\n",
      "Epoch [1621/5000], Train Loss: 0.015824834040055673, R2 Score (Train): 0.9923783106084347, RMSE (Train): 0.14664002082294128\n",
      "Test Loss: 0.6824373602867126, R2 Score (Test): 0.7438601016363896, RMSE (Test): 0.7938522100448608\n",
      "Epoch [1626/5000], Train Loss: 0.013620593662684163, R2 Score (Train): 0.9911935015536438, RMSE (Train): 0.1576262421851106\n",
      "Test Loss: 0.626678854227066, R2 Score (Test): 0.7430932851446976, RMSE (Test): 0.7950395941734314\n",
      "Epoch [1631/5000], Train Loss: 0.012703433555240432, R2 Score (Train): 0.9942120924327867, RMSE (Train): 0.12778729705798908\n",
      "Test Loss: 0.5695928335189819, R2 Score (Test): 0.752315555396258, RMSE (Test): 0.7806392908096313\n",
      "Epoch [1636/5000], Train Loss: 0.01910336862783879, R2 Score (Train): 0.9889896522532102, RMSE (Train): 0.17624930315210385\n",
      "Test Loss: 0.6106740236282349, R2 Score (Test): 0.7454894505737437, RMSE (Test): 0.7913232445716858\n",
      "Epoch [1641/5000], Train Loss: 0.013905011971170703, R2 Score (Train): 0.9924633790802364, RMSE (Train): 0.1458193729711968\n",
      "Test Loss: 0.6888889670372009, R2 Score (Test): 0.7407516801185982, RMSE (Test): 0.7986546754837036\n",
      "Epoch [1646/5000], Train Loss: 0.014420625443259874, R2 Score (Train): 0.9913219176157306, RMSE (Train): 0.15647277151304148\n",
      "Test Loss: 0.6188419461250305, R2 Score (Test): 0.7546826776245292, RMSE (Test): 0.7768999934196472\n",
      "Epoch [1651/5000], Train Loss: 0.01870588508124153, R2 Score (Train): 0.9915829521495927, RMSE (Train): 0.15410147255127984\n",
      "Test Loss: 0.6073303818702698, R2 Score (Test): 0.7383889786543645, RMSE (Test): 0.8022856712341309\n",
      "Epoch [1656/5000], Train Loss: 0.011179099674336612, R2 Score (Train): 0.9939257869972938, RMSE (Train): 0.13090972159264158\n",
      "Test Loss: 0.5929126441478729, R2 Score (Test): 0.7520704949524126, RMSE (Test): 0.7810254096984863\n",
      "Epoch [1661/5000], Train Loss: 0.014383874367922544, R2 Score (Train): 0.9890732547895204, RMSE (Train): 0.1755788898903801\n",
      "Test Loss: 0.5951899588108063, R2 Score (Test): 0.7543713810182393, RMSE (Test): 0.7773928046226501\n",
      "Epoch [1666/5000], Train Loss: 0.018481240530187886, R2 Score (Train): 0.9930880813483933, RMSE (Train): 0.13964526025499388\n",
      "Test Loss: 0.5955704152584076, R2 Score (Test): 0.7484874416166463, RMSE (Test): 0.7866488099098206\n",
      "Epoch [1671/5000], Train Loss: 0.010327159776352346, R2 Score (Train): 0.9869591916799025, RMSE (Train): 0.19181348961725567\n",
      "Test Loss: 0.6302128434181213, R2 Score (Test): 0.7459024591083661, RMSE (Test): 0.7906809449195862\n",
      "Epoch [1676/5000], Train Loss: 0.01584161794744432, R2 Score (Train): 0.9913376733437796, RMSE (Train): 0.1563306627874246\n",
      "Test Loss: 0.6630535423755646, R2 Score (Test): 0.7401282549125984, RMSE (Test): 0.7996143698692322\n",
      "Epoch [1681/5000], Train Loss: 0.014462253622089824, R2 Score (Train): 0.9921746031815615, RMSE (Train): 0.14858674719498047\n",
      "Test Loss: 0.6230067610740662, R2 Score (Test): 0.7415898360098889, RMSE (Test): 0.7973625659942627\n",
      "Epoch [1686/5000], Train Loss: 0.01746051339432597, R2 Score (Train): 0.9902578795748612, RMSE (Train): 0.16578820808939493\n",
      "Test Loss: 0.6189617216587067, R2 Score (Test): 0.7424763683326189, RMSE (Test): 0.7959936261177063\n",
      "Epoch [1691/5000], Train Loss: 0.010101222316734493, R2 Score (Train): 0.991428475836985, RMSE (Train): 0.15550913925438367\n",
      "Test Loss: 0.6549051105976105, R2 Score (Test): 0.7507254344624614, RMSE (Test): 0.7831410765647888\n",
      "Epoch [1696/5000], Train Loss: 0.014040572025502721, R2 Score (Train): 0.9903905192025246, RMSE (Train): 0.16465573136606085\n",
      "Test Loss: 0.6520530581474304, R2 Score (Test): 0.7511267919709228, RMSE (Test): 0.7825103998184204\n",
      "Epoch [1701/5000], Train Loss: 0.01346557323510448, R2 Score (Train): 0.9927211122304466, RMSE (Train): 0.14330435946505266\n",
      "Test Loss: 0.677272230386734, R2 Score (Test): 0.7482964696144998, RMSE (Test): 0.7869473695755005\n",
      "Epoch [1706/5000], Train Loss: 0.013912599766626954, R2 Score (Train): 0.9911470856103464, RMSE (Train): 0.15804109239969094\n",
      "Test Loss: 0.5939716100692749, R2 Score (Test): 0.7619885112031086, RMSE (Test): 0.7652441263198853\n",
      "Epoch [1711/5000], Train Loss: 0.011471055215224624, R2 Score (Train): 0.989555770722622, RMSE (Train): 0.1716584114697921\n",
      "Test Loss: 0.5885410606861115, R2 Score (Test): 0.7529875013557048, RMSE (Test): 0.7795796990394592\n",
      "Epoch [1716/5000], Train Loss: 0.010112459034038087, R2 Score (Train): 0.9921819909730957, RMSE (Train): 0.14851659183121518\n",
      "Test Loss: 0.6045482158660889, R2 Score (Test): 0.7495008181806715, RMSE (Test): 0.7850624322891235\n",
      "Epoch [1721/5000], Train Loss: 0.012542785378172994, R2 Score (Train): 0.9921729627440893, RMSE (Train): 0.1486023204943792\n",
      "Test Loss: 0.5950950086116791, R2 Score (Test): 0.7526615956463241, RMSE (Test): 0.780093789100647\n",
      "Epoch [1726/5000], Train Loss: 0.011764898430556059, R2 Score (Train): 0.9923564951860987, RMSE (Train): 0.14684973339267218\n",
      "Test Loss: 0.6305948495864868, R2 Score (Test): 0.7490845208128789, RMSE (Test): 0.7857145071029663\n",
      "Epoch [1731/5000], Train Loss: 0.012742785038426518, R2 Score (Train): 0.9899771050581079, RMSE (Train): 0.1681603024390483\n",
      "Test Loss: 0.6486157476902008, R2 Score (Test): 0.7488146411039289, RMSE (Test): 0.7861369848251343\n",
      "Epoch [1736/5000], Train Loss: 0.01288993536339452, R2 Score (Train): 0.992818175944152, RMSE (Train): 0.14234567341802523\n",
      "Test Loss: 0.5525002628564835, R2 Score (Test): 0.7631470267347267, RMSE (Test): 0.7633794546127319\n",
      "Epoch [1741/5000], Train Loss: 0.016604696012412507, R2 Score (Train): 0.9908657348801017, RMSE (Train): 0.16053276908407857\n",
      "Test Loss: 0.5701766908168793, R2 Score (Test): 0.7549169687294391, RMSE (Test): 0.7765289545059204\n",
      "Epoch [1746/5000], Train Loss: 0.01136407240604361, R2 Score (Train): 0.990117399022909, RMSE (Train): 0.16697925572011077\n",
      "Test Loss: 0.5361593216657639, R2 Score (Test): 0.7595762168313045, RMSE (Test): 0.7691122889518738\n",
      "Epoch [1751/5000], Train Loss: 0.011947256435329715, R2 Score (Train): 0.9928902608100915, RMSE (Train): 0.14162950114795803\n",
      "Test Loss: 0.5648694932460785, R2 Score (Test): 0.7614768504101036, RMSE (Test): 0.7660662531852722\n",
      "Epoch [1756/5000], Train Loss: 0.01143450515034298, R2 Score (Train): 0.9932190010599695, RMSE (Train): 0.13831641656564236\n",
      "Test Loss: 0.6299337446689606, R2 Score (Test): 0.7542527657740309, RMSE (Test): 0.7775804400444031\n",
      "Epoch [1761/5000], Train Loss: 0.01096934596231828, R2 Score (Train): 0.9917182738487565, RMSE (Train): 0.15285770103506324\n",
      "Test Loss: 0.6560781002044678, R2 Score (Test): 0.7552341949352346, RMSE (Test): 0.7760262489318848\n",
      "Epoch [1766/5000], Train Loss: 0.011627474295285841, R2 Score (Train): 0.9924365437249494, RMSE (Train): 0.14607874899605072\n",
      "Test Loss: 0.6121527552604675, R2 Score (Test): 0.7525638029955657, RMSE (Test): 0.7802479863166809\n",
      "Epoch [1771/5000], Train Loss: 0.011630826241647204, R2 Score (Train): 0.9922480788896577, RMSE (Train): 0.14788753251313871\n",
      "Test Loss: 0.5589812994003296, R2 Score (Test): 0.7544715286445327, RMSE (Test): 0.7772342562675476\n",
      "Epoch [1776/5000], Train Loss: 0.01271064835600555, R2 Score (Train): 0.9881945386930602, RMSE (Train): 0.1825023124710028\n",
      "Test Loss: 0.6139057278633118, R2 Score (Test): 0.7572781268384158, RMSE (Test): 0.7727793455123901\n",
      "Epoch [1781/5000], Train Loss: 0.013538961143543323, R2 Score (Train): 0.9900804558442541, RMSE (Train): 0.1672910658515704\n",
      "Test Loss: 0.6420716643333435, R2 Score (Test): 0.7439185856145674, RMSE (Test): 0.7937616109848022\n",
      "Epoch [1786/5000], Train Loss: 0.012050705418611566, R2 Score (Train): 0.9925393680532947, RMSE (Train): 0.14508238906373055\n",
      "Test Loss: 0.5764177441596985, R2 Score (Test): 0.7516180286427356, RMSE (Test): 0.781737744808197\n",
      "Epoch [1791/5000], Train Loss: 0.01454315916635096, R2 Score (Train): 0.9938887665737266, RMSE (Train): 0.13130804245632074\n",
      "Test Loss: 0.6071175932884216, R2 Score (Test): 0.7544957735276204, RMSE (Test): 0.7771959900856018\n",
      "Epoch [1796/5000], Train Loss: 0.009800555611339709, R2 Score (Train): 0.9909561353021188, RMSE (Train): 0.15973640971649528\n",
      "Test Loss: 0.5736028552055359, R2 Score (Test): 0.7610287324759207, RMSE (Test): 0.7667854428291321\n",
      "Epoch [1801/5000], Train Loss: 0.014090429060161114, R2 Score (Train): 0.9915251502057696, RMSE (Train): 0.1546296936588179\n",
      "Test Loss: 0.6356960833072662, R2 Score (Test): 0.7614372259576206, RMSE (Test): 0.7661298513412476\n",
      "Epoch [1806/5000], Train Loss: 0.013708974545200666, R2 Score (Train): 0.990399900194681, RMSE (Train): 0.16457534142003716\n",
      "Test Loss: 0.551596075296402, R2 Score (Test): 0.762644833128759, RMSE (Test): 0.7641883492469788\n",
      "Epoch [1811/5000], Train Loss: 0.010287147500397017, R2 Score (Train): 0.9946826833836727, RMSE (Train): 0.12248224869131774\n",
      "Test Loss: 0.6410583853721619, R2 Score (Test): 0.7614787961188343, RMSE (Test): 0.7660630345344543\n",
      "Epoch [1816/5000], Train Loss: 0.011248058561856547, R2 Score (Train): 0.9901707082243384, RMSE (Train): 0.16652828296752725\n",
      "Test Loss: 0.5845294892787933, R2 Score (Test): 0.7510495938755288, RMSE (Test): 0.7826316952705383\n",
      "Epoch [1821/5000], Train Loss: 0.011107089774062237, R2 Score (Train): 0.9929279484187161, RMSE (Train): 0.14125362452132287\n",
      "Test Loss: 0.5605580359697342, R2 Score (Test): 0.7447377537720242, RMSE (Test): 0.7924909591674805\n",
      "Epoch [1826/5000], Train Loss: 0.012702205994476875, R2 Score (Train): 0.9910865378125469, RMSE (Train): 0.15858061723852668\n",
      "Test Loss: 0.5989502370357513, R2 Score (Test): 0.7516927197352987, RMSE (Test): 0.7816202044487\n",
      "Epoch [1831/5000], Train Loss: 0.01347248574408392, R2 Score (Train): 0.9923557753677978, RMSE (Train): 0.14685664793250688\n",
      "Test Loss: 0.6155077517032623, R2 Score (Test): 0.743524963970162, RMSE (Test): 0.7943713665008545\n",
      "Epoch [1836/5000], Train Loss: 0.017653508034224313, R2 Score (Train): 0.9886483323230074, RMSE (Train): 0.17896031064548734\n",
      "Test Loss: 0.5873136818408966, R2 Score (Test): 0.7586641205319113, RMSE (Test): 0.7705698013305664\n",
      "Epoch [1841/5000], Train Loss: 0.015248588441560665, R2 Score (Train): 0.9914711006056983, RMSE (Train): 0.1551219967707411\n",
      "Test Loss: 0.6152440905570984, R2 Score (Test): 0.7568650413066886, RMSE (Test): 0.7734366655349731\n",
      "Epoch [1846/5000], Train Loss: 0.01250462579385688, R2 Score (Train): 0.9892410634453421, RMSE (Train): 0.17422543756647538\n",
      "Test Loss: 0.6355113089084625, R2 Score (Test): 0.7496186584885194, RMSE (Test): 0.7848777770996094\n",
      "Epoch [1851/5000], Train Loss: 0.009871128907737633, R2 Score (Train): 0.991820799426058, RMSE (Train): 0.15190858550125205\n",
      "Test Loss: 0.6098381578922272, R2 Score (Test): 0.7543187695560792, RMSE (Test): 0.7774761319160461\n",
      "Epoch [1856/5000], Train Loss: 0.013102496780144671, R2 Score (Train): 0.990911965850979, RMSE (Train): 0.16012600399817983\n",
      "Test Loss: 0.5694795697927475, R2 Score (Test): 0.753714542573452, RMSE (Test): 0.7784315943717957\n",
      "Epoch [1861/5000], Train Loss: 0.012105470911289254, R2 Score (Train): 0.9941275133695258, RMSE (Train): 0.12871759273682745\n",
      "Test Loss: 0.5958063900470734, R2 Score (Test): 0.7570486207352519, RMSE (Test): 0.7731446623802185\n",
      "Epoch [1866/5000], Train Loss: 0.01019149828546991, R2 Score (Train): 0.9906467930358417, RMSE (Train): 0.1624453046203497\n",
      "Test Loss: 0.6399896740913391, R2 Score (Test): 0.7475329400470065, RMSE (Test): 0.788140058517456\n",
      "Epoch [1871/5000], Train Loss: 0.011445368252073726, R2 Score (Train): 0.987816756775943, RMSE (Train): 0.18539941007063607\n",
      "Test Loss: 0.6743020415306091, R2 Score (Test): 0.7482203986763944, RMSE (Test): 0.7870663404464722\n",
      "Epoch [1876/5000], Train Loss: 0.01578588845829169, R2 Score (Train): 0.9906682739829646, RMSE (Train): 0.16225865822006474\n",
      "Test Loss: 0.6023881733417511, R2 Score (Test): 0.7582105124519293, RMSE (Test): 0.7712936401367188\n",
      "Epoch [1881/5000], Train Loss: 0.012109236015627781, R2 Score (Train): 0.9929734047126516, RMSE (Train): 0.1407989320477934\n",
      "Test Loss: 0.6116381585597992, R2 Score (Test): 0.7488307725974853, RMSE (Test): 0.7861117124557495\n",
      "Epoch [1886/5000], Train Loss: 0.012361424276605248, R2 Score (Train): 0.9937412611154819, RMSE (Train): 0.1328832701551085\n",
      "Test Loss: 0.5445661544799805, R2 Score (Test): 0.7558123910274133, RMSE (Test): 0.7751091122627258\n",
      "Epoch [1891/5000], Train Loss: 0.009971053339540958, R2 Score (Train): 0.9902943130055648, RMSE (Train): 0.165477911623933\n",
      "Test Loss: 0.5754227787256241, R2 Score (Test): 0.7500199750214329, RMSE (Test): 0.7842485308647156\n",
      "Epoch [1896/5000], Train Loss: 0.009477269176083306, R2 Score (Train): 0.9902219941386581, RMSE (Train): 0.1660932707084142\n",
      "Test Loss: 0.6041327714920044, R2 Score (Test): 0.748806496778885, RMSE (Test): 0.7861496806144714\n",
      "Epoch [1901/5000], Train Loss: 0.015489735330144564, R2 Score (Train): 0.994398050806069, RMSE (Train): 0.1257177134677379\n",
      "Test Loss: 0.5983569324016571, R2 Score (Test): 0.759272832568797, RMSE (Test): 0.7695974111557007\n",
      "Epoch [1906/5000], Train Loss: 0.015576101645516852, R2 Score (Train): 0.9896699771446108, RMSE (Train): 0.17071729936378344\n",
      "Test Loss: 0.6107092499732971, R2 Score (Test): 0.754675643126666, RMSE (Test): 0.776911199092865\n",
      "Epoch [1911/5000], Train Loss: 0.01178533494627724, R2 Score (Train): 0.9938292308182552, RMSE (Train): 0.1319460950005235\n",
      "Test Loss: 0.5908778607845306, R2 Score (Test): 0.7495220980765468, RMSE (Test): 0.7850291132926941\n",
      "Epoch [1916/5000], Train Loss: 0.016255380197738607, R2 Score (Train): 0.9955126402659984, RMSE (Train): 0.11251808549095219\n",
      "Test Loss: 0.6136341691017151, R2 Score (Test): 0.7580616118148231, RMSE (Test): 0.7715311050415039\n",
      "Epoch [1921/5000], Train Loss: 0.01349724231598278, R2 Score (Train): 0.992946582041035, RMSE (Train): 0.14106741245066406\n",
      "Test Loss: 0.5356549173593521, R2 Score (Test): 0.757165653022874, RMSE (Test): 0.7729583382606506\n",
      "Epoch [1926/5000], Train Loss: 0.011417252322038015, R2 Score (Train): 0.9909780127473379, RMSE (Train): 0.15954308857356236\n",
      "Test Loss: 0.6077474057674408, R2 Score (Test): 0.74746898019363, RMSE (Test): 0.7882398962974548\n",
      "Epoch [1931/5000], Train Loss: 0.014626273419708014, R2 Score (Train): 0.9935041510326404, RMSE (Train): 0.1353769888667032\n",
      "Test Loss: 0.6358770728111267, R2 Score (Test): 0.7486003272265734, RMSE (Test): 0.7864722013473511\n",
      "Epoch [1936/5000], Train Loss: 0.01230007018117855, R2 Score (Train): 0.9947332431204023, RMSE (Train): 0.12189854624690914\n",
      "Test Loss: 0.5773783326148987, R2 Score (Test): 0.7580781830881902, RMSE (Test): 0.7715047001838684\n",
      "Epoch [1941/5000], Train Loss: 0.011883187340572476, R2 Score (Train): 0.9908674732060102, RMSE (Train): 0.1605174930029687\n",
      "Test Loss: 0.6339068710803986, R2 Score (Test): 0.7637603901671317, RMSE (Test): 0.7623903751373291\n",
      "Epoch [1946/5000], Train Loss: 0.012474446014190713, R2 Score (Train): 0.9907074877472204, RMSE (Train): 0.1619173777725734\n",
      "Test Loss: 0.6292116343975067, R2 Score (Test): 0.7467991856527276, RMSE (Test): 0.7892844676971436\n",
      "Epoch [1951/5000], Train Loss: 0.015232574582720796, R2 Score (Train): 0.994276731024696, RMSE (Train): 0.1270717377514795\n",
      "Test Loss: 0.666857898235321, R2 Score (Test): 0.7476837033323611, RMSE (Test): 0.787904679775238\n",
      "Epoch [1956/5000], Train Loss: 0.01248493732418865, R2 Score (Train): 0.9932818651091682, RMSE (Train): 0.13767378438440522\n",
      "Test Loss: 0.6009095013141632, R2 Score (Test): 0.7561415451863782, RMSE (Test): 0.7745864987373352\n",
      "Epoch [1961/5000], Train Loss: 0.012767974209661285, R2 Score (Train): 0.9921200073560967, RMSE (Train): 0.14910417245946508\n",
      "Test Loss: 0.5767278373241425, R2 Score (Test): 0.7610364390901793, RMSE (Test): 0.7667731642723083\n",
      "Epoch [1966/5000], Train Loss: 0.011920958020103475, R2 Score (Train): 0.9916307977144085, RMSE (Train): 0.15366286391931455\n",
      "Test Loss: 0.6399618685245514, R2 Score (Test): 0.7498797213661013, RMSE (Test): 0.7844684720039368\n",
      "Epoch [1971/5000], Train Loss: 0.01685044588521123, R2 Score (Train): 0.9897792915237907, RMSE (Train): 0.1698116145603104\n",
      "Test Loss: 0.547063872218132, R2 Score (Test): 0.753115695217787, RMSE (Test): 0.7793774008750916\n",
      "Epoch [1976/5000], Train Loss: 0.015862335916608572, R2 Score (Train): 0.9943387108299939, RMSE (Train): 0.12638180681404504\n",
      "Test Loss: 0.6408009827136993, R2 Score (Test): 0.7512443000160896, RMSE (Test): 0.7823256254196167\n",
      "Epoch [1981/5000], Train Loss: 0.011158748141800364, R2 Score (Train): 0.9902773641385784, RMSE (Train): 0.16562233415423103\n",
      "Test Loss: 0.6527580916881561, R2 Score (Test): 0.7377033462181624, RMSE (Test): 0.8033363819122314\n",
      "Epoch [1986/5000], Train Loss: 0.012940369352387885, R2 Score (Train): 0.9910665294402544, RMSE (Train): 0.1587585033030911\n",
      "Test Loss: 0.6237332820892334, R2 Score (Test): 0.742440812121393, RMSE (Test): 0.796048641204834\n",
      "Epoch [1991/5000], Train Loss: 0.014688423679520687, R2 Score (Train): 0.992644253242248, RMSE (Train): 0.1440589600065947\n",
      "Test Loss: 0.5862613469362259, R2 Score (Test): 0.7488285437798046, RMSE (Test): 0.7861152291297913\n",
      "Epoch [1996/5000], Train Loss: 0.012046322614575425, R2 Score (Train): 0.9906139950781628, RMSE (Train): 0.1627298707449118\n",
      "Test Loss: 0.5652996599674225, R2 Score (Test): 0.7567466322792674, RMSE (Test): 0.7736250162124634\n",
      "Epoch [2001/5000], Train Loss: 0.012755103292874992, R2 Score (Train): 0.9922348927940331, RMSE (Train): 0.1480132581593792\n",
      "Test Loss: 0.6141237020492554, R2 Score (Test): 0.7520586342285152, RMSE (Test): 0.781044065952301\n",
      "Epoch [2006/5000], Train Loss: 0.012595887606342634, R2 Score (Train): 0.9898377912415183, RMSE (Train): 0.16932494639320253\n",
      "Test Loss: 0.5798579007387161, R2 Score (Test): 0.7488617469590941, RMSE (Test): 0.7860631942749023\n",
      "Epoch [2011/5000], Train Loss: 0.010247783386148512, R2 Score (Train): 0.990704466456094, RMSE (Train): 0.16194369787666676\n",
      "Test Loss: 0.6176346242427826, R2 Score (Test): 0.7500074089654893, RMSE (Test): 0.7842682600021362\n",
      "Epoch [2016/5000], Train Loss: 0.011633503367193043, R2 Score (Train): 0.9918305838376148, RMSE (Train): 0.1518176975910259\n",
      "Test Loss: 0.5897234976291656, R2 Score (Test): 0.7662462885968461, RMSE (Test): 0.7583684921264648\n",
      "Epoch [2021/5000], Train Loss: 0.008604629896581173, R2 Score (Train): 0.9952004411586763, RMSE (Train): 0.11636638714001107\n",
      "Test Loss: 0.6126834452152252, R2 Score (Test): 0.7599729287435839, RMSE (Test): 0.7684774994850159\n",
      "Epoch [2026/5000], Train Loss: 0.010923807509243488, R2 Score (Train): 0.9919726258042829, RMSE (Train): 0.15049207998223893\n",
      "Test Loss: 0.5544223785400391, R2 Score (Test): 0.7565505126891127, RMSE (Test): 0.7739368081092834\n",
      "Epoch [2031/5000], Train Loss: 0.009914520041396221, R2 Score (Train): 0.9918772140258788, RMSE (Train): 0.15138379762955925\n",
      "Test Loss: 0.6331118643283844, R2 Score (Test): 0.7538702822690633, RMSE (Test): 0.7781853675842285\n",
      "Epoch [2036/5000], Train Loss: 0.013979053124785423, R2 Score (Train): 0.9908184991111187, RMSE (Train): 0.1609473131479051\n",
      "Test Loss: 0.552644282579422, R2 Score (Test): 0.7644930126626808, RMSE (Test): 0.7612072825431824\n",
      "Epoch [2041/5000], Train Loss: 0.020336989235753816, R2 Score (Train): 0.991562475656601, RMSE (Train): 0.15428880313363552\n",
      "Test Loss: 0.6027444005012512, R2 Score (Test): 0.7582472168152868, RMSE (Test): 0.7712351083755493\n",
      "Epoch [2046/5000], Train Loss: 0.011398941705313822, R2 Score (Train): 0.9891119078880152, RMSE (Train): 0.17526806170907414\n",
      "Test Loss: 0.6018441915512085, R2 Score (Test): 0.7558299834317614, RMSE (Test): 0.775081217288971\n",
      "Epoch [2051/5000], Train Loss: 0.012685057086249193, R2 Score (Train): 0.9914821841851608, RMSE (Train): 0.15502117098801033\n",
      "Test Loss: 0.5348382741212845, R2 Score (Test): 0.7545040582402399, RMSE (Test): 0.7771828770637512\n",
      "Epoch [2056/5000], Train Loss: 0.010965316634004315, R2 Score (Train): 0.9888176355392907, RMSE (Train): 0.17762075514753023\n",
      "Test Loss: 0.5454791933298111, R2 Score (Test): 0.7549754338300259, RMSE (Test): 0.7764363288879395\n",
      "Epoch [2061/5000], Train Loss: 0.010741016439472636, R2 Score (Train): 0.9916027214386391, RMSE (Train): 0.1539203955548382\n",
      "Test Loss: 0.5633070915937424, R2 Score (Test): 0.7537708281772251, RMSE (Test): 0.7783426642417908\n",
      "Epoch [2066/5000], Train Loss: 0.008813870372250676, R2 Score (Train): 0.9956597492799181, RMSE (Train): 0.11065837785337558\n",
      "Test Loss: 0.6437578201293945, R2 Score (Test): 0.7572298735851769, RMSE (Test): 0.7728561162948608\n",
      "Epoch [2071/5000], Train Loss: 0.011258652006896833, R2 Score (Train): 0.989114305353639, RMSE (Train): 0.1752487643757031\n",
      "Test Loss: 0.6218314468860626, R2 Score (Test): 0.7467437925930862, RMSE (Test): 0.7893708944320679\n",
      "Epoch [2076/5000], Train Loss: 0.011930274233842889, R2 Score (Train): 0.9945736306004049, RMSE (Train): 0.12373186759582393\n",
      "Test Loss: 0.5844982266426086, R2 Score (Test): 0.7540434544445652, RMSE (Test): 0.7779116034507751\n",
      "Epoch [2081/5000], Train Loss: 0.008834740030579269, R2 Score (Train): 0.991367391997316, RMSE (Train): 0.15606226327701295\n",
      "Test Loss: 0.5627176910638809, R2 Score (Test): 0.7512150705754342, RMSE (Test): 0.7823715806007385\n",
      "Epoch [2086/5000], Train Loss: 0.013461908170332512, R2 Score (Train): 0.9921844739050799, RMSE (Train): 0.14849300616650826\n",
      "Test Loss: 0.5889738500118256, R2 Score (Test): 0.7518731195059072, RMSE (Test): 0.7813361883163452\n",
      "Epoch [2091/5000], Train Loss: 0.012454710939588646, R2 Score (Train): 0.9938702847427577, RMSE (Train): 0.1315064460215175\n",
      "Test Loss: 0.645872563123703, R2 Score (Test): 0.7457955687356737, RMSE (Test): 0.7908472418785095\n",
      "Epoch [2096/5000], Train Loss: 0.014924196138357123, R2 Score (Train): 0.992600867076967, RMSE (Train): 0.14448318455147507\n",
      "Test Loss: 0.5880985260009766, R2 Score (Test): 0.7575043284923706, RMSE (Test): 0.7724191546440125\n",
      "Epoch [2101/5000], Train Loss: 0.011988872157720229, R2 Score (Train): 0.9940502745097477, RMSE (Train): 0.1295613172952451\n",
      "Test Loss: 0.6255503594875336, R2 Score (Test): 0.7538197872266713, RMSE (Test): 0.7782651782035828\n",
      "Epoch [2106/5000], Train Loss: 0.008757555391639471, R2 Score (Train): 0.9910740248510983, RMSE (Train): 0.15869188809986157\n",
      "Test Loss: 0.6296636760234833, R2 Score (Test): 0.7586616222215673, RMSE (Test): 0.7705737948417664\n",
      "Epoch [2111/5000], Train Loss: 0.01337667228654027, R2 Score (Train): 0.9916137199229448, RMSE (Train): 0.1538195625451153\n",
      "Test Loss: 0.6139160096645355, R2 Score (Test): 0.7528695271315036, RMSE (Test): 0.7797658443450928\n",
      "Epoch [2116/5000], Train Loss: 0.009791982166158656, R2 Score (Train): 0.9948279871467626, RMSE (Train): 0.12079715012432848\n",
      "Test Loss: 0.5961506962776184, R2 Score (Test): 0.7585932539028953, RMSE (Test): 0.7706829309463501\n",
      "Epoch [2121/5000], Train Loss: 0.010034335350307325, R2 Score (Train): 0.9892813298701998, RMSE (Train): 0.17389910365032396\n",
      "Test Loss: 0.6070953011512756, R2 Score (Test): 0.7616445711109018, RMSE (Test): 0.7657968401908875\n",
      "Epoch [2126/5000], Train Loss: 0.01061496235585461, R2 Score (Train): 0.9924202382204904, RMSE (Train): 0.1462361244903944\n",
      "Test Loss: 0.5885571837425232, R2 Score (Test): 0.7554037385052873, RMSE (Test): 0.7757574915885925\n",
      "Epoch [2131/5000], Train Loss: 0.015035809056522945, R2 Score (Train): 0.9924279144296875, RMSE (Train): 0.14616205729236795\n",
      "Test Loss: 0.6317627429962158, R2 Score (Test): 0.7541577472652908, RMSE (Test): 0.7777308225631714\n",
      "Epoch [2136/5000], Train Loss: 0.01168452420582374, R2 Score (Train): 0.9911673099328303, RMSE (Train): 0.15786046817679333\n",
      "Test Loss: 0.5941725969314575, R2 Score (Test): 0.7556612615024008, RMSE (Test): 0.775348961353302\n",
      "Epoch [2141/5000], Train Loss: 0.015199384031196436, R2 Score (Train): 0.9912733201939734, RMSE (Train): 0.15691028505355653\n",
      "Test Loss: 0.5921992063522339, R2 Score (Test): 0.7556599996400355, RMSE (Test): 0.7753509879112244\n",
      "Epoch [2146/5000], Train Loss: 0.011146801640279591, R2 Score (Train): 0.9948848882860166, RMSE (Train): 0.1201308229637577\n",
      "Test Loss: 0.544190376996994, R2 Score (Test): 0.7627782745876621, RMSE (Test): 0.7639734745025635\n",
      "Epoch [2151/5000], Train Loss: 0.011519914027303457, R2 Score (Train): 0.9911296496738676, RMSE (Train): 0.15819664789622506\n",
      "Test Loss: 0.6125104427337646, R2 Score (Test): 0.7497270299400491, RMSE (Test): 0.7847079038619995\n",
      "Epoch [2156/5000], Train Loss: 0.010386234925438961, R2 Score (Train): 0.9921150143688802, RMSE (Train): 0.14915140329887355\n",
      "Test Loss: 0.6327595412731171, R2 Score (Test): 0.7529514695747004, RMSE (Test): 0.779636561870575\n",
      "Epoch [2161/5000], Train Loss: 0.00984087714459747, R2 Score (Train): 0.9940491873745548, RMSE (Train): 0.12957315349125295\n",
      "Test Loss: 0.6023415923118591, R2 Score (Test): 0.7470284078140386, RMSE (Test): 0.7889271974563599\n",
      "Epoch [2166/5000], Train Loss: 0.01277511469864597, R2 Score (Train): 0.9926991864360986, RMSE (Train): 0.1435200311037748\n",
      "Test Loss: 0.6208401322364807, R2 Score (Test): 0.7484635131879025, RMSE (Test): 0.7866862416267395\n",
      "Epoch [2171/5000], Train Loss: 0.011650227863962451, R2 Score (Train): 0.9877670829348886, RMSE (Train): 0.18577698328499695\n",
      "Test Loss: 0.5786516964435577, R2 Score (Test): 0.7598604508058525, RMSE (Test): 0.7686575651168823\n",
      "Epoch [2176/5000], Train Loss: 0.011263430817052722, R2 Score (Train): 0.989314691295599, RMSE (Train): 0.17362826579486249\n",
      "Test Loss: 0.5735666751861572, R2 Score (Test): 0.7449962816651146, RMSE (Test): 0.792089581489563\n",
      "Epoch [2181/5000], Train Loss: 0.012281390139833093, R2 Score (Train): 0.9909285379741362, RMSE (Train): 0.15997994167881546\n",
      "Test Loss: 0.5917035043239594, R2 Score (Test): 0.7552734677177546, RMSE (Test): 0.7759640216827393\n",
      "Epoch [2186/5000], Train Loss: 0.011803362052887678, R2 Score (Train): 0.9915881929949503, RMSE (Train): 0.1540534897100105\n",
      "Test Loss: 0.5923663377761841, R2 Score (Test): 0.7645669411790263, RMSE (Test): 0.7610878348350525\n",
      "Epoch [2191/5000], Train Loss: 0.010328304837457836, R2 Score (Train): 0.9926450960294003, RMSE (Train): 0.14405070696835545\n",
      "Test Loss: 0.5756195783615112, R2 Score (Test): 0.7600285439518277, RMSE (Test): 0.7683885097503662\n",
      "Epoch [2196/5000], Train Loss: 0.011144463205710053, R2 Score (Train): 0.9885683737873039, RMSE (Train): 0.17958948228406804\n",
      "Test Loss: 0.6305036246776581, R2 Score (Test): 0.7533654315406151, RMSE (Test): 0.7789830565452576\n",
      "Epoch [2201/5000], Train Loss: 0.007789111540963252, R2 Score (Train): 0.9960258623150858, RMSE (Train): 0.10588839006508871\n",
      "Test Loss: 0.5998903214931488, R2 Score (Test): 0.7588576325067129, RMSE (Test): 0.7702608704566956\n",
      "Epoch [2206/5000], Train Loss: 0.011239575454965234, R2 Score (Train): 0.9939720330805458, RMSE (Train): 0.130410428176202\n",
      "Test Loss: 0.5642949342727661, R2 Score (Test): 0.7561808939557424, RMSE (Test): 0.7745240926742554\n",
      "Epoch [2211/5000], Train Loss: 0.009431254196291169, R2 Score (Train): 0.9945586218827815, RMSE (Train): 0.12390286354138613\n",
      "Test Loss: 0.6808555722236633, R2 Score (Test): 0.7547441726100961, RMSE (Test): 0.776802659034729\n",
      "Epoch [2216/5000], Train Loss: 0.010746378684416413, R2 Score (Train): 0.9917137919376502, RMSE (Train): 0.15289905726627148\n",
      "Test Loss: 0.5622268617153168, R2 Score (Test): 0.7486537625629018, RMSE (Test): 0.786388635635376\n",
      "Epoch [2221/5000], Train Loss: 0.010645001738642653, R2 Score (Train): 0.9921391668691933, RMSE (Train): 0.1489227952627449\n",
      "Test Loss: 0.5822706669569016, R2 Score (Test): 0.7506271313705761, RMSE (Test): 0.7832955718040466\n",
      "Epoch [2226/5000], Train Loss: 0.010033272226185849, R2 Score (Train): 0.990621181680955, RMSE (Train): 0.16266755994827584\n",
      "Test Loss: 0.6297166645526886, R2 Score (Test): 0.75155565485016, RMSE (Test): 0.7818359136581421\n",
      "Epoch [2231/5000], Train Loss: 0.009964186038511494, R2 Score (Train): 0.9941057659166844, RMSE (Train): 0.12895571102584796\n",
      "Test Loss: 0.6245186626911163, R2 Score (Test): 0.7516467386225482, RMSE (Test): 0.7816925048828125\n",
      "Epoch [2236/5000], Train Loss: 0.012735812808386981, R2 Score (Train): 0.9947139963995059, RMSE (Train): 0.12212107481914915\n",
      "Test Loss: 0.5561636686325073, R2 Score (Test): 0.7521880275780485, RMSE (Test): 0.7808402180671692\n",
      "Epoch [2241/5000], Train Loss: 0.010995923541486263, R2 Score (Train): 0.9937704435660597, RMSE (Train): 0.1325731125976504\n",
      "Test Loss: 0.630010724067688, R2 Score (Test): 0.7591229727031774, RMSE (Test): 0.769836962223053\n",
      "Epoch [2246/5000], Train Loss: 0.012068885145708919, R2 Score (Train): 0.9931710119813004, RMSE (Train): 0.13880498581098805\n",
      "Test Loss: 0.592264711856842, R2 Score (Test): 0.7545554645129188, RMSE (Test): 0.7771015167236328\n",
      "Epoch [2251/5000], Train Loss: 0.012437347633143267, R2 Score (Train): 0.9942121995623998, RMSE (Train): 0.12778611443137758\n",
      "Test Loss: 0.6159304082393646, R2 Score (Test): 0.7512016045023591, RMSE (Test): 0.7823927998542786\n",
      "Epoch [2256/5000], Train Loss: 0.011732400860637426, R2 Score (Train): 0.9883970154471012, RMSE (Train): 0.18093048514711138\n",
      "Test Loss: 0.6580232083797455, R2 Score (Test): 0.7474671103174677, RMSE (Test): 0.7882428169250488\n",
      "Epoch [2261/5000], Train Loss: 0.008857979982470473, R2 Score (Train): 0.9923566537192359, RMSE (Train): 0.14684821048726338\n",
      "Test Loss: 0.61912801861763, R2 Score (Test): 0.7565924161889571, RMSE (Test): 0.7738701105117798\n",
      "Epoch [2266/5000], Train Loss: 0.010030057126035294, R2 Score (Train): 0.993710573714287, RMSE (Train): 0.1332086436823624\n",
      "Test Loss: 0.5783110111951828, R2 Score (Test): 0.7486969414998337, RMSE (Test): 0.7863211035728455\n",
      "Epoch [2271/5000], Train Loss: 0.011067496457447609, R2 Score (Train): 0.9926682624031368, RMSE (Train): 0.14382366359103796\n",
      "Test Loss: 0.6546908617019653, R2 Score (Test): 0.7575597135654432, RMSE (Test): 0.7723309397697449\n",
      "Epoch [2276/5000], Train Loss: 0.010755008785054088, R2 Score (Train): 0.9933485539897158, RMSE (Train): 0.1369887572506558\n",
      "Test Loss: 0.6109773814678192, R2 Score (Test): 0.7590922911597915, RMSE (Test): 0.7698860168457031\n",
      "Epoch [2281/5000], Train Loss: 0.01519340742379427, R2 Score (Train): 0.9889595059399576, RMSE (Train): 0.1764904233631608\n",
      "Test Loss: 0.5714926421642303, R2 Score (Test): 0.7582821838911215, RMSE (Test): 0.7711793780326843\n",
      "Epoch [2286/5000], Train Loss: 0.012553771957755089, R2 Score (Train): 0.9923645500593912, RMSE (Train): 0.14677233645123244\n",
      "Test Loss: 0.6089848577976227, R2 Score (Test): 0.7549203995368599, RMSE (Test): 0.7765235304832458\n",
      "Epoch [2291/5000], Train Loss: 0.013456009794026613, R2 Score (Train): 0.9921714868176965, RMSE (Train): 0.1486163306326465\n",
      "Test Loss: 0.6481115818023682, R2 Score (Test): 0.7513966390384996, RMSE (Test): 0.7820860743522644\n",
      "Epoch [2296/5000], Train Loss: 0.00999749517844369, R2 Score (Train): 0.992142226753633, RMSE (Train): 0.1488938078202073\n",
      "Test Loss: 0.6219281256198883, R2 Score (Test): 0.753579051177312, RMSE (Test): 0.7786455750465393\n",
      "Epoch [2301/5000], Train Loss: 0.01444456388708204, R2 Score (Train): 0.9914723746574483, RMSE (Train): 0.15511041023281213\n",
      "Test Loss: 0.6025726199150085, R2 Score (Test): 0.7581542939298379, RMSE (Test): 0.7713832855224609\n",
      "Epoch [2306/5000], Train Loss: 0.012153285594346622, R2 Score (Train): 0.9926023524120112, RMSE (Train): 0.14446868172318023\n",
      "Test Loss: 0.5971525311470032, R2 Score (Test): 0.7431628066071685, RMSE (Test): 0.7949320077896118\n",
      "Epoch [2311/5000], Train Loss: 0.010955400570916632, R2 Score (Train): 0.9916714513246103, RMSE (Train): 0.15328919898571036\n",
      "Test Loss: 0.6177833378314972, R2 Score (Test): 0.7506460484022461, RMSE (Test): 0.7832657694816589\n",
      "Epoch [2316/5000], Train Loss: 0.008631769490117827, R2 Score (Train): 0.9949543502963547, RMSE (Train): 0.11931236071729641\n",
      "Test Loss: 0.6330616474151611, R2 Score (Test): 0.7528490586848381, RMSE (Test): 0.7797980904579163\n",
      "Epoch [2321/5000], Train Loss: 0.010122408935179314, R2 Score (Train): 0.9929706396311713, RMSE (Train): 0.14082663267733367\n",
      "Test Loss: 0.5321283638477325, R2 Score (Test): 0.7642201243641925, RMSE (Test): 0.7616481781005859\n",
      "Epoch [2326/5000], Train Loss: 0.01153092443322142, R2 Score (Train): 0.9901458016453282, RMSE (Train): 0.16673913364774356\n",
      "Test Loss: 0.6462186872959137, R2 Score (Test): 0.758248362713994, RMSE (Test): 0.7712332606315613\n",
      "Epoch [2331/5000], Train Loss: 0.01069272767441968, R2 Score (Train): 0.9914852944599876, RMSE (Train): 0.154992865464668\n",
      "Test Loss: 0.585389107465744, R2 Score (Test): 0.7627631211621725, RMSE (Test): 0.7639978528022766\n",
      "Epoch [2336/5000], Train Loss: 0.010777411322730282, R2 Score (Train): 0.9919253179218301, RMSE (Train): 0.15093487752323775\n",
      "Test Loss: 0.6241604089736938, R2 Score (Test): 0.7560778481938017, RMSE (Test): 0.7746877074241638\n",
      "Epoch [2341/5000], Train Loss: 0.007586359512060881, R2 Score (Train): 0.9933612953137313, RMSE (Train): 0.13685748844008416\n",
      "Test Loss: 0.5722032785415649, R2 Score (Test): 0.7538212242513157, RMSE (Test): 0.7782629728317261\n",
      "Epoch [2346/5000], Train Loss: 0.008962068861971298, R2 Score (Train): 0.9950880405213295, RMSE (Train): 0.11772109101236596\n",
      "Test Loss: 0.5951986908912659, R2 Score (Test): 0.7608150376352876, RMSE (Test): 0.7671282291412354\n",
      "Epoch [2351/5000], Train Loss: 0.010583541200806698, R2 Score (Train): 0.9916993289161841, RMSE (Train): 0.15303243662490057\n",
      "Test Loss: 0.5649243593215942, R2 Score (Test): 0.7568667165803777, RMSE (Test): 0.7734339833259583\n",
      "Epoch [2356/5000], Train Loss: 0.00894699893736591, R2 Score (Train): 0.994081110010755, RMSE (Train): 0.12922514397886314\n",
      "Test Loss: 0.6355726718902588, R2 Score (Test): 0.7509859423150552, RMSE (Test): 0.782731831073761\n",
      "Epoch [2361/5000], Train Loss: 0.013216314720921218, R2 Score (Train): 0.9934262308714226, RMSE (Train): 0.13618651750322244\n",
      "Test Loss: 0.5420100837945938, R2 Score (Test): 0.7589851903126699, RMSE (Test): 0.7700570821762085\n",
      "Epoch [2366/5000], Train Loss: 0.01438266372618576, R2 Score (Train): 0.9889651201093936, RMSE (Train): 0.17644554434187615\n",
      "Test Loss: 0.5992865562438965, R2 Score (Test): 0.7670764133650629, RMSE (Test): 0.7570206522941589\n",
      "Epoch [2371/5000], Train Loss: 0.01103993613893787, R2 Score (Train): 0.9905043231801294, RMSE (Train): 0.16367782798400551\n",
      "Test Loss: 0.6322067975997925, R2 Score (Test): 0.7509920050742486, RMSE (Test): 0.7827222943305969\n",
      "Epoch [2376/5000], Train Loss: 0.01111250677301238, R2 Score (Train): 0.99623061215778, RMSE (Train): 0.10312460626460764\n",
      "Test Loss: 0.5853283405303955, R2 Score (Test): 0.7505534498328263, RMSE (Test): 0.7834112644195557\n",
      "Epoch [2381/5000], Train Loss: 0.009813796263188124, R2 Score (Train): 0.9884592711962625, RMSE (Train): 0.18044444154769657\n",
      "Test Loss: 0.5861032903194427, R2 Score (Test): 0.7564040379229956, RMSE (Test): 0.7741695642471313\n",
      "Epoch [2386/5000], Train Loss: 0.010896536327588061, R2 Score (Train): 0.992699272040728, RMSE (Train): 0.14351918968966354\n",
      "Test Loss: 0.635603666305542, R2 Score (Test): 0.7607289529239896, RMSE (Test): 0.7672662734985352\n",
      "Epoch [2391/5000], Train Loss: 0.011365440130854646, R2 Score (Train): 0.991996544886534, RMSE (Train): 0.15026770288205063\n",
      "Test Loss: 0.6610486209392548, R2 Score (Test): 0.746196255063343, RMSE (Test): 0.7902237176895142\n",
      "Epoch [2396/5000], Train Loss: 0.010953264737812182, R2 Score (Train): 0.9908662609958516, RMSE (Train): 0.16052814583123415\n",
      "Test Loss: 0.6194536983966827, R2 Score (Test): 0.7567918495903645, RMSE (Test): 0.7735531330108643\n",
      "Epoch [2401/5000], Train Loss: 0.008575424745989343, R2 Score (Train): 0.9914678482475179, RMSE (Train): 0.15515157058701562\n",
      "Test Loss: 0.511906698346138, R2 Score (Test): 0.7637131066272064, RMSE (Test): 0.7624666094779968\n",
      "Epoch [2406/5000], Train Loss: 0.0101964488470306, R2 Score (Train): 0.9934311303761598, RMSE (Train): 0.13613575736489444\n",
      "Test Loss: 0.6223986148834229, R2 Score (Test): 0.7498527887256737, RMSE (Test): 0.7845107316970825\n",
      "Epoch [2411/5000], Train Loss: 0.010687653712617854, R2 Score (Train): 0.9911873378818955, RMSE (Train): 0.15768139387127544\n",
      "Test Loss: 0.5533771216869354, R2 Score (Test): 0.7566338252551943, RMSE (Test): 0.7738043665885925\n",
      "Epoch [2416/5000], Train Loss: 0.00921843732551982, R2 Score (Train): 0.9926766850945679, RMSE (Train): 0.1437410276243569\n",
      "Test Loss: 0.5828643441200256, R2 Score (Test): 0.7575962462165893, RMSE (Test): 0.7722727656364441\n",
      "Epoch [2421/5000], Train Loss: 0.011435766238719225, R2 Score (Train): 0.9897719783882372, RMSE (Train): 0.1698723556182646\n",
      "Test Loss: 0.5596723407506943, R2 Score (Test): 0.7634662381116302, RMSE (Test): 0.762864887714386\n",
      "Epoch [2426/5000], Train Loss: 0.008560110271597901, R2 Score (Train): 0.9931586241170884, RMSE (Train): 0.13893082570270204\n",
      "Test Loss: 0.608291745185852, R2 Score (Test): 0.7592023994845845, RMSE (Test): 0.7697099447250366\n",
      "Epoch [2431/5000], Train Loss: 0.012025456371096274, R2 Score (Train): 0.991113600946292, RMSE (Train): 0.15833969227951886\n",
      "Test Loss: 0.5487086921930313, R2 Score (Test): 0.7507651132033248, RMSE (Test): 0.7830787897109985\n",
      "Epoch [2436/5000], Train Loss: 0.010709043320578834, R2 Score (Train): 0.994664308633345, RMSE (Train): 0.12269369366935472\n",
      "Test Loss: 0.6145370006561279, R2 Score (Test): 0.7559420536024447, RMSE (Test): 0.7749033570289612\n",
      "Epoch [2441/5000], Train Loss: 0.009340710879769176, R2 Score (Train): 0.9931694241027843, RMSE (Train): 0.13882112236325528\n",
      "Test Loss: 0.6316884458065033, R2 Score (Test): 0.7547767496469918, RMSE (Test): 0.7767510414123535\n",
      "Epoch [2446/5000], Train Loss: 0.007497318903915584, R2 Score (Train): 0.9937972129776256, RMSE (Train): 0.13228796187954886\n",
      "Test Loss: 0.5518834888935089, R2 Score (Test): 0.7543425510840749, RMSE (Test): 0.7774384617805481\n",
      "Epoch [2451/5000], Train Loss: 0.008173237826364735, R2 Score (Train): 0.9922809588152207, RMSE (Train): 0.14757356529871143\n",
      "Test Loss: 0.5790479183197021, R2 Score (Test): 0.7664686550497484, RMSE (Test): 0.7580077052116394\n",
      "Epoch [2456/5000], Train Loss: 0.011536756918455163, R2 Score (Train): 0.9940245156671632, RMSE (Train): 0.12984147687458014\n",
      "Test Loss: 0.6086298227310181, R2 Score (Test): 0.7570767335452997, RMSE (Test): 0.7730998396873474\n",
      "Epoch [2461/5000], Train Loss: 0.008256808194952706, R2 Score (Train): 0.9949570772470157, RMSE (Train): 0.11928011483099314\n",
      "Test Loss: 0.6232708394527435, R2 Score (Test): 0.7555349168128742, RMSE (Test): 0.7755494117736816\n",
      "Epoch [2466/5000], Train Loss: 0.010034858481958508, R2 Score (Train): 0.9928321283045813, RMSE (Train): 0.14220733643994815\n",
      "Test Loss: 0.5324152410030365, R2 Score (Test): 0.755081708579731, RMSE (Test): 0.7762680053710938\n",
      "Epoch [2471/5000], Train Loss: 0.007657215232029557, R2 Score (Train): 0.9949709259487266, RMSE (Train): 0.11911622075095045\n",
      "Test Loss: 0.6253520846366882, R2 Score (Test): 0.7616820198811644, RMSE (Test): 0.7657366394996643\n",
      "Epoch [2476/5000], Train Loss: 0.009704366015891234, R2 Score (Train): 0.9924193074859641, RMSE (Train): 0.1462451025319253\n",
      "Test Loss: 0.6709301471710205, R2 Score (Test): 0.7552165928204582, RMSE (Test): 0.7760541439056396\n",
      "Epoch [2481/5000], Train Loss: 0.009170759469270706, R2 Score (Train): 0.9932765957870675, RMSE (Train): 0.13772776553634408\n",
      "Test Loss: 0.6114264130592346, R2 Score (Test): 0.7559615013676461, RMSE (Test): 0.7748724818229675\n",
      "Epoch [2486/5000], Train Loss: 0.012428064364939928, R2 Score (Train): 0.9882615063389116, RMSE (Train): 0.18198394510202076\n",
      "Test Loss: 0.5696002840995789, R2 Score (Test): 0.7556661104256172, RMSE (Test): 0.7753412127494812\n",
      "Epoch [2491/5000], Train Loss: 0.011269052474138638, R2 Score (Train): 0.9948132044389328, RMSE (Train): 0.12096965886166802\n",
      "Test Loss: 0.6194574534893036, R2 Score (Test): 0.7556836758551039, RMSE (Test): 0.7753133773803711\n",
      "Epoch [2496/5000], Train Loss: 0.010243164996306101, R2 Score (Train): 0.9931031496136714, RMSE (Train): 0.13949296101644823\n",
      "Test Loss: 0.5787950456142426, R2 Score (Test): 0.7602828511263557, RMSE (Test): 0.7679811716079712\n",
      "Epoch [2501/5000], Train Loss: 0.010567999832953015, R2 Score (Train): 0.9902484391744054, RMSE (Train): 0.16586851545739398\n",
      "Test Loss: 0.5523484200239182, R2 Score (Test): 0.7581768230994012, RMSE (Test): 0.7713472843170166\n",
      "Epoch [2506/5000], Train Loss: 0.010591688643520078, R2 Score (Train): 0.9942113490335689, RMSE (Train): 0.12779550329879524\n",
      "Test Loss: 0.6611925959587097, R2 Score (Test): 0.751650462716769, RMSE (Test): 0.7816866636276245\n",
      "Epoch [2511/5000], Train Loss: 0.010680416715331376, R2 Score (Train): 0.9920381339999956, RMSE (Train): 0.14987676919546897\n",
      "Test Loss: 0.5871650278568268, R2 Score (Test): 0.7509675275234333, RMSE (Test): 0.7827607989311218\n",
      "Epoch [2516/5000], Train Loss: 0.00911830656696111, R2 Score (Train): 0.9929064696698312, RMSE (Train): 0.1414679647986783\n",
      "Test Loss: 0.6014415323734283, R2 Score (Test): 0.7473404411449017, RMSE (Test): 0.788440465927124\n",
      "Epoch [2521/5000], Train Loss: 0.008206252047481636, R2 Score (Train): 0.9917509126862928, RMSE (Train): 0.15255619245573263\n",
      "Test Loss: 0.6048952043056488, R2 Score (Test): 0.7497118353291211, RMSE (Test): 0.7847316861152649\n",
      "Epoch [2526/5000], Train Loss: 0.008452786947600543, R2 Score (Train): 0.9918135180739127, RMSE (Train): 0.15197618708616356\n",
      "Test Loss: 0.6634325385093689, R2 Score (Test): 0.7400659886383236, RMSE (Test): 0.7997101545333862\n",
      "Epoch [2531/5000], Train Loss: 0.008708517959651848, R2 Score (Train): 0.9936128002145285, RMSE (Train): 0.13424006105761124\n",
      "Test Loss: 0.6468623876571655, R2 Score (Test): 0.7528830410071394, RMSE (Test): 0.7797445058822632\n",
      "Epoch [2536/5000], Train Loss: 0.009255789414358636, R2 Score (Train): 0.9924996734474433, RMSE (Train): 0.14546783552235412\n",
      "Test Loss: 0.6614963114261627, R2 Score (Test): 0.7517462590913601, RMSE (Test): 0.7815359234809875\n",
      "Epoch [2541/5000], Train Loss: 0.008323559809165696, R2 Score (Train): 0.9945397213689104, RMSE (Train): 0.12411786401481652\n",
      "Test Loss: 0.6454119682312012, R2 Score (Test): 0.752646718843921, RMSE (Test): 0.7801172137260437\n",
      "Epoch [2546/5000], Train Loss: 0.00708131156473731, R2 Score (Train): 0.9943092430760602, RMSE (Train): 0.12671029686489726\n",
      "Test Loss: 0.5576726794242859, R2 Score (Test): 0.7565649609910725, RMSE (Test): 0.7739138007164001\n",
      "Epoch [2551/5000], Train Loss: 0.012652418770206472, R2 Score (Train): 0.9942940109474134, RMSE (Train): 0.12687976269943352\n",
      "Test Loss: 0.5701287090778351, R2 Score (Test): 0.7523532084644786, RMSE (Test): 0.7805799841880798\n",
      "Epoch [2556/5000], Train Loss: 0.008353654062375426, R2 Score (Train): 0.993897250165189, RMSE (Train): 0.1312168701365234\n",
      "Test Loss: 0.6213144361972809, R2 Score (Test): 0.7492565964101916, RMSE (Test): 0.7854450941085815\n",
      "Epoch [2561/5000], Train Loss: 0.0115203185317417, R2 Score (Train): 0.9939875388736901, RMSE (Train): 0.13024259222065862\n",
      "Test Loss: 0.5693164765834808, R2 Score (Test): 0.759404400032496, RMSE (Test): 0.7693870663642883\n",
      "Epoch [2566/5000], Train Loss: 0.008983209651584426, R2 Score (Train): 0.9928017400623934, RMSE (Train): 0.14250846212014048\n",
      "Test Loss: 0.6336546838283539, R2 Score (Test): 0.7542088980264549, RMSE (Test): 0.7776499390602112\n",
      "Epoch [2571/5000], Train Loss: 0.008450482389889657, R2 Score (Train): 0.9940831323562093, RMSE (Train): 0.1292030654970843\n",
      "Test Loss: 0.5706638693809509, R2 Score (Test): 0.7575345497120063, RMSE (Test): 0.7723710536956787\n",
      "Epoch [2576/5000], Train Loss: 0.008661134090895454, R2 Score (Train): 0.9928263235066745, RMSE (Train): 0.1422649070707328\n",
      "Test Loss: 0.5916413068771362, R2 Score (Test): 0.7553777615885156, RMSE (Test): 0.7757986783981323\n",
      "Epoch [2581/5000], Train Loss: 0.012742435093969107, R2 Score (Train): 0.9897300352213082, RMSE (Train): 0.17022030633770036\n",
      "Test Loss: 0.6216374337673187, R2 Score (Test): 0.7525145664053822, RMSE (Test): 0.7803255915641785\n",
      "Epoch [2586/5000], Train Loss: 0.011862650901700059, R2 Score (Train): 0.9910434965868298, RMSE (Train): 0.158963032292425\n",
      "Test Loss: 0.546859472990036, R2 Score (Test): 0.7531859831573555, RMSE (Test): 0.7792664170265198\n",
      "Epoch [2591/5000], Train Loss: 0.013314363895915449, R2 Score (Train): 0.9937513311392112, RMSE (Train): 0.13277632557790367\n",
      "Test Loss: 0.5632771849632263, R2 Score (Test): 0.7551034968893613, RMSE (Test): 0.7762333750724792\n",
      "Epoch [2596/5000], Train Loss: 0.012506661703810096, R2 Score (Train): 0.990816094177277, RMSE (Train): 0.16096839043699615\n",
      "Test Loss: 0.586769625544548, R2 Score (Test): 0.7455803791568916, RMSE (Test): 0.7911819219589233\n",
      "Epoch [2601/5000], Train Loss: 0.008520444893899063, R2 Score (Train): 0.9931336572350392, RMSE (Train): 0.13918410155852437\n",
      "Test Loss: 0.6213081777095795, R2 Score (Test): 0.7525829252669078, RMSE (Test): 0.7802177667617798\n",
      "Epoch [2606/5000], Train Loss: 0.009172189282253385, R2 Score (Train): 0.991645075064429, RMSE (Train): 0.15353173822287935\n",
      "Test Loss: 0.6023038327693939, R2 Score (Test): 0.7507592700103494, RMSE (Test): 0.783087968826294\n",
      "Epoch [2611/5000], Train Loss: 0.008953124517574906, R2 Score (Train): 0.9947254963225342, RMSE (Train): 0.12198816272503386\n",
      "Test Loss: 0.5663943141698837, R2 Score (Test): 0.7498529055956877, RMSE (Test): 0.7845105528831482\n",
      "Epoch [2616/5000], Train Loss: 0.007079381107663115, R2 Score (Train): 0.992954256892185, RMSE (Train): 0.14099064356542082\n",
      "Test Loss: 0.6084672510623932, R2 Score (Test): 0.7601602602746494, RMSE (Test): 0.7681775689125061\n",
      "Epoch [2621/5000], Train Loss: 0.009800611219058434, R2 Score (Train): 0.9918079699124002, RMSE (Train): 0.15202767719058854\n",
      "Test Loss: 0.6411494761705399, R2 Score (Test): 0.7606746708195489, RMSE (Test): 0.7673532962799072\n",
      "Epoch [2626/5000], Train Loss: 0.0112848785550644, R2 Score (Train): 0.9906648845007614, RMSE (Train): 0.16228812344985993\n",
      "Test Loss: 0.5656310170888901, R2 Score (Test): 0.755027874275699, RMSE (Test): 0.7763532400131226\n",
      "Epoch [2631/5000], Train Loss: 0.008935514915113648, R2 Score (Train): 0.9928245383014999, RMSE (Train): 0.14228260763542355\n",
      "Test Loss: 0.5946921706199646, R2 Score (Test): 0.758198090565976, RMSE (Test): 0.7713134288787842\n",
      "Epoch [2636/5000], Train Loss: 0.009714089799672365, R2 Score (Train): 0.9911273675397204, RMSE (Train): 0.15821699674046258\n",
      "Test Loss: 0.5911446809768677, R2 Score (Test): 0.751831747650911, RMSE (Test): 0.7814013361930847\n",
      "Epoch [2641/5000], Train Loss: 0.013238527773258587, R2 Score (Train): 0.9945871006625271, RMSE (Train): 0.12357820025237502\n",
      "Test Loss: 0.5497166514396667, R2 Score (Test): 0.7612320145512158, RMSE (Test): 0.7664592266082764\n",
      "Epoch [2646/5000], Train Loss: 0.01075671900374194, R2 Score (Train): 0.9921803244820852, RMSE (Train): 0.14853241992608954\n",
      "Test Loss: 0.5682561099529266, R2 Score (Test): 0.7627152504656409, RMSE (Test): 0.7640749216079712\n",
      "Epoch [2651/5000], Train Loss: 0.01576775114517659, R2 Score (Train): 0.996341104318298, RMSE (Train): 0.10160191752979249\n",
      "Test Loss: 0.6007541120052338, R2 Score (Test): 0.7606758847002941, RMSE (Test): 0.7673513889312744\n",
      "Epoch [2656/5000], Train Loss: 0.006344725688298543, R2 Score (Train): 0.9947213978456337, RMSE (Train): 0.12203554809174899\n",
      "Test Loss: 0.6217436194419861, R2 Score (Test): 0.7587635704816273, RMSE (Test): 0.7704110145568848\n",
      "Epoch [2661/5000], Train Loss: 0.01050854183267802, R2 Score (Train): 0.9920841645529711, RMSE (Train): 0.14944289409669656\n",
      "Test Loss: 0.6076697111129761, R2 Score (Test): 0.7516070556209348, RMSE (Test): 0.7817550301551819\n",
      "Epoch [2666/5000], Train Loss: 0.009097861358895898, R2 Score (Train): 0.9940191627698368, RMSE (Train): 0.12989962048873333\n",
      "Test Loss: 0.5745584666728973, R2 Score (Test): 0.7633570726536127, RMSE (Test): 0.7630409598350525\n",
      "Epoch [2671/5000], Train Loss: 0.008506420883350074, R2 Score (Train): 0.9924558187481206, RMSE (Train): 0.14589249370800075\n",
      "Test Loss: 0.6133013069629669, R2 Score (Test): 0.7594049111880634, RMSE (Test): 0.7693862318992615\n",
      "Epoch [2676/5000], Train Loss: 0.009160191752016544, R2 Score (Train): 0.9928041412443523, RMSE (Train): 0.14248469128575603\n",
      "Test Loss: 0.6373252868652344, R2 Score (Test): 0.7538362535812023, RMSE (Test): 0.7782391905784607\n",
      "Epoch [2681/5000], Train Loss: 0.008842590924662849, R2 Score (Train): 0.9897611117158434, RMSE (Train): 0.16996257136403448\n",
      "Test Loss: 0.6523635238409042, R2 Score (Test): 0.7633881155223317, RMSE (Test): 0.7629908323287964\n",
      "Epoch [2686/5000], Train Loss: 0.008442046489411345, R2 Score (Train): 0.9933421304031923, RMSE (Train): 0.1370548892453035\n",
      "Test Loss: 0.6265160441398621, R2 Score (Test): 0.7572319293554727, RMSE (Test): 0.772852897644043\n",
      "Epoch [2691/5000], Train Loss: 0.008042762599264583, R2 Score (Train): 0.9940359025361873, RMSE (Train): 0.12971770507372327\n",
      "Test Loss: 0.5360897034406662, R2 Score (Test): 0.7627256071457628, RMSE (Test): 0.7640582919120789\n",
      "Epoch [2696/5000], Train Loss: 0.009512614808045328, R2 Score (Train): 0.9919406731133119, RMSE (Train): 0.1507912968347197\n",
      "Test Loss: 0.6003226935863495, R2 Score (Test): 0.7619306033178802, RMSE (Test): 0.7653372287750244\n",
      "Epoch [2701/5000], Train Loss: 0.00735032061735789, R2 Score (Train): 0.994304004054447, RMSE (Train): 0.126768609430124\n",
      "Test Loss: 0.6376471221446991, R2 Score (Test): 0.749786419984684, RMSE (Test): 0.7846148014068604\n",
      "Epoch [2706/5000], Train Loss: 0.009922003606334329, R2 Score (Train): 0.9950448146176036, RMSE (Train): 0.11823793713724923\n",
      "Test Loss: 0.5698281526565552, R2 Score (Test): 0.759169198637242, RMSE (Test): 0.7697630524635315\n",
      "Epoch [2711/5000], Train Loss: 0.007982641439108798, R2 Score (Train): 0.9898569699760245, RMSE (Train): 0.16916509079943845\n",
      "Test Loss: 0.643288791179657, R2 Score (Test): 0.7593879350178059, RMSE (Test): 0.7694133520126343\n",
      "Epoch [2716/5000], Train Loss: 0.008222445185917119, R2 Score (Train): 0.9947650772067389, RMSE (Train): 0.12152958954973049\n",
      "Test Loss: 0.581162303686142, R2 Score (Test): 0.7561842062939597, RMSE (Test): 0.7745187878608704\n",
      "Epoch [2721/5000], Train Loss: 0.007874085529086491, R2 Score (Train): 0.9913573285921481, RMSE (Train): 0.15615320105315936\n",
      "Test Loss: 0.6128261685371399, R2 Score (Test): 0.7616388772682603, RMSE (Test): 0.7658060193061829\n",
      "Epoch [2726/5000], Train Loss: 0.008921087486669421, R2 Score (Train): 0.9940156550730921, RMSE (Train): 0.1299377072707137\n",
      "Test Loss: 0.6169919371604919, R2 Score (Test): 0.757639972650799, RMSE (Test): 0.7722030878067017\n",
      "Epoch [2731/5000], Train Loss: 0.007030225203682979, R2 Score (Train): 0.9927625820944388, RMSE (Train): 0.14289555381024596\n",
      "Test Loss: 0.5852786004543304, R2 Score (Test): 0.7575610473365543, RMSE (Test): 0.7723288536071777\n",
      "Epoch [2736/5000], Train Loss: 0.008375675650313497, R2 Score (Train): 0.9917433312768011, RMSE (Train): 0.15262628053331778\n",
      "Test Loss: 0.5107342153787613, R2 Score (Test): 0.766869091576233, RMSE (Test): 0.7573575377464294\n",
      "Epoch [2741/5000], Train Loss: 0.011908137278320888, R2 Score (Train): 0.9950074793894111, RMSE (Train): 0.1186825376887339\n",
      "Test Loss: 0.5737396478652954, R2 Score (Test): 0.7568435502124533, RMSE (Test): 0.7734708189964294\n",
      "Epoch [2746/5000], Train Loss: 0.01059286113983641, R2 Score (Train): 0.9928536134176007, RMSE (Train): 0.1419940490237397\n",
      "Test Loss: 0.6123670935630798, R2 Score (Test): 0.7588273222771896, RMSE (Test): 0.7703092098236084\n",
      "Epoch [2751/5000], Train Loss: 0.009505952708423138, R2 Score (Train): 0.9891928621867058, RMSE (Train): 0.17461527630542453\n",
      "Test Loss: 0.5975625514984131, R2 Score (Test): 0.7547721037524664, RMSE (Test): 0.7767584323883057\n",
      "Epoch [2756/5000], Train Loss: 0.01217696761401991, R2 Score (Train): 0.9889456433939873, RMSE (Train): 0.17660119010422456\n",
      "Test Loss: 0.6013214886188507, R2 Score (Test): 0.7558790779214257, RMSE (Test): 0.7750033140182495\n",
      "Epoch [2761/5000], Train Loss: 0.008954562751265863, R2 Score (Train): 0.9945250504848571, RMSE (Train): 0.1242844944586846\n",
      "Test Loss: 0.599299967288971, R2 Score (Test): 0.7530880930142161, RMSE (Test): 0.7794209122657776\n",
      "Epoch [2766/5000], Train Loss: 0.0076780330467348295, R2 Score (Train): 0.994027424828871, RMSE (Train): 0.12980986639606967\n",
      "Test Loss: 0.541694387793541, R2 Score (Test): 0.7553097929717526, RMSE (Test): 0.7759063839912415\n",
      "Epoch [2771/5000], Train Loss: 0.0077809159411117435, R2 Score (Train): 0.9917927074572555, RMSE (Train): 0.152169231594557\n",
      "Test Loss: 0.6322313547134399, R2 Score (Test): 0.7534678506310195, RMSE (Test): 0.7788212895393372\n",
      "Epoch [2776/5000], Train Loss: 0.00761819949063162, R2 Score (Train): 0.9897410367949269, RMSE (Train): 0.17012910870143289\n",
      "Test Loss: 0.5929217338562012, R2 Score (Test): 0.7572920743850879, RMSE (Test): 0.7727571725845337\n",
      "Epoch [2781/5000], Train Loss: 0.01052160196316739, R2 Score (Train): 0.9924830438237435, RMSE (Train): 0.1456290109023371\n",
      "Test Loss: 0.5889897644519806, R2 Score (Test): 0.7656292149886416, RMSE (Test): 0.759368896484375\n",
      "Epoch [2786/5000], Train Loss: 0.014139149958888689, R2 Score (Train): 0.990983775053847, RMSE (Train): 0.15949213067520837\n",
      "Test Loss: 0.6086977124214172, R2 Score (Test): 0.7551811120983907, RMSE (Test): 0.7761104106903076\n",
      "Epoch [2791/5000], Train Loss: 0.010910372288587192, R2 Score (Train): 0.9952516974726382, RMSE (Train): 0.11574335883704714\n",
      "Test Loss: 0.599968433380127, R2 Score (Test): 0.762945658278133, RMSE (Test): 0.7637039422988892\n",
      "Epoch [2796/5000], Train Loss: 0.010464253059277931, R2 Score (Train): 0.9956451749733, RMSE (Train): 0.11084401430678005\n",
      "Test Loss: 0.5745275318622589, R2 Score (Test): 0.7577583931630092, RMSE (Test): 0.7720144391059875\n",
      "Epoch [2801/5000], Train Loss: 0.009095365802447, R2 Score (Train): 0.9915511166228346, RMSE (Train): 0.15439262400568285\n",
      "Test Loss: 0.6048799157142639, R2 Score (Test): 0.765115911110993, RMSE (Test): 0.7601999640464783\n",
      "Epoch [2806/5000], Train Loss: 0.008567571717624864, R2 Score (Train): 0.9898935726948453, RMSE (Train): 0.16885958551879676\n",
      "Test Loss: 0.5931952595710754, R2 Score (Test): 0.7545518168999099, RMSE (Test): 0.7771072387695312\n",
      "Epoch [2811/5000], Train Loss: 0.006953056319616735, R2 Score (Train): 0.989442143371955, RMSE (Train): 0.17258965902888193\n",
      "Test Loss: 0.5802252888679504, R2 Score (Test): 0.7632958909368828, RMSE (Test): 0.763139545917511\n",
      "Epoch [2816/5000], Train Loss: 0.00902979359185944, R2 Score (Train): 0.9941936411374049, RMSE (Train): 0.12799082181914134\n",
      "Test Loss: 0.6066184341907501, R2 Score (Test): 0.7567699482142017, RMSE (Test): 0.7735878825187683\n",
      "Epoch [2821/5000], Train Loss: 0.007785931656447549, R2 Score (Train): 0.9936957449728834, RMSE (Train): 0.1333655859452301\n",
      "Test Loss: 0.5833591520786285, R2 Score (Test): 0.7602978182636172, RMSE (Test): 0.7679572701454163\n",
      "Epoch [2826/5000], Train Loss: 0.007635741455790897, R2 Score (Train): 0.9943114670286861, RMSE (Train): 0.12668553519977183\n",
      "Test Loss: 0.5505816042423248, R2 Score (Test): 0.7609475926062099, RMSE (Test): 0.7669156789779663\n",
      "Epoch [2831/5000], Train Loss: 0.007102118067753811, R2 Score (Train): 0.9937061373544718, RMSE (Train): 0.13325561595554267\n",
      "Test Loss: 0.588081955909729, R2 Score (Test): 0.7588565282478342, RMSE (Test): 0.770262598991394\n",
      "Epoch [2836/5000], Train Loss: 0.008578608433405558, R2 Score (Train): 0.9913623569007493, RMSE (Train): 0.1561077694565173\n",
      "Test Loss: 0.5946024954319, R2 Score (Test): 0.7667942209947568, RMSE (Test): 0.757479190826416\n",
      "Epoch [2841/5000], Train Loss: 0.008577142667490989, R2 Score (Train): 0.9938718699283411, RMSE (Train): 0.13148944069641133\n",
      "Test Loss: 0.5514018535614014, R2 Score (Test): 0.7697737210449617, RMSE (Test): 0.7526247501373291\n",
      "Epoch [2846/5000], Train Loss: 0.010670880418426046, R2 Score (Train): 0.9931083540234718, RMSE (Train): 0.1394403199193166\n",
      "Test Loss: 0.5565766245126724, R2 Score (Test): 0.7573256967453895, RMSE (Test): 0.7727035880088806\n",
      "Epoch [2851/5000], Train Loss: 0.010073542362079024, R2 Score (Train): 0.9912618138279664, RMSE (Train): 0.15701369622841216\n",
      "Test Loss: 0.5957699716091156, R2 Score (Test): 0.7565939537677785, RMSE (Test): 0.7738677263259888\n",
      "Epoch [2856/5000], Train Loss: 0.009434102723995844, R2 Score (Train): 0.9922815640095529, RMSE (Train): 0.14756778009600324\n",
      "Test Loss: 0.611708790063858, R2 Score (Test): 0.7527761689064505, RMSE (Test): 0.7799131274223328\n",
      "Epoch [2861/5000], Train Loss: 0.00816753029357642, R2 Score (Train): 0.9936961979534705, RMSE (Train): 0.13336079449037908\n",
      "Test Loss: 0.5955947935581207, R2 Score (Test): 0.7611872623643445, RMSE (Test): 0.7665310502052307\n",
      "Epoch [2866/5000], Train Loss: 0.008781021577306092, R2 Score (Train): 0.9911827181884838, RMSE (Train): 0.15772271760794299\n",
      "Test Loss: 0.6206299066543579, R2 Score (Test): 0.7543838568940083, RMSE (Test): 0.7773730754852295\n",
      "Epoch [2871/5000], Train Loss: 0.009092733729630709, R2 Score (Train): 0.9916596732412603, RMSE (Train): 0.153397550102684\n",
      "Test Loss: 0.5736659169197083, R2 Score (Test): 0.7565236826499446, RMSE (Test): 0.7739793658256531\n",
      "Epoch [2876/5000], Train Loss: 0.010181532319014272, R2 Score (Train): 0.9917377587261005, RMSE (Train): 0.1526777767393924\n",
      "Test Loss: 0.6106456518173218, R2 Score (Test): 0.7527279128332314, RMSE (Test): 0.7799892425537109\n",
      "Epoch [2881/5000], Train Loss: 0.01038425803805391, R2 Score (Train): 0.9922194317037285, RMSE (Train): 0.14816053934252002\n",
      "Test Loss: 0.5617152154445648, R2 Score (Test): 0.7602216807616161, RMSE (Test): 0.7680791616439819\n",
      "Epoch [2886/5000], Train Loss: 0.008478664637853702, R2 Score (Train): 0.9927798084151739, RMSE (Train): 0.14272539428939954\n",
      "Test Loss: 0.5801931619644165, R2 Score (Test): 0.7670000294403775, RMSE (Test): 0.7571448087692261\n",
      "Epoch [2891/5000], Train Loss: 0.010524314905827245, R2 Score (Train): 0.989391201080445, RMSE (Train): 0.1730055357036139\n",
      "Test Loss: 0.6028152108192444, R2 Score (Test): 0.7623561045720905, RMSE (Test): 0.7646529674530029\n",
      "Epoch [2896/5000], Train Loss: 0.009786081888402501, R2 Score (Train): 0.9950619532342136, RMSE (Train): 0.11803328385256032\n",
      "Test Loss: 0.5671570003032684, R2 Score (Test): 0.7594260387815166, RMSE (Test): 0.7693524956703186\n",
      "Epoch [2901/5000], Train Loss: 0.011748828769971928, R2 Score (Train): 0.9911499061482886, RMSE (Train): 0.15801591445328664\n",
      "Test Loss: 0.6170278489589691, R2 Score (Test): 0.7571454117388877, RMSE (Test): 0.7729905843734741\n",
      "Epoch [2906/5000], Train Loss: 0.008929565665312111, R2 Score (Train): 0.9906991572064746, RMSE (Train): 0.16198993926648253\n",
      "Test Loss: 0.5455965548753738, R2 Score (Test): 0.7636306185579017, RMSE (Test): 0.7625997066497803\n",
      "Epoch [2911/5000], Train Loss: 0.006856200033022712, R2 Score (Train): 0.9931428093210554, RMSE (Train): 0.1390913120151166\n",
      "Test Loss: 0.5764786005020142, R2 Score (Test): 0.7623709400615055, RMSE (Test): 0.7646291255950928\n",
      "Epoch [2916/5000], Train Loss: 0.0066906573677745955, R2 Score (Train): 0.9906352380388522, RMSE (Train): 0.1625456165048273\n",
      "Test Loss: 0.6516401171684265, R2 Score (Test): 0.7606431890072747, RMSE (Test): 0.767403781414032\n",
      "Epoch [2921/5000], Train Loss: 0.008322623577745011, R2 Score (Train): 0.9920706826381859, RMSE (Train): 0.14957010235136037\n",
      "Test Loss: 0.5674166977405548, R2 Score (Test): 0.7622783425581718, RMSE (Test): 0.7647780776023865\n",
      "Epoch [2926/5000], Train Loss: 0.008969338688378533, R2 Score (Train): 0.9903157683758964, RMSE (Train): 0.16529490787724416\n",
      "Test Loss: 0.5765381157398224, R2 Score (Test): 0.7643569341167394, RMSE (Test): 0.761427104473114\n",
      "Epoch [2931/5000], Train Loss: 0.009018431999720633, R2 Score (Train): 0.9916715411361402, RMSE (Train): 0.15328837248068977\n",
      "Test Loss: 0.6065059304237366, R2 Score (Test): 0.7624817864001998, RMSE (Test): 0.76445072889328\n",
      "Epoch [2936/5000], Train Loss: 0.00894102950890859, R2 Score (Train): 0.99577356299665, RMSE (Train): 0.1091978507742943\n",
      "Test Loss: 0.6073205471038818, R2 Score (Test): 0.7600314157509611, RMSE (Test): 0.7683839201927185\n",
      "Epoch [2941/5000], Train Loss: 0.006581930967513472, R2 Score (Train): 0.9920854550720117, RMSE (Train): 0.14943071175898226\n",
      "Test Loss: 0.6001056134700775, R2 Score (Test): 0.7566274662559027, RMSE (Test): 0.7738143801689148\n",
      "Epoch [2946/5000], Train Loss: 0.008466301136650145, R2 Score (Train): 0.9919479075571893, RMSE (Train): 0.15072360283814423\n",
      "Test Loss: 0.5896048545837402, R2 Score (Test): 0.7658973619125959, RMSE (Test): 0.7589343190193176\n",
      "Epoch [2951/5000], Train Loss: 0.007605494000017643, R2 Score (Train): 0.9944242795116537, RMSE (Train): 0.1254230587396396\n",
      "Test Loss: 0.5769370496273041, R2 Score (Test): 0.7652056651342737, RMSE (Test): 0.7600547671318054\n",
      "Epoch [2956/5000], Train Loss: 0.009548079455271363, R2 Score (Train): 0.9913630210861293, RMSE (Train): 0.15610176744357\n",
      "Test Loss: 0.567835658788681, R2 Score (Test): 0.7624314582831073, RMSE (Test): 0.7645317912101746\n",
      "Epoch [2961/5000], Train Loss: 0.0118094013693432, R2 Score (Train): 0.996051269075554, RMSE (Train): 0.10554937381377207\n",
      "Test Loss: 0.6091262102127075, R2 Score (Test): 0.7540928731583284, RMSE (Test): 0.7778334021568298\n",
      "Epoch [2966/5000], Train Loss: 0.008371506235562265, R2 Score (Train): 0.9897246382296004, RMSE (Train): 0.17026502688636955\n",
      "Test Loss: 0.6045020520687103, R2 Score (Test): 0.758664036796044, RMSE (Test): 0.770569920539856\n",
      "Epoch [2971/5000], Train Loss: 0.009060396036754051, R2 Score (Train): 0.9892493304428585, RMSE (Train): 0.17415848866131547\n",
      "Test Loss: 0.4968884736299515, R2 Score (Test): 0.7671343750916527, RMSE (Test): 0.7569265365600586\n",
      "Epoch [2976/5000], Train Loss: 0.010681158610774824, R2 Score (Train): 0.993853500057738, RMSE (Train): 0.1316863716148802\n",
      "Test Loss: 0.54360631108284, R2 Score (Test): 0.7556806340298672, RMSE (Test): 0.7753182649612427\n",
      "Epoch [2981/5000], Train Loss: 0.009759124834090471, R2 Score (Train): 0.992230120538846, RMSE (Train): 0.14805873393491858\n",
      "Test Loss: 0.5639677047729492, R2 Score (Test): 0.7673724940326191, RMSE (Test): 0.7565394639968872\n",
      "Epoch [2986/5000], Train Loss: 0.006986328206645946, R2 Score (Train): 0.9922254640655891, RMSE (Train): 0.14810309294175178\n",
      "Test Loss: 0.5764366388320923, R2 Score (Test): 0.761071440004915, RMSE (Test): 0.7667170166969299\n",
      "Epoch [2991/5000], Train Loss: 0.008174834462503592, R2 Score (Train): 0.9894099832939075, RMSE (Train): 0.172852320130972\n",
      "Test Loss: 0.6500462293624878, R2 Score (Test): 0.7504546355953381, RMSE (Test): 0.783566415309906\n",
      "Epoch [2996/5000], Train Loss: 0.006619894159181665, R2 Score (Train): 0.9948586106967595, RMSE (Train): 0.12043899849648729\n",
      "Test Loss: 0.5566438436508179, R2 Score (Test): 0.7601939966075903, RMSE (Test): 0.7681235074996948\n",
      "Epoch [3001/5000], Train Loss: 0.009479847775461772, R2 Score (Train): 0.9895583801022161, RMSE (Train): 0.17163696661614247\n",
      "Test Loss: 0.6215868294239044, R2 Score (Test): 0.7538754514809547, RMSE (Test): 0.7781772017478943\n",
      "Epoch [3006/5000], Train Loss: 0.007341863626303772, R2 Score (Train): 0.9916554170210942, RMSE (Train): 0.15343668588779336\n",
      "Test Loss: 0.5856328904628754, R2 Score (Test): 0.7551962867034141, RMSE (Test): 0.7760863900184631\n",
      "Epoch [3011/5000], Train Loss: 0.005659669676485161, R2 Score (Train): 0.9935930627608626, RMSE (Train): 0.13444731254053996\n",
      "Test Loss: 0.5899147093296051, R2 Score (Test): 0.7579748022080123, RMSE (Test): 0.7716695666313171\n",
      "Epoch [3016/5000], Train Loss: 0.00847402346941332, R2 Score (Train): 0.9950346751430877, RMSE (Train): 0.11835884662820072\n",
      "Test Loss: 0.639896810054779, R2 Score (Test): 0.7487392384280959, RMSE (Test): 0.7862548828125\n",
      "Epoch [3021/5000], Train Loss: 0.008660988882184029, R2 Score (Train): 0.9886780925077916, RMSE (Train): 0.1787255703714245\n",
      "Test Loss: 0.6773015260696411, R2 Score (Test): 0.749536106510144, RMSE (Test): 0.785007119178772\n",
      "Epoch [3026/5000], Train Loss: 0.010020224067072073, R2 Score (Train): 0.9936114278081215, RMSE (Train): 0.13425448224613845\n",
      "Test Loss: 0.5766860842704773, R2 Score (Test): 0.7564843498911802, RMSE (Test): 0.7740418910980225\n",
      "Epoch [3031/5000], Train Loss: 0.008056006549547115, R2 Score (Train): 0.9926880493424818, RMSE (Train): 0.14362945636856125\n",
      "Test Loss: 0.535654291510582, R2 Score (Test): 0.7505241230530497, RMSE (Test): 0.7834572792053223\n",
      "Epoch [3036/5000], Train Loss: 0.011476544740920266, R2 Score (Train): 0.9937634165317268, RMSE (Train): 0.1326478637663912\n",
      "Test Loss: 0.5630720257759094, R2 Score (Test): 0.7572044287402755, RMSE (Test): 0.7728966474533081\n",
      "Epoch [3041/5000], Train Loss: 0.007085145334713161, R2 Score (Train): 0.9901702215965061, RMSE (Train): 0.166532405151314\n",
      "Test Loss: 0.5992857813835144, R2 Score (Test): 0.7542459349133483, RMSE (Test): 0.7775912880897522\n",
      "Epoch [3046/5000], Train Loss: 0.009795987396501005, R2 Score (Train): 0.9948871596430375, RMSE (Train): 0.12010414805410634\n",
      "Test Loss: 0.5292518734931946, R2 Score (Test): 0.7573939050454896, RMSE (Test): 0.7725949883460999\n",
      "Epoch [3051/5000], Train Loss: 0.00837972981389612, R2 Score (Train): 0.9909068972432641, RMSE (Train): 0.16017065076959372\n",
      "Test Loss: 0.6451834440231323, R2 Score (Test): 0.7564989243562822, RMSE (Test): 0.7740187048912048\n",
      "Epoch [3056/5000], Train Loss: 0.008185284794308245, R2 Score (Train): 0.9944320038537858, RMSE (Train): 0.1253361510128416\n",
      "Test Loss: 0.6089040637016296, R2 Score (Test): 0.7551459502133115, RMSE (Test): 0.7761661410331726\n",
      "Epoch [3061/5000], Train Loss: 0.0068449331835533185, R2 Score (Train): 0.9951505130139824, RMSE (Train): 0.11697008076540688\n",
      "Test Loss: 0.5981406569480896, R2 Score (Test): 0.7524385159817785, RMSE (Test): 0.7804455161094666\n",
      "Epoch [3066/5000], Train Loss: 0.009321653245327374, R2 Score (Train): 0.9919687220884286, RMSE (Train): 0.1505286677193298\n",
      "Test Loss: 0.6433583199977875, R2 Score (Test): 0.7480716668149711, RMSE (Test): 0.7872987389564514\n",
      "Epoch [3071/5000], Train Loss: 0.0059875489872259395, R2 Score (Train): 0.9922641537705398, RMSE (Train): 0.1477341184025003\n",
      "Test Loss: 0.6529699563980103, R2 Score (Test): 0.7522957781373598, RMSE (Test): 0.7806704640388489\n",
      "Epoch [3076/5000], Train Loss: 0.006976248851666848, R2 Score (Train): 0.9931613205529275, RMSE (Train): 0.13890344415080905\n",
      "Test Loss: 0.5810939371585846, R2 Score (Test): 0.7501502146552754, RMSE (Test): 0.7840442061424255\n",
      "Epoch [3081/5000], Train Loss: 0.007263399350146453, R2 Score (Train): 0.9929129308383492, RMSE (Train): 0.14140352180951524\n",
      "Test Loss: 0.5473722815513611, R2 Score (Test): 0.7554236925597302, RMSE (Test): 0.775725781917572\n",
      "Epoch [3086/5000], Train Loss: 0.006797456900433947, R2 Score (Train): 0.9946029872607209, RMSE (Train): 0.12339671896944078\n",
      "Test Loss: 0.633506566286087, R2 Score (Test): 0.7498869490366715, RMSE (Test): 0.7844571471214294\n",
      "Epoch [3091/5000], Train Loss: 0.007908168520467976, R2 Score (Train): 0.9929333819808168, RMSE (Train): 0.14119935046725066\n",
      "Test Loss: 0.6219214200973511, R2 Score (Test): 0.7457366706068269, RMSE (Test): 0.7909388542175293\n",
      "Epoch [3096/5000], Train Loss: 0.010348777286708355, R2 Score (Train): 0.9917858290031558, RMSE (Train): 0.1522329840352218\n",
      "Test Loss: 0.6080872714519501, R2 Score (Test): 0.7608478420076307, RMSE (Test): 0.7670755982398987\n",
      "Epoch [3101/5000], Train Loss: 0.009692651530106863, R2 Score (Train): 0.9936014751244863, RMSE (Train): 0.13435901831018815\n",
      "Test Loss: 0.6269598007202148, R2 Score (Test): 0.7562314986304104, RMSE (Test): 0.7744436860084534\n",
      "Epoch [3106/5000], Train Loss: 0.01123015695096304, R2 Score (Train): 0.9927572513096321, RMSE (Train): 0.14294816962165519\n",
      "Test Loss: 0.6010712683200836, R2 Score (Test): 0.7516009285820674, RMSE (Test): 0.781764566898346\n",
      "Epoch [3111/5000], Train Loss: 0.006743740348611027, R2 Score (Train): 0.9912953572488037, RMSE (Train): 0.15671204091195168\n",
      "Test Loss: 0.6577480435371399, R2 Score (Test): 0.757703083347182, RMSE (Test): 0.7721025943756104\n",
      "Epoch [3116/5000], Train Loss: 0.01066720283900698, R2 Score (Train): 0.9924282019985149, RMSE (Train): 0.14615928183175683\n",
      "Test Loss: 0.619380533695221, R2 Score (Test): 0.7588074991754883, RMSE (Test): 0.7703408598899841\n",
      "Epoch [3121/5000], Train Loss: 0.007798506645485759, R2 Score (Train): 0.9941731728115533, RMSE (Train): 0.12821621721796558\n",
      "Test Loss: 0.6036511063575745, R2 Score (Test): 0.7612077470561407, RMSE (Test): 0.7664982080459595\n",
      "Epoch [3126/5000], Train Loss: 0.007689538489406307, R2 Score (Train): 0.9919534623948226, RMSE (Train): 0.1506716045784941\n",
      "Test Loss: 0.5918425619602203, R2 Score (Test): 0.7512750506427048, RMSE (Test): 0.7822772860527039\n",
      "Epoch [3131/5000], Train Loss: 0.007927708250160018, R2 Score (Train): 0.9931977697288006, RMSE (Train): 0.1385327819275085\n",
      "Test Loss: 0.5699988156557083, R2 Score (Test): 0.7565304153105061, RMSE (Test): 0.7739686965942383\n",
      "Epoch [3136/5000], Train Loss: 0.010259122354909778, R2 Score (Train): 0.9933710669298443, RMSE (Train): 0.13675672998115018\n",
      "Test Loss: 0.6078369915485382, R2 Score (Test): 0.7600824610468964, RMSE (Test): 0.7683021426200867\n",
      "Epoch [3141/5000], Train Loss: 0.00881937596326073, R2 Score (Train): 0.9930091590939863, RMSE (Train): 0.14044025199925758\n",
      "Test Loss: 0.5987913906574249, R2 Score (Test): 0.7567392126102013, RMSE (Test): 0.7736367583274841\n",
      "Epoch [3146/5000], Train Loss: 0.00818496651481837, R2 Score (Train): 0.9934975520589876, RMSE (Train): 0.13544573449329558\n",
      "Test Loss: 0.6043217778205872, R2 Score (Test): 0.7511764394613808, RMSE (Test): 0.7824323773384094\n",
      "Epoch [3151/5000], Train Loss: 0.009409640760471424, R2 Score (Train): 0.992333694939972, RMSE (Train): 0.1470685935211028\n",
      "Test Loss: 0.6057302951812744, R2 Score (Test): 0.7584588434332891, RMSE (Test): 0.7708975076675415\n",
      "Epoch [3156/5000], Train Loss: 0.006673914904240519, R2 Score (Train): 0.9945072000082221, RMSE (Train): 0.12448693761138195\n",
      "Test Loss: 0.6183483600616455, R2 Score (Test): 0.7572813951570609, RMSE (Test): 0.7727741599082947\n",
      "Epoch [3161/5000], Train Loss: 0.007661865830111007, R2 Score (Train): 0.9940330748499945, RMSE (Train): 0.12974845218168005\n",
      "Test Loss: 0.6082659661769867, R2 Score (Test): 0.7567299589527179, RMSE (Test): 0.7736514806747437\n",
      "Epoch [3166/5000], Train Loss: 0.007248702924698591, R2 Score (Train): 0.9904337729552277, RMSE (Train): 0.164284743026326\n",
      "Test Loss: 0.5816210508346558, R2 Score (Test): 0.7602929632579568, RMSE (Test): 0.7679650187492371\n",
      "Epoch [3171/5000], Train Loss: 0.007397802352594833, R2 Score (Train): 0.9931802490262668, RMSE (Train): 0.13871107864999407\n",
      "Test Loss: 0.5781131088733673, R2 Score (Test): 0.7641420389839901, RMSE (Test): 0.7617743015289307\n",
      "Epoch [3176/5000], Train Loss: 0.007049407693557441, R2 Score (Train): 0.9942265170025453, RMSE (Train): 0.12762796256258643\n",
      "Test Loss: 0.533487543463707, R2 Score (Test): 0.7591735485787573, RMSE (Test): 0.7697561383247375\n",
      "Epoch [3181/5000], Train Loss: 0.007022700330708176, R2 Score (Train): 0.9948193543541851, RMSE (Train): 0.1208979215244204\n",
      "Test Loss: 0.575721800327301, R2 Score (Test): 0.7577395166822017, RMSE (Test): 0.7720445394515991\n",
      "Epoch [3186/5000], Train Loss: 0.007592844233537714, R2 Score (Train): 0.994047535251845, RMSE (Train): 0.12959113892508384\n",
      "Test Loss: 0.5514972656965256, R2 Score (Test): 0.7593279567587912, RMSE (Test): 0.7695092558860779\n",
      "Epoch [3191/5000], Train Loss: 0.00976387585978955, R2 Score (Train): 0.9932655795309849, RMSE (Train): 0.13784055239970844\n",
      "Test Loss: 0.545206293463707, R2 Score (Test): 0.7538259181274688, RMSE (Test): 0.7782555222511292\n",
      "Epoch [3196/5000], Train Loss: 0.009257143829017878, R2 Score (Train): 0.990671860398138, RMSE (Train): 0.16222747519798508\n",
      "Test Loss: 0.6039620339870453, R2 Score (Test): 0.767097064778872, RMSE (Test): 0.7569871544837952\n",
      "Epoch [3201/5000], Train Loss: 0.00756170426029712, R2 Score (Train): 0.9918519090601292, RMSE (Train): 0.15161941770901424\n",
      "Test Loss: 0.5951710045337677, R2 Score (Test): 0.7443409319480723, RMSE (Test): 0.793106734752655\n",
      "Epoch [3206/5000], Train Loss: 0.008123862207867205, R2 Score (Train): 0.9940843806591236, RMSE (Train): 0.12918943555971596\n",
      "Test Loss: 0.6325414180755615, R2 Score (Test): 0.7577937351368607, RMSE (Test): 0.7719581127166748\n",
      "Epoch [3211/5000], Train Loss: 0.01040433863333116, R2 Score (Train): 0.9906520647622998, RMSE (Train): 0.1623995188318106\n",
      "Test Loss: 0.6007296144962311, R2 Score (Test): 0.7562600350074447, RMSE (Test): 0.7743983864784241\n",
      "Epoch [3216/5000], Train Loss: 0.009203781412603954, R2 Score (Train): 0.9908991045074604, RMSE (Train): 0.1602392687262355\n",
      "Test Loss: 0.6780211478471756, R2 Score (Test): 0.7509094760734125, RMSE (Test): 0.7828519940376282\n",
      "Epoch [3221/5000], Train Loss: 0.006385283622269829, R2 Score (Train): 0.9932727614560283, RMSE (Train): 0.13776703274077218\n",
      "Test Loss: 0.6737959980964661, R2 Score (Test): 0.7477821227134585, RMSE (Test): 0.7877510190010071\n",
      "Epoch [3226/5000], Train Loss: 0.007940280173594752, R2 Score (Train): 0.9903850333922016, RMSE (Train): 0.16470272356330568\n",
      "Test Loss: 0.6112723052501678, R2 Score (Test): 0.7578949172216598, RMSE (Test): 0.7717968821525574\n",
      "Epoch [3231/5000], Train Loss: 0.00865531499342372, R2 Score (Train): 0.9946033827568058, RMSE (Train): 0.12339219759662774\n",
      "Test Loss: 0.5827888548374176, R2 Score (Test): 0.7618019449516933, RMSE (Test): 0.7655439972877502\n",
      "Epoch [3236/5000], Train Loss: 0.010072042350657284, R2 Score (Train): 0.9895507015498557, RMSE (Train): 0.17170006416439898\n",
      "Test Loss: 0.5794975757598877, R2 Score (Test): 0.7600798080607837, RMSE (Test): 0.7683063745498657\n",
      "Epoch [3241/5000], Train Loss: 0.008754069412437579, R2 Score (Train): 0.9934023275685309, RMSE (Train): 0.13643389109315449\n",
      "Test Loss: 0.5612352639436722, R2 Score (Test): 0.7585647504907405, RMSE (Test): 0.7707284688949585\n",
      "Epoch [3246/5000], Train Loss: 0.006033053350013991, R2 Score (Train): 0.9932912751361285, RMSE (Train): 0.1375773314242207\n",
      "Test Loss: 0.5572207421064377, R2 Score (Test): 0.7595580753572051, RMSE (Test): 0.7691413164138794\n",
      "Epoch [3251/5000], Train Loss: 0.007779164821840823, R2 Score (Train): 0.9929311403139657, RMSE (Train): 0.14122174426331904\n",
      "Test Loss: 0.5650014132261276, R2 Score (Test): 0.7484271641448988, RMSE (Test): 0.7867430448532104\n",
      "Epoch [3256/5000], Train Loss: 0.007897339256790778, R2 Score (Train): 0.9919614145389001, RMSE (Train): 0.15059713413295267\n",
      "Test Loss: 0.5866032540798187, R2 Score (Test): 0.7442080077246966, RMSE (Test): 0.7933129072189331\n",
      "Epoch [3261/5000], Train Loss: 0.007006238602722685, R2 Score (Train): 0.9877596109018071, RMSE (Train): 0.18583371218497002\n",
      "Test Loss: 0.6132369637489319, R2 Score (Test): 0.7506423700691279, RMSE (Test): 0.7832715511322021\n",
      "Epoch [3266/5000], Train Loss: 0.007276454261348893, R2 Score (Train): 0.9928253290251388, RMSE (Train): 0.14227476776910822\n",
      "Test Loss: 0.59079310297966, R2 Score (Test): 0.7562542078478751, RMSE (Test): 0.7744075655937195\n",
      "Epoch [3271/5000], Train Loss: 0.006156685529276729, R2 Score (Train): 0.9939643160792563, RMSE (Train): 0.1304938771707656\n",
      "Test Loss: 0.5951902568340302, R2 Score (Test): 0.7517752207164067, RMSE (Test): 0.7814903259277344\n",
      "Epoch [3276/5000], Train Loss: 0.009129288528735438, R2 Score (Train): 0.9930907380066778, RMSE (Train): 0.13961842071799904\n",
      "Test Loss: 0.6014935970306396, R2 Score (Test): 0.7553825475024514, RMSE (Test): 0.7757910490036011\n",
      "Epoch [3281/5000], Train Loss: 0.006673617482495804, R2 Score (Train): 0.9936421828332047, RMSE (Train): 0.13393093723759675\n",
      "Test Loss: 0.6112086176872253, R2 Score (Test): 0.7631750606726474, RMSE (Test): 0.7633342146873474\n",
      "Epoch [3286/5000], Train Loss: 0.00702681636903435, R2 Score (Train): 0.9945692675956925, RMSE (Train): 0.12378160013445114\n",
      "Test Loss: 0.5804909467697144, R2 Score (Test): 0.7602590120578173, RMSE (Test): 0.768019437789917\n",
      "Epoch [3291/5000], Train Loss: 0.0072491758813460665, R2 Score (Train): 0.9938219853627842, RMSE (Train): 0.1320235350299523\n",
      "Test Loss: 0.6153114438056946, R2 Score (Test): 0.7610611698550834, RMSE (Test): 0.7667334675788879\n",
      "Epoch [3296/5000], Train Loss: 0.006579220372562607, R2 Score (Train): 0.9903199383054397, RMSE (Train): 0.16525931691038315\n",
      "Test Loss: 0.5966436862945557, R2 Score (Test): 0.7618229988252037, RMSE (Test): 0.7655101418495178\n",
      "Epoch [3301/5000], Train Loss: 0.007211022273016472, R2 Score (Train): 0.9914619773115046, RMSE (Train): 0.15520494096353943\n",
      "Test Loss: 0.6014091968536377, R2 Score (Test): 0.7688254384762562, RMSE (Test): 0.7541731595993042\n",
      "Epoch [3306/5000], Train Loss: 0.0069700930422792835, R2 Score (Train): 0.993416544412525, RMSE (Train): 0.1362868160905429\n",
      "Test Loss: 0.5903196334838867, R2 Score (Test): 0.7618009279229964, RMSE (Test): 0.7655456066131592\n",
      "Epoch [3311/5000], Train Loss: 0.010675085242837667, R2 Score (Train): 0.9950078508229574, RMSE (Train): 0.11867812273492222\n",
      "Test Loss: 0.6103615462779999, R2 Score (Test): 0.7582408302757631, RMSE (Test): 0.7712453007698059\n",
      "Epoch [3316/5000], Train Loss: 0.009853164471375445, R2 Score (Train): 0.9928104391170164, RMSE (Train): 0.14242232577103414\n",
      "Test Loss: 0.6175480484962463, R2 Score (Test): 0.7587188333590594, RMSE (Test): 0.7704824805259705\n",
      "Epoch [3321/5000], Train Loss: 0.0052842264412902296, R2 Score (Train): 0.9941458887176521, RMSE (Train): 0.12851605258344023\n",
      "Test Loss: 0.5443119704723358, R2 Score (Test): 0.7599786002081969, RMSE (Test): 0.76846843957901\n",
      "Epoch [3326/5000], Train Loss: 0.00713769057377552, R2 Score (Train): 0.9930393842393768, RMSE (Train): 0.14013632395952214\n",
      "Test Loss: 0.5660090446472168, R2 Score (Test): 0.7512626293153362, RMSE (Test): 0.7822968363761902\n",
      "Epoch [3331/5000], Train Loss: 0.011614472682898244, R2 Score (Train): 0.9920667203068635, RMSE (Train): 0.14960746825914242\n",
      "Test Loss: 0.5719587802886963, R2 Score (Test): 0.7615780902094039, RMSE (Test): 0.765903651714325\n",
      "Epoch [3336/5000], Train Loss: 0.008151235951421162, R2 Score (Train): 0.9914829532477513, RMSE (Train): 0.15501417249936436\n",
      "Test Loss: 0.6176019906997681, R2 Score (Test): 0.7560161195753065, RMSE (Test): 0.7747856974601746\n",
      "Epoch [3341/5000], Train Loss: 0.009744569271182021, R2 Score (Train): 0.9916354203245183, RMSE (Train): 0.15362042130678236\n",
      "Test Loss: 0.5701880753040314, R2 Score (Test): 0.7612519664326287, RMSE (Test): 0.7664272785186768\n",
      "Epoch [3346/5000], Train Loss: 0.007059599641555299, R2 Score (Train): 0.9923184971400583, RMSE (Train): 0.1472142968556012\n",
      "Test Loss: 0.6355390548706055, R2 Score (Test): 0.762108435357203, RMSE (Test): 0.7650513052940369\n",
      "Epoch [3351/5000], Train Loss: 0.007517620222643018, R2 Score (Train): 0.9914782785224074, RMSE (Train): 0.15505670774046737\n",
      "Test Loss: 0.610291987657547, R2 Score (Test): 0.7605840431917283, RMSE (Test): 0.7674986720085144\n",
      "Epoch [3356/5000], Train Loss: 0.0060654702635171516, R2 Score (Train): 0.9921204569296536, RMSE (Train): 0.14909991901328123\n",
      "Test Loss: 0.548432320356369, R2 Score (Test): 0.7622287990263881, RMSE (Test): 0.7648577094078064\n",
      "Epoch [3361/5000], Train Loss: 0.006169634832379718, R2 Score (Train): 0.9940321756288256, RMSE (Train): 0.1297582284195714\n",
      "Test Loss: 0.5463002622127533, R2 Score (Test): 0.7615845058122619, RMSE (Test): 0.7658933401107788\n",
      "Epoch [3366/5000], Train Loss: 0.007598970162992676, R2 Score (Train): 0.9916854814281171, RMSE (Train): 0.15316003063257302\n",
      "Test Loss: 0.5946273803710938, R2 Score (Test): 0.7504158755868378, RMSE (Test): 0.7836272120475769\n",
      "Epoch [3371/5000], Train Loss: 0.006364382978063077, R2 Score (Train): 0.994542380521993, RMSE (Train): 0.12408763766409911\n",
      "Test Loss: 0.5520710349082947, R2 Score (Test): 0.766534360381415, RMSE (Test): 0.7579010725021362\n",
      "Epoch [3376/5000], Train Loss: 0.008972797232369581, R2 Score (Train): 0.9927488518841232, RMSE (Train): 0.14303103419746083\n",
      "Test Loss: 0.5743201076984406, R2 Score (Test): 0.7623023516810989, RMSE (Test): 0.764739453792572\n",
      "Epoch [3381/5000], Train Loss: 0.007501785992644727, R2 Score (Train): 0.9939911531308698, RMSE (Train): 0.13020344011824342\n",
      "Test Loss: 0.594990074634552, R2 Score (Test): 0.7630807547196503, RMSE (Test): 0.7634862661361694\n",
      "Epoch [3386/5000], Train Loss: 0.007312947923007111, R2 Score (Train): 0.9898999741836333, RMSE (Train): 0.16880609856757461\n",
      "Test Loss: 0.597030371427536, R2 Score (Test): 0.7634886577627459, RMSE (Test): 0.7628286480903625\n",
      "Epoch [3391/5000], Train Loss: 0.007420774820881586, R2 Score (Train): 0.9937451611158926, RMSE (Train): 0.13284186200753836\n",
      "Test Loss: 0.5940857529640198, R2 Score (Test): 0.7598330824174591, RMSE (Test): 0.7687013149261475\n",
      "Epoch [3396/5000], Train Loss: 0.006944398880780985, R2 Score (Train): 0.9931524762144953, RMSE (Train): 0.13899323576434824\n",
      "Test Loss: 0.6153832376003265, R2 Score (Test): 0.7496297731741841, RMSE (Test): 0.784860372543335\n",
      "Epoch [3401/5000], Train Loss: 0.00852482597110793, R2 Score (Train): 0.9926406931858199, RMSE (Train): 0.14409381683383823\n",
      "Test Loss: 0.5579925775527954, R2 Score (Test): 0.7589112236108638, RMSE (Test): 0.7701752185821533\n",
      "Epoch [3406/5000], Train Loss: 0.008601968875154853, R2 Score (Train): 0.9870965121724109, RMSE (Train): 0.19080091317429332\n",
      "Test Loss: 0.5429269969463348, R2 Score (Test): 0.7544753534437477, RMSE (Test): 0.7772282958030701\n",
      "Epoch [3411/5000], Train Loss: 0.005644469618952523, R2 Score (Train): 0.9924762249522269, RMSE (Train): 0.14569504829778943\n",
      "Test Loss: 0.5749461650848389, R2 Score (Test): 0.7574494179994913, RMSE (Test): 0.772506594657898\n",
      "Epoch [3416/5000], Train Loss: 0.006680690004335095, R2 Score (Train): 0.9931008077685192, RMSE (Train): 0.13951664162211713\n",
      "Test Loss: 0.5525256842374802, R2 Score (Test): 0.7593774291741436, RMSE (Test): 0.7694301605224609\n",
      "Epoch [3421/5000], Train Loss: 0.008498144006201377, R2 Score (Train): 0.9945639026472143, RMSE (Train): 0.12384272614190159\n",
      "Test Loss: 0.6342673301696777, R2 Score (Test): 0.759544138669596, RMSE (Test): 0.7691636085510254\n",
      "Epoch [3426/5000], Train Loss: 0.008426593655409912, R2 Score (Train): 0.9901771261084389, RMSE (Train): 0.1664739080563764\n",
      "Test Loss: 0.5798515677452087, R2 Score (Test): 0.7619391341242161, RMSE (Test): 0.7653234601020813\n",
      "Epoch [3431/5000], Train Loss: 0.007874500724331787, R2 Score (Train): 0.991937358762121, RMSE (Train): 0.15082229966841817\n",
      "Test Loss: 0.6218745112419128, R2 Score (Test): 0.755413101200366, RMSE (Test): 0.7757425904273987\n",
      "Epoch [3436/5000], Train Loss: 0.006963326731541504, R2 Score (Train): 0.9931853082456598, RMSE (Train): 0.13865961782820097\n",
      "Test Loss: 0.6771039366722107, R2 Score (Test): 0.7598013143162576, RMSE (Test): 0.7687521576881409\n",
      "Epoch [3441/5000], Train Loss: 0.008830736391246319, R2 Score (Train): 0.9915654551326807, RMSE (Train): 0.1542615593373936\n",
      "Test Loss: 0.6417897641658783, R2 Score (Test): 0.7538582178232072, RMSE (Test): 0.7782045006752014\n",
      "Epoch [3446/5000], Train Loss: 0.009043937762423107, R2 Score (Train): 0.9919985730715151, RMSE (Train): 0.15024866173016518\n",
      "Test Loss: 0.5522952824831009, R2 Score (Test): 0.7567040989539264, RMSE (Test): 0.7736926078796387\n",
      "Epoch [3451/5000], Train Loss: 0.006713714392390102, R2 Score (Train): 0.9934056223197215, RMSE (Train): 0.1363998206036227\n",
      "Test Loss: 0.5688764750957489, R2 Score (Test): 0.7610320614471088, RMSE (Test): 0.7667801976203918\n",
      "Epoch [3456/5000], Train Loss: 0.01527907489798963, R2 Score (Train): 0.9936623953624366, RMSE (Train): 0.13371787370466817\n",
      "Test Loss: 0.5891078412532806, R2 Score (Test): 0.752565122113794, RMSE (Test): 0.7802459001541138\n",
      "Epoch [3461/5000], Train Loss: 0.01129387435503304, R2 Score (Train): 0.993493202338058, RMSE (Train): 0.13549102917757078\n",
      "Test Loss: 0.5233851969242096, R2 Score (Test): 0.7610192204433693, RMSE (Test): 0.7668007612228394\n",
      "Epoch [3466/5000], Train Loss: 0.00955897633684799, R2 Score (Train): 0.989179057432153, RMSE (Train): 0.17472676520190047\n",
      "Test Loss: 0.6031578779220581, R2 Score (Test): 0.7627814006495303, RMSE (Test): 0.7639684677124023\n",
      "Epoch [3471/5000], Train Loss: 0.010420202083575228, R2 Score (Train): 0.9945259505651977, RMSE (Train): 0.12427427786875417\n",
      "Test Loss: 0.632447361946106, R2 Score (Test): 0.756237688598524, RMSE (Test): 0.7744338512420654\n",
      "Epoch [3476/5000], Train Loss: 0.007700898335315287, R2 Score (Train): 0.9943366859946936, RMSE (Train): 0.1264044058598144\n",
      "Test Loss: 0.5951784551143646, R2 Score (Test): 0.7590339866185887, RMSE (Test): 0.7699791193008423\n",
      "Epoch [3481/5000], Train Loss: 0.006011523849641283, R2 Score (Train): 0.9948508241627085, RMSE (Train): 0.12053016525174948\n",
      "Test Loss: 0.6204430460929871, R2 Score (Test): 0.7559354923673196, RMSE (Test): 0.7749137282371521\n",
      "Epoch [3486/5000], Train Loss: 0.006273129450467725, R2 Score (Train): 0.995042711835213, RMSE (Train): 0.11826302220082612\n",
      "Test Loss: 0.5627763271331787, R2 Score (Test): 0.7534648369354737, RMSE (Test): 0.7788260579109192\n",
      "Epoch [3491/5000], Train Loss: 0.009222719081056615, R2 Score (Train): 0.9938667625382731, RMSE (Train): 0.1315442231493836\n",
      "Test Loss: 0.5599571466445923, R2 Score (Test): 0.7601275706095352, RMSE (Test): 0.7682299613952637\n",
      "Epoch [3496/5000], Train Loss: 0.0080136883771047, R2 Score (Train): 0.9922759897974762, RMSE (Train): 0.14762105679881832\n",
      "Test Loss: 0.639077216386795, R2 Score (Test): 0.7541005211816242, RMSE (Test): 0.7778213620185852\n",
      "Epoch [3501/5000], Train Loss: 0.007962468623494109, R2 Score (Train): 0.9876626261072273, RMSE (Train): 0.18656847168924695\n",
      "Test Loss: 0.6226156651973724, R2 Score (Test): 0.7567214759350096, RMSE (Test): 0.7736649513244629\n",
      "Epoch [3506/5000], Train Loss: 0.00597492551120619, R2 Score (Train): 0.9912668352149442, RMSE (Train): 0.1569685758902973\n",
      "Test Loss: 0.6740780174732208, R2 Score (Test): 0.7500266018270255, RMSE (Test): 0.7842380404472351\n",
      "Epoch [3511/5000], Train Loss: 0.01048378343693912, R2 Score (Train): 0.990481593366922, RMSE (Train): 0.16387360880226678\n",
      "Test Loss: 0.5827677547931671, R2 Score (Test): 0.7554223960753159, RMSE (Test): 0.7757278084754944\n",
      "Epoch [3516/5000], Train Loss: 0.008276635276464125, R2 Score (Train): 0.9945329690809841, RMSE (Train): 0.12419458359955722\n",
      "Test Loss: 0.6194882094860077, R2 Score (Test): 0.7604939869803041, RMSE (Test): 0.7676429152488708\n",
      "Epoch [3521/5000], Train Loss: 0.006285407153579096, R2 Score (Train): 0.9912154892209043, RMSE (Train): 0.15742934216720286\n",
      "Test Loss: 0.6120621562004089, R2 Score (Test): 0.7633013495762249, RMSE (Test): 0.763130784034729\n",
      "Epoch [3526/5000], Train Loss: 0.009315988048911095, R2 Score (Train): 0.9883690513784638, RMSE (Train): 0.1811483820026296\n",
      "Test Loss: 0.6303646862506866, R2 Score (Test): 0.754940283466104, RMSE (Test): 0.7764920592308044\n",
      "Epoch [3531/5000], Train Loss: 0.010302807165620228, R2 Score (Train): 0.9947168383035031, RMSE (Train): 0.12208824254582368\n",
      "Test Loss: 0.5869890451431274, R2 Score (Test): 0.7544435288033541, RMSE (Test): 0.7772786617279053\n",
      "Epoch [3536/5000], Train Loss: 0.009277773050901791, R2 Score (Train): 0.9917930509149384, RMSE (Train): 0.152166047582398\n",
      "Test Loss: 0.6073755621910095, R2 Score (Test): 0.7491979121647026, RMSE (Test): 0.7855369448661804\n",
      "Epoch [3541/5000], Train Loss: 0.00755566048125426, R2 Score (Train): 0.995245079825624, RMSE (Train): 0.11582398575595619\n",
      "Test Loss: 0.5752582550048828, R2 Score (Test): 0.76167715593385, RMSE (Test): 0.7657445073127747\n",
      "Epoch [3546/5000], Train Loss: 0.009469420513293395, R2 Score (Train): 0.9921077282224939, RMSE (Train): 0.14922029930412584\n",
      "Test Loss: 0.5428891628980637, R2 Score (Test): 0.7631642612988883, RMSE (Test): 0.7633516788482666\n",
      "Epoch [3551/5000], Train Loss: 0.010327403685854128, R2 Score (Train): 0.9932929908989534, RMSE (Train): 0.1375597375319353\n",
      "Test Loss: 0.5310387760400772, R2 Score (Test): 0.7632963362892399, RMSE (Test): 0.7631388306617737\n",
      "Epoch [3556/5000], Train Loss: 0.005742454144638032, R2 Score (Train): 0.9928678215280833, RMSE (Train): 0.1418528258426569\n",
      "Test Loss: 0.5735750496387482, R2 Score (Test): 0.7594911893903946, RMSE (Test): 0.7692483067512512\n",
      "Epoch [3561/5000], Train Loss: 0.006796761726339658, R2 Score (Train): 0.9934992796499015, RMSE (Train): 0.1354277404726942\n",
      "Test Loss: 0.6395446360111237, R2 Score (Test): 0.7558969926163205, RMSE (Test): 0.7749749422073364\n",
      "Epoch [3566/5000], Train Loss: 0.0072952002519741654, R2 Score (Train): 0.99192044576561, RMSE (Train): 0.15098040670936064\n",
      "Test Loss: 0.5853547751903534, R2 Score (Test): 0.7691377722211246, RMSE (Test): 0.7536634802818298\n",
      "Epoch [3571/5000], Train Loss: 0.005841983035982897, R2 Score (Train): 0.9947330772397555, RMSE (Train): 0.12190046587704906\n",
      "Test Loss: 0.5462645888328552, R2 Score (Test): 0.7620143344489725, RMSE (Test): 0.7652026414871216\n",
      "Epoch [3576/5000], Train Loss: 0.007571888389065862, R2 Score (Train): 0.99153791509237, RMSE (Train): 0.1545131975342471\n",
      "Test Loss: 0.5373460501432419, R2 Score (Test): 0.7581020829607069, RMSE (Test): 0.7714666128158569\n",
      "Epoch [3581/5000], Train Loss: 0.007883950136601925, R2 Score (Train): 0.9929412254027493, RMSE (Train): 0.14112096830786358\n",
      "Test Loss: 0.5810088515281677, R2 Score (Test): 0.7563469764347374, RMSE (Test): 0.7742601633071899\n",
      "Epoch [3586/5000], Train Loss: 0.008995314206307134, R2 Score (Train): 0.9908441514659829, RMSE (Train): 0.16072231913696286\n",
      "Test Loss: 0.6148311495780945, R2 Score (Test): 0.752298744014526, RMSE (Test): 0.7806658148765564\n",
      "Epoch [3591/5000], Train Loss: 0.00604764084952573, R2 Score (Train): 0.9956109330810176, RMSE (Train): 0.11127894303112547\n",
      "Test Loss: 0.591326504945755, R2 Score (Test): 0.7551082838553195, RMSE (Test): 0.7762258052825928\n",
      "Epoch [3596/5000], Train Loss: 0.0070646676467731595, R2 Score (Train): 0.9920217151819112, RMSE (Train): 0.15003122620722764\n",
      "Test Loss: 0.6233161091804504, R2 Score (Test): 0.7591695252977131, RMSE (Test): 0.7697625160217285\n",
      "Epoch [3601/5000], Train Loss: 0.007860228535719216, R2 Score (Train): 0.9938013295619987, RMSE (Train): 0.13224405702365932\n",
      "Test Loss: 0.6293191909790039, R2 Score (Test): 0.7603145493484472, RMSE (Test): 0.7679304480552673\n",
      "Epoch [3606/5000], Train Loss: 0.006225439079571515, R2 Score (Train): 0.9951685495688292, RMSE (Train): 0.11675235643593163\n",
      "Test Loss: 0.5843440890312195, R2 Score (Test): 0.7547539667682643, RMSE (Test): 0.7767871618270874\n",
      "Epoch [3611/5000], Train Loss: 0.007278389530256391, R2 Score (Train): 0.989159573318294, RMSE (Train): 0.17488400035314483\n",
      "Test Loss: 0.5798444449901581, R2 Score (Test): 0.7564813861258802, RMSE (Test): 0.7740466594696045\n",
      "Epoch [3616/5000], Train Loss: 0.007530628742339711, R2 Score (Train): 0.9944216316124345, RMSE (Train): 0.1254528367925417\n",
      "Test Loss: 0.5745977759361267, R2 Score (Test): 0.7566366373778278, RMSE (Test): 0.7737998962402344\n",
      "Epoch [3621/5000], Train Loss: 0.00809567862112696, R2 Score (Train): 0.9945308017497897, RMSE (Train): 0.12421919879994632\n",
      "Test Loss: 0.5724862813949585, R2 Score (Test): 0.7600413951615528, RMSE (Test): 0.7683678865432739\n",
      "Epoch [3626/5000], Train Loss: 0.007818832256210348, R2 Score (Train): 0.9948646037897114, RMSE (Train): 0.12036878278859006\n",
      "Test Loss: 0.5453180819749832, R2 Score (Test): 0.7586694598954893, RMSE (Test): 0.7705612778663635\n",
      "Epoch [3631/5000], Train Loss: 0.01040648304236432, R2 Score (Train): 0.9911128384216501, RMSE (Train): 0.1583464855445111\n",
      "Test Loss: 0.6107659041881561, R2 Score (Test): 0.755133011931057, RMSE (Test): 0.7761865854263306\n",
      "Epoch [3636/5000], Train Loss: 0.006911395505691568, R2 Score (Train): 0.9945641440904499, RMSE (Train): 0.12383997588573496\n",
      "Test Loss: 0.5966919362545013, R2 Score (Test): 0.7583997798900434, RMSE (Test): 0.7709917426109314\n",
      "Epoch [3641/5000], Train Loss: 0.006756429715702931, R2 Score (Train): 0.9959786202321804, RMSE (Train): 0.10651589845632775\n",
      "Test Loss: 0.5708484798669815, R2 Score (Test): 0.7544302267195176, RMSE (Test): 0.777299702167511\n",
      "Epoch [3646/5000], Train Loss: 0.009425833045194546, R2 Score (Train): 0.9917456669720066, RMSE (Train): 0.15260469109461122\n",
      "Test Loss: 0.6385201513767242, R2 Score (Test): 0.7498055992767476, RMSE (Test): 0.7845847010612488\n",
      "Epoch [3651/5000], Train Loss: 0.00814633013214916, R2 Score (Train): 0.9933777247344523, RMSE (Train): 0.1366880365497516\n",
      "Test Loss: 0.6572098731994629, R2 Score (Test): 0.7474978487385152, RMSE (Test): 0.7881948351860046\n",
      "Epoch [3656/5000], Train Loss: 0.0076469286189725, R2 Score (Train): 0.9897130986492584, RMSE (Train): 0.17036060676375012\n",
      "Test Loss: 0.564677357673645, R2 Score (Test): 0.7684499335158055, RMSE (Test): 0.7547854781150818\n",
      "Epoch [3661/5000], Train Loss: 0.009697088156826794, R2 Score (Train): 0.988784468628438, RMSE (Train): 0.17788397183850208\n",
      "Test Loss: 0.6571917682886124, R2 Score (Test): 0.7572363724902129, RMSE (Test): 0.7728458046913147\n",
      "Epoch [3666/5000], Train Loss: 0.008465406989368299, R2 Score (Train): 0.9920528413547671, RMSE (Train): 0.14973827718160537\n",
      "Test Loss: 0.5439155101776123, R2 Score (Test): 0.7566035370689972, RMSE (Test): 0.7738524675369263\n",
      "Epoch [3671/5000], Train Loss: 0.006625413273771604, R2 Score (Train): 0.9938364951552062, RMSE (Train): 0.1318684075095184\n",
      "Test Loss: 0.5507164001464844, R2 Score (Test): 0.7611590477821462, RMSE (Test): 0.76657634973526\n",
      "Epoch [3676/5000], Train Loss: 0.006287221680395305, R2 Score (Train): 0.9925559467525701, RMSE (Train): 0.1449211014523992\n",
      "Test Loss: 0.6137445271015167, R2 Score (Test): 0.7484250947758453, RMSE (Test): 0.7867463231086731\n",
      "Epoch [3681/5000], Train Loss: 0.01197334355674684, R2 Score (Train): 0.9906411255290286, RMSE (Train): 0.16249451343448712\n",
      "Test Loss: 0.5058186799287796, R2 Score (Test): 0.7558711469947056, RMSE (Test): 0.7750158905982971\n",
      "Epoch [3686/5000], Train Loss: 0.0063256654927196605, R2 Score (Train): 0.9922404754557895, RMSE (Train): 0.14796004211843242\n",
      "Test Loss: 0.613814651966095, R2 Score (Test): 0.7569109837997461, RMSE (Test): 0.7733635902404785\n",
      "Epoch [3691/5000], Train Loss: 0.007043312594760209, R2 Score (Train): 0.9921533134223023, RMSE (Train): 0.14878873231492773\n",
      "Test Loss: 0.6488257497549057, R2 Score (Test): 0.7586409039353461, RMSE (Test): 0.7706068754196167\n",
      "Epoch [3696/5000], Train Loss: 0.006453810376115143, R2 Score (Train): 0.9942354190636715, RMSE (Train): 0.12752953062668074\n",
      "Test Loss: 0.5627270042896271, R2 Score (Test): 0.7604478086454207, RMSE (Test): 0.7677169442176819\n",
      "Epoch [3701/5000], Train Loss: 0.00799903970134134, R2 Score (Train): 0.9920874156265983, RMSE (Train): 0.1494122024691026\n",
      "Test Loss: 0.5793056190013885, R2 Score (Test): 0.7563814830096853, RMSE (Test): 0.7742053866386414\n",
      "Epoch [3706/5000], Train Loss: 0.0065006366348825395, R2 Score (Train): 0.993351082826127, RMSE (Train): 0.1369627136587918\n",
      "Test Loss: 0.5573357045650482, R2 Score (Test): 0.7569034542812679, RMSE (Test): 0.7733755707740784\n",
      "Epoch [3711/5000], Train Loss: 0.006538900857170423, R2 Score (Train): 0.9903069923431597, RMSE (Train): 0.16536978759305634\n",
      "Test Loss: 0.5754244923591614, R2 Score (Test): 0.7599032075214428, RMSE (Test): 0.7685891389846802\n",
      "Epoch [3716/5000], Train Loss: 0.008259328082203865, R2 Score (Train): 0.990759977432681, RMSE (Train): 0.16145942690485438\n",
      "Test Loss: 0.5713151395320892, R2 Score (Test): 0.7633324973439499, RMSE (Test): 0.7630805373191833\n",
      "Epoch [3721/5000], Train Loss: 0.006606732359311233, R2 Score (Train): 0.9931784919520552, RMSE (Train): 0.13872894660173066\n",
      "Test Loss: 0.5682706832885742, R2 Score (Test): 0.7674721003010692, RMSE (Test): 0.7563773989677429\n",
      "Epoch [3726/5000], Train Loss: 0.005817055333560954, R2 Score (Train): 0.9911299475332959, RMSE (Train): 0.15819399181405608\n",
      "Test Loss: 0.5822050869464874, R2 Score (Test): 0.7627642082120897, RMSE (Test): 0.7639961242675781\n",
      "Epoch [3731/5000], Train Loss: 0.007386301062069833, R2 Score (Train): 0.9933555884473992, RMSE (Train): 0.13691629958264476\n",
      "Test Loss: 0.6566676199436188, R2 Score (Test): 0.7670308015223777, RMSE (Test): 0.7570948600769043\n",
      "Epoch [3736/5000], Train Loss: 0.008312678042178353, R2 Score (Train): 0.9909106895201372, RMSE (Train): 0.16013724771596408\n",
      "Test Loss: 0.5660452246665955, R2 Score (Test): 0.759887230184443, RMSE (Test): 0.768614649772644\n",
      "Epoch [3741/5000], Train Loss: 0.008543455546411375, R2 Score (Train): 0.9955506863647717, RMSE (Train): 0.1120400776007196\n",
      "Test Loss: 0.5670094341039658, R2 Score (Test): 0.7579216876229208, RMSE (Test): 0.7717542052268982\n",
      "Epoch [3746/5000], Train Loss: 0.005054422013927251, R2 Score (Train): 0.9922329727118978, RMSE (Train): 0.1480315566863018\n",
      "Test Loss: 0.5370268076658249, R2 Score (Test): 0.7616874687567085, RMSE (Test): 0.7657279372215271\n",
      "Epoch [3751/5000], Train Loss: 0.009275817351105312, R2 Score (Train): 0.9935331434758767, RMSE (Train): 0.13507454195595897\n",
      "Test Loss: 0.5372330993413925, R2 Score (Test): 0.7613974490958296, RMSE (Test): 0.7661937475204468\n",
      "Epoch [3756/5000], Train Loss: 0.007154847340037425, R2 Score (Train): 0.9928485182220406, RMSE (Train): 0.14204465911379194\n",
      "Test Loss: 0.6429381370544434, R2 Score (Test): 0.7556758327431065, RMSE (Test): 0.7753258347511292\n",
      "Epoch [3761/5000], Train Loss: 0.006140144444846858, R2 Score (Train): 0.9947848530223161, RMSE (Train): 0.12129982295089459\n",
      "Test Loss: 0.6465269029140472, R2 Score (Test): 0.7559923455762695, RMSE (Test): 0.7748234868049622\n",
      "Epoch [3766/5000], Train Loss: 0.00549206678988412, R2 Score (Train): 0.993065168835652, RMSE (Train): 0.139876525748792\n",
      "Test Loss: 0.5378679186105728, R2 Score (Test): 0.765271384347333, RMSE (Test): 0.7599483728408813\n",
      "Epoch [3771/5000], Train Loss: 0.007148942386265844, R2 Score (Train): 0.9928796502090168, RMSE (Train): 0.14173514594227016\n",
      "Test Loss: 0.6148468554019928, R2 Score (Test): 0.7594980321967006, RMSE (Test): 0.7692373394966125\n",
      "Epoch [3776/5000], Train Loss: 0.011599287080268065, R2 Score (Train): 0.9893813836192267, RMSE (Train): 0.17308556749545617\n",
      "Test Loss: 0.6387643814086914, R2 Score (Test): 0.763221202927433, RMSE (Test): 0.7632598876953125\n",
      "Epoch [3781/5000], Train Loss: 0.006822744462018211, R2 Score (Train): 0.9965229070454504, RMSE (Train): 0.09904556669607906\n",
      "Test Loss: 0.5412243157625198, R2 Score (Test): 0.7591577209528376, RMSE (Test): 0.7697814106941223\n",
      "Epoch [3786/5000], Train Loss: 0.007373370967494945, R2 Score (Train): 0.9942438197252106, RMSE (Train): 0.1274365730440034\n",
      "Test Loss: 0.6407906711101532, R2 Score (Test): 0.7629309907616588, RMSE (Test): 0.7637274861335754\n",
      "Epoch [3791/5000], Train Loss: 0.006419978589595606, R2 Score (Train): 0.9946220462061227, RMSE (Train): 0.12317864543441069\n",
      "Test Loss: 0.582961916923523, R2 Score (Test): 0.7495799574985678, RMSE (Test): 0.784938395023346\n",
      "Epoch [3796/5000], Train Loss: 0.0048937490209937096, R2 Score (Train): 0.9940010578173997, RMSE (Train): 0.1300960853990883\n",
      "Test Loss: 0.5570328086614609, R2 Score (Test): 0.7584034965041124, RMSE (Test): 0.7709857821464539\n",
      "Epoch [3801/5000], Train Loss: 0.007894294220022857, R2 Score (Train): 0.9917042886204771, RMSE (Train): 0.15298671086156604\n",
      "Test Loss: 0.6168580651283264, R2 Score (Test): 0.7600261007783805, RMSE (Test): 0.7683923244476318\n",
      "Epoch [3806/5000], Train Loss: 0.008380028574417034, R2 Score (Train): 0.9913884303233833, RMSE (Train): 0.15587197943530673\n",
      "Test Loss: 0.5892291367053986, R2 Score (Test): 0.7583602089315615, RMSE (Test): 0.7710548639297485\n",
      "Epoch [3811/5000], Train Loss: 0.007226565309489767, R2 Score (Train): 0.991458749932775, RMSE (Train): 0.15523427198178455\n",
      "Test Loss: 0.6194023191928864, R2 Score (Test): 0.7560946507732541, RMSE (Test): 0.7746610641479492\n",
      "Epoch [3816/5000], Train Loss: 0.005702447844669223, R2 Score (Train): 0.991150846677716, RMSE (Train): 0.15800751778717273\n",
      "Test Loss: 0.6462135016918182, R2 Score (Test): 0.7565023991001228, RMSE (Test): 0.7740132212638855\n",
      "Epoch [3821/5000], Train Loss: 0.005494480875010292, R2 Score (Train): 0.9922275463454587, RMSE (Train): 0.1480832581410742\n",
      "Test Loss: 0.5590672641992569, R2 Score (Test): 0.7590272252745793, RMSE (Test): 0.7699899077415466\n",
      "Epoch [3826/5000], Train Loss: 0.00664681817094485, R2 Score (Train): 0.992008423458904, RMSE (Train): 0.15015614927469767\n",
      "Test Loss: 0.561457559466362, R2 Score (Test): 0.7575140276892817, RMSE (Test): 0.7724037170410156\n",
      "Epoch [3831/5000], Train Loss: 0.0070930968116347986, R2 Score (Train): 0.9909739780810345, RMSE (Train): 0.15957875871778074\n",
      "Test Loss: 0.6400804221630096, R2 Score (Test): 0.7527903532026612, RMSE (Test): 0.7798907160758972\n",
      "Epoch [3836/5000], Train Loss: 0.007279580769439538, R2 Score (Train): 0.9956993977926244, RMSE (Train): 0.11015178200085231\n",
      "Test Loss: 0.5516596883535385, R2 Score (Test): 0.7587525047039493, RMSE (Test): 0.7704287767410278\n",
      "Epoch [3841/5000], Train Loss: 0.007265967355730633, R2 Score (Train): 0.9915044575451075, RMSE (Train): 0.15481835476177358\n",
      "Test Loss: 0.6593998968601227, R2 Score (Test): 0.7450106121094893, RMSE (Test): 0.7920673489570618\n",
      "Epoch [3846/5000], Train Loss: 0.007861086633056402, R2 Score (Train): 0.9950718992632621, RMSE (Train): 0.11791435482459804\n",
      "Test Loss: 0.5784233212471008, R2 Score (Test): 0.7570939674355053, RMSE (Test): 0.7730724215507507\n",
      "Epoch [3851/5000], Train Loss: 0.006666152466398974, R2 Score (Train): 0.9934422231880606, RMSE (Train): 0.13602076296623483\n",
      "Test Loss: 0.5841875076293945, R2 Score (Test): 0.7563521721964519, RMSE (Test): 0.7742519974708557\n",
      "Epoch [3856/5000], Train Loss: 0.0069806855559969945, R2 Score (Train): 0.992110482245943, RMSE (Train): 0.14919426167692582\n",
      "Test Loss: 0.5901920795440674, R2 Score (Test): 0.7621352551860545, RMSE (Test): 0.7650082111358643\n",
      "Epoch [3861/5000], Train Loss: 0.007515314675401896, R2 Score (Train): 0.993452476478033, RMSE (Train): 0.13591438500154007\n",
      "Test Loss: 0.5533926784992218, R2 Score (Test): 0.7594289579963711, RMSE (Test): 0.7693478465080261\n",
      "Epoch [3866/5000], Train Loss: 0.005162228752548496, R2 Score (Train): 0.9937989291561655, RMSE (Train): 0.13226965998830206\n",
      "Test Loss: 0.561289831995964, R2 Score (Test): 0.7533498893848222, RMSE (Test): 0.779007613658905\n",
      "Epoch [3871/5000], Train Loss: 0.005490583988527457, R2 Score (Train): 0.9929895846702154, RMSE (Train): 0.1406367316074145\n",
      "Test Loss: 0.6066382527351379, R2 Score (Test): 0.7611256034409373, RMSE (Test): 0.7666301131248474\n",
      "Epoch [3876/5000], Train Loss: 0.004908551830643167, R2 Score (Train): 0.9942122553869185, RMSE (Train): 0.12778549816828935\n",
      "Test Loss: 0.6558584272861481, R2 Score (Test): 0.7562508061187971, RMSE (Test): 0.7744130492210388\n",
      "Epoch [3881/5000], Train Loss: 0.006328395800665021, R2 Score (Train): 0.9960226672375847, RMSE (Train): 0.10593094692483758\n",
      "Test Loss: 0.6693774163722992, R2 Score (Test): 0.745471555500455, RMSE (Test): 0.7913510799407959\n",
      "Epoch [3886/5000], Train Loss: 0.0058985925279557705, R2 Score (Train): 0.9925456643125723, RMSE (Train): 0.14502115637232926\n",
      "Test Loss: 0.5656248331069946, R2 Score (Test): 0.7584586012695975, RMSE (Test): 0.7708978652954102\n",
      "Epoch [3891/5000], Train Loss: 0.008003440084091077, R2 Score (Train): 0.9928619613179661, RMSE (Train): 0.14191109111743277\n",
      "Test Loss: 0.5843275189399719, R2 Score (Test): 0.7544581949870938, RMSE (Test): 0.7772553563117981\n",
      "Epoch [3896/5000], Train Loss: 0.008156242857997617, R2 Score (Train): 0.9925101452023793, RMSE (Train): 0.1453662509057116\n",
      "Test Loss: 0.5490207672119141, R2 Score (Test): 0.7538792490289249, RMSE (Test): 0.7781712412834167\n",
      "Epoch [3901/5000], Train Loss: 0.0068037197343073785, R2 Score (Train): 0.994568707230349, RMSE (Train): 0.1237879861174578\n",
      "Test Loss: 0.6364698112010956, R2 Score (Test): 0.7521152434071714, RMSE (Test): 0.780954897403717\n",
      "Epoch [3906/5000], Train Loss: 0.005764360966471334, R2 Score (Train): 0.991190238297411, RMSE (Train): 0.15765544375589977\n",
      "Test Loss: 0.5302103161811829, R2 Score (Test): 0.760490905424918, RMSE (Test): 0.7676478624343872\n",
      "Epoch [3911/5000], Train Loss: 0.009323884383775294, R2 Score (Train): 0.9926782208428713, RMSE (Train): 0.14372595510483727\n",
      "Test Loss: 0.5746557712554932, R2 Score (Test): 0.7542404411903626, RMSE (Test): 0.7775999903678894\n",
      "Epoch [3916/5000], Train Loss: 0.007851032811837891, R2 Score (Train): 0.9889583789529276, RMSE (Train): 0.1764994309918666\n",
      "Test Loss: 0.5778049230575562, R2 Score (Test): 0.7582125235355166, RMSE (Test): 0.7712904214859009\n",
      "Epoch [3921/5000], Train Loss: 0.009733603917993605, R2 Score (Train): 0.9916682683245173, RMSE (Train): 0.1533184881778826\n",
      "Test Loss: 0.6198448240756989, R2 Score (Test): 0.7552288994179427, RMSE (Test): 0.7760346531867981\n",
      "Epoch [3926/5000], Train Loss: 0.010990351322107017, R2 Score (Train): 0.9920995277713477, RMSE (Train): 0.14929780272202464\n",
      "Test Loss: 0.6050670146942139, R2 Score (Test): 0.7457470417498798, RMSE (Test): 0.7909227013587952\n",
      "Epoch [3931/5000], Train Loss: 0.00560104928445071, R2 Score (Train): 0.9894956541133931, RMSE (Train): 0.17215173243462328\n",
      "Test Loss: 0.6086559295654297, R2 Score (Test): 0.7534635842083726, RMSE (Test): 0.7788280844688416\n",
      "Epoch [3936/5000], Train Loss: 0.00789424175551782, R2 Score (Train): 0.9940644863500443, RMSE (Train): 0.1294064861497931\n",
      "Test Loss: 0.5593774616718292, R2 Score (Test): 0.7547482178147223, RMSE (Test): 0.776796281337738\n",
      "Epoch [3941/5000], Train Loss: 0.004892446061906715, R2 Score (Train): 0.9930299905769622, RMSE (Train): 0.1402308521974727\n",
      "Test Loss: 0.5635794997215271, R2 Score (Test): 0.7631424751790423, RMSE (Test): 0.7633867859840393\n",
      "Epoch [3946/5000], Train Loss: 0.006470432349791129, R2 Score (Train): 0.9939612560121998, RMSE (Train): 0.13052695290992672\n",
      "Test Loss: 0.6461749076843262, R2 Score (Test): 0.7459935936712997, RMSE (Test): 0.7905391454696655\n",
      "Epoch [3951/5000], Train Loss: 0.00758471693067501, R2 Score (Train): 0.9943536023936774, RMSE (Train): 0.12621547879884795\n",
      "Test Loss: 0.566435381770134, R2 Score (Test): 0.7508111537662588, RMSE (Test): 0.783006489276886\n",
      "Epoch [3956/5000], Train Loss: 0.005834471705990533, R2 Score (Train): 0.992887539383795, RMSE (Train): 0.14165660462967114\n",
      "Test Loss: 0.5990009009838104, R2 Score (Test): 0.7485062756652072, RMSE (Test): 0.7866193652153015\n",
      "Epoch [3961/5000], Train Loss: 0.006918720396546026, R2 Score (Train): 0.9908051979602541, RMSE (Train): 0.16106385235794368\n",
      "Test Loss: 0.6360558569431305, R2 Score (Test): 0.7595559881388789, RMSE (Test): 0.7691446542739868\n",
      "Epoch [3966/5000], Train Loss: 0.007244388844507436, R2 Score (Train): 0.994467154536291, RMSE (Train): 0.12493990195183857\n",
      "Test Loss: 0.5985217988491058, R2 Score (Test): 0.7505550001596454, RMSE (Test): 0.7834088206291199\n",
      "Epoch [3971/5000], Train Loss: 0.006411589914932847, R2 Score (Train): 0.9939375638956047, RMSE (Train): 0.1307827538287213\n",
      "Test Loss: 0.5687901228666306, R2 Score (Test): 0.7519165278005824, RMSE (Test): 0.7812678813934326\n",
      "Epoch [3976/5000], Train Loss: 0.007573351846076548, R2 Score (Train): 0.9932771832563255, RMSE (Train): 0.13772174830213\n",
      "Test Loss: 0.6137254238128662, R2 Score (Test): 0.7540434609294743, RMSE (Test): 0.7779115438461304\n",
      "Epoch [3981/5000], Train Loss: 0.005866889589621375, R2 Score (Train): 0.993571853123353, RMSE (Train): 0.1346696670428442\n",
      "Test Loss: 0.5536071509122849, R2 Score (Test): 0.7494811190906888, RMSE (Test): 0.7850933074951172\n",
      "Epoch [3986/5000], Train Loss: 0.006154266477096826, R2 Score (Train): 0.9903161313707214, RMSE (Train): 0.16529180996711818\n",
      "Test Loss: 0.577851802110672, R2 Score (Test): 0.7433949051466788, RMSE (Test): 0.7945727705955505\n",
      "Epoch [3991/5000], Train Loss: 0.005071933381259441, R2 Score (Train): 0.9932649403331272, RMSE (Train): 0.13784709381597673\n",
      "Test Loss: 0.5567798912525177, R2 Score (Test): 0.754122853896128, RMSE (Test): 0.7777860164642334\n",
      "Epoch [3996/5000], Train Loss: 0.0063239064572068555, R2 Score (Train): 0.9941115862900669, RMSE (Train): 0.1288920254171545\n",
      "Test Loss: 0.671230673789978, R2 Score (Test): 0.7484328696764813, RMSE (Test): 0.7867341637611389\n",
      "Epoch [4001/5000], Train Loss: 0.00647441454930231, R2 Score (Train): 0.9919584899736233, RMSE (Train): 0.15062452645836355\n",
      "Test Loss: 0.5946130156517029, R2 Score (Test): 0.7623218411192356, RMSE (Test): 0.7647080421447754\n",
      "Epoch [4006/5000], Train Loss: 0.006638527964241803, R2 Score (Train): 0.9923322339590527, RMSE (Train): 0.14708260641072102\n",
      "Test Loss: 0.5932266414165497, R2 Score (Test): 0.7516309769491655, RMSE (Test): 0.7817173600196838\n",
      "Epoch [4011/5000], Train Loss: 0.004414947625870506, R2 Score (Train): 0.9942550087724179, RMSE (Train): 0.1273126551538039\n",
      "Test Loss: 0.6026155650615692, R2 Score (Test): 0.74995258041091, RMSE (Test): 0.7843542098999023\n",
      "Epoch [4016/5000], Train Loss: 0.005814266856759787, R2 Score (Train): 0.9951485037009916, RMSE (Train): 0.1169943106647023\n",
      "Test Loss: 0.594221293926239, R2 Score (Test): 0.7553950346635656, RMSE (Test): 0.7757712602615356\n",
      "Epoch [4021/5000], Train Loss: 0.0055791080230847, R2 Score (Train): 0.994555808789114, RMSE (Train): 0.1239348871681597\n",
      "Test Loss: 0.6169626712799072, R2 Score (Test): 0.7555104016510655, RMSE (Test): 0.7755882740020752\n",
      "Epoch [4026/5000], Train Loss: 0.00931978850470235, R2 Score (Train): 0.9901555752205907, RMSE (Train): 0.1666564256643105\n",
      "Test Loss: 0.5533372461795807, R2 Score (Test): 0.7569235337172602, RMSE (Test): 0.773343563079834\n",
      "Epoch [4031/5000], Train Loss: 0.007924557430669665, R2 Score (Train): 0.988177808570437, RMSE (Train): 0.18263158336568341\n",
      "Test Loss: 0.5397989004850388, R2 Score (Test): 0.7646368176931663, RMSE (Test): 0.7609748840332031\n",
      "Epoch [4036/5000], Train Loss: 0.006943798177720358, R2 Score (Train): 0.9926298972144705, RMSE (Train): 0.14419946962520794\n",
      "Test Loss: 0.6100848913192749, R2 Score (Test): 0.7563895590328893, RMSE (Test): 0.7741925120353699\n",
      "Epoch [4041/5000], Train Loss: 0.004634657098601262, R2 Score (Train): 0.9941967812486104, RMSE (Train): 0.12795620806410377\n",
      "Test Loss: 0.6132632791996002, R2 Score (Test): 0.7586887374774725, RMSE (Test): 0.7705305218696594\n",
      "Epoch [4046/5000], Train Loss: 0.008098456504133841, R2 Score (Train): 0.9891274143265225, RMSE (Train): 0.17514321193163723\n",
      "Test Loss: 0.5603624880313873, R2 Score (Test): 0.7576641227090857, RMSE (Test): 0.7721646428108215\n",
      "Epoch [4051/5000], Train Loss: 0.006519546635293712, R2 Score (Train): 0.9912436240737953, RMSE (Train): 0.15717703423202764\n",
      "Test Loss: 0.5942263305187225, R2 Score (Test): 0.7503058739893921, RMSE (Test): 0.7837998867034912\n",
      "Epoch [4056/5000], Train Loss: 0.008425136872877678, R2 Score (Train): 0.9914077246208143, RMSE (Train): 0.15569726524937677\n",
      "Test Loss: 0.60040283203125, R2 Score (Test): 0.7631277875624846, RMSE (Test): 0.7634104490280151\n",
      "Epoch [4061/5000], Train Loss: 0.0076258534876008826, R2 Score (Train): 0.9912420986620815, RMSE (Train): 0.15719072421348815\n",
      "Test Loss: 0.6075047552585602, R2 Score (Test): 0.7518828539271394, RMSE (Test): 0.7813208699226379\n",
      "Epoch [4066/5000], Train Loss: 0.0075057233140493436, R2 Score (Train): 0.9913397479866021, RMSE (Train): 0.15631194092884687\n",
      "Test Loss: 0.634352833032608, R2 Score (Test): 0.7519155909420885, RMSE (Test): 0.7812693119049072\n",
      "Epoch [4071/5000], Train Loss: 0.008216969358424345, R2 Score (Train): 0.9940624606540359, RMSE (Train): 0.12942856644947986\n",
      "Test Loss: 0.6524743437767029, R2 Score (Test): 0.7547031002563487, RMSE (Test): 0.776867687702179\n",
      "Epoch [4076/5000], Train Loss: 0.0057465349479268, R2 Score (Train): 0.9946932490288863, RMSE (Train): 0.12236050048897859\n",
      "Test Loss: 0.5826270580291748, R2 Score (Test): 0.7568404320679664, RMSE (Test): 0.7734757661819458\n",
      "Epoch [4081/5000], Train Loss: 0.006080959225073457, R2 Score (Train): 0.9944539619463247, RMSE (Train): 0.1250887674435307\n",
      "Test Loss: 0.606298953294754, R2 Score (Test): 0.755080525623044, RMSE (Test): 0.776269793510437\n",
      "Epoch [4086/5000], Train Loss: 0.0066528913254539175, R2 Score (Train): 0.9915226819930095, RMSE (Train): 0.15465220917282269\n",
      "Test Loss: 0.5783304274082184, R2 Score (Test): 0.7581793132728577, RMSE (Test): 0.7713434100151062\n",
      "Epoch [4091/5000], Train Loss: 0.006927650460662941, R2 Score (Train): 0.9926364887431346, RMSE (Train): 0.14413497204753423\n",
      "Test Loss: 0.5263594686985016, R2 Score (Test): 0.7560565423236335, RMSE (Test): 0.7747215032577515\n",
      "Epoch [4096/5000], Train Loss: 0.0061612512993936734, R2 Score (Train): 0.9930620688038904, RMSE (Train): 0.13990778629525127\n",
      "Test Loss: 0.5668521076440811, R2 Score (Test): 0.7480388769850668, RMSE (Test): 0.7873499989509583\n",
      "Epoch [4101/5000], Train Loss: 0.008063610022266706, R2 Score (Train): 0.990971425148536, RMSE (Train): 0.15960132486042836\n",
      "Test Loss: 0.6040601134300232, R2 Score (Test): 0.7521821325413525, RMSE (Test): 0.7808495163917542\n",
      "Epoch [4106/5000], Train Loss: 0.007297964844231804, R2 Score (Train): 0.9937214726294533, RMSE (Train): 0.13309317534283527\n",
      "Test Loss: 0.5949409604072571, R2 Score (Test): 0.7600928869325746, RMSE (Test): 0.7682853937149048\n",
      "Epoch [4111/5000], Train Loss: 0.005550113584225376, R2 Score (Train): 0.992895513352086, RMSE (Train): 0.14157717487192026\n",
      "Test Loss: 0.6485328376293182, R2 Score (Test): 0.754482134824001, RMSE (Test): 0.7772175669670105\n",
      "Epoch [4116/5000], Train Loss: 0.009340919243792692, R2 Score (Train): 0.9909988443714901, RMSE (Train): 0.1593587909081286\n",
      "Test Loss: 0.5880712866783142, R2 Score (Test): 0.7599992783274288, RMSE (Test): 0.7684352993965149\n",
      "Epoch [4121/5000], Train Loss: 0.007383298361673951, R2 Score (Train): 0.9926347749345907, RMSE (Train): 0.14415174430241143\n",
      "Test Loss: 0.622134804725647, R2 Score (Test): 0.7611792793495792, RMSE (Test): 0.7665439248085022\n",
      "Epoch [4126/5000], Train Loss: 0.006298797476726274, R2 Score (Train): 0.9947086949684899, RMSE (Train): 0.12218229821753508\n",
      "Test Loss: 0.5763555467128754, R2 Score (Test): 0.755212578650999, RMSE (Test): 0.7760605216026306\n",
      "Epoch [4131/5000], Train Loss: 0.006744089187122881, R2 Score (Train): 0.9917057866024898, RMSE (Train): 0.15297289759848165\n",
      "Test Loss: 0.6160969436168671, R2 Score (Test): 0.7563753321357931, RMSE (Test): 0.7742152214050293\n",
      "Epoch [4136/5000], Train Loss: 0.005822330014780164, R2 Score (Train): 0.9916978492442243, RMSE (Train): 0.15304607574593374\n",
      "Test Loss: 0.5918204188346863, R2 Score (Test): 0.758251050531844, RMSE (Test): 0.7712289690971375\n",
      "Epoch [4141/5000], Train Loss: 0.007735802908428013, R2 Score (Train): 0.9939045561257007, RMSE (Train): 0.13113830290141187\n",
      "Test Loss: 0.5795596539974213, R2 Score (Test): 0.7588842437320611, RMSE (Test): 0.7702183723449707\n",
      "Epoch [4146/5000], Train Loss: 0.005087683307162176, R2 Score (Train): 0.9938498748497712, RMSE (Train): 0.13172520022607834\n",
      "Test Loss: 0.5428285747766495, R2 Score (Test): 0.7580751249269688, RMSE (Test): 0.7715095281600952\n",
      "Epoch [4151/5000], Train Loss: 0.0065737820890111225, R2 Score (Train): 0.9908084448039032, RMSE (Train): 0.1610354126309323\n",
      "Test Loss: 0.5853844881057739, R2 Score (Test): 0.7506815237687086, RMSE (Test): 0.7832100987434387\n",
      "Epoch [4156/5000], Train Loss: 0.006035784084815532, R2 Score (Train): 0.9926986827158103, RMSE (Train): 0.14352498210710185\n",
      "Test Loss: 0.56344835460186, R2 Score (Test): 0.7535908568333157, RMSE (Test): 0.7786269783973694\n",
      "Epoch [4161/5000], Train Loss: 0.007089085993357003, R2 Score (Train): 0.9901455984986018, RMSE (Train): 0.16674085232304114\n",
      "Test Loss: 0.6162735223770142, R2 Score (Test): 0.7496854900671408, RMSE (Test): 0.7847729921340942\n",
      "Epoch [4166/5000], Train Loss: 0.0055446012411266565, R2 Score (Train): 0.9944099066207427, RMSE (Train): 0.12558461021187722\n",
      "Test Loss: 0.6474015712738037, R2 Score (Test): 0.7534337528309611, RMSE (Test): 0.7788751721382141\n",
      "Epoch [4171/5000], Train Loss: 0.0070732511424769955, R2 Score (Train): 0.9932483174183085, RMSE (Train): 0.13801710035492704\n",
      "Test Loss: 0.6005873382091522, R2 Score (Test): 0.75889690606406, RMSE (Test): 0.7701981067657471\n",
      "Epoch [4176/5000], Train Loss: 0.007790604218219717, R2 Score (Train): 0.9933004213326382, RMSE (Train): 0.13748351788298177\n",
      "Test Loss: 0.5967415869235992, R2 Score (Test): 0.7628055694797911, RMSE (Test): 0.763929545879364\n",
      "Epoch [4181/5000], Train Loss: 0.008047873309502998, R2 Score (Train): 0.992438191284208, RMSE (Train): 0.14606283785075963\n",
      "Test Loss: 0.5886210203170776, R2 Score (Test): 0.7626151414200097, RMSE (Test): 0.7642361521720886\n",
      "Epoch [4186/5000], Train Loss: 0.005373640878436466, R2 Score (Train): 0.9928886916913453, RMSE (Train): 0.14164512909428262\n",
      "Test Loss: 0.5518563538789749, R2 Score (Test): 0.7584741862284136, RMSE (Test): 0.770872950553894\n",
      "Epoch [4191/5000], Train Loss: 0.006231852535468836, R2 Score (Train): 0.9932079102182763, RMSE (Train): 0.13842948388386275\n",
      "Test Loss: 0.5731639862060547, R2 Score (Test): 0.7557567293195524, RMSE (Test): 0.775197446346283\n",
      "Epoch [4196/5000], Train Loss: 0.0054000346378112836, R2 Score (Train): 0.9934046782957249, RMSE (Train): 0.13640958347299403\n",
      "Test Loss: 0.6257404983043671, R2 Score (Test): 0.7632816401086261, RMSE (Test): 0.7631624937057495\n",
      "Epoch [4201/5000], Train Loss: 0.0050487332822134095, R2 Score (Train): 0.9920749091497609, RMSE (Train): 0.14953023485700662\n",
      "Test Loss: 0.603620707988739, R2 Score (Test): 0.7629583199886087, RMSE (Test): 0.7636834383010864\n",
      "Epoch [4206/5000], Train Loss: 0.0051629125373438, R2 Score (Train): 0.9924087752697188, RMSE (Train): 0.14634665990385085\n",
      "Test Loss: 0.6631196141242981, R2 Score (Test): 0.7595595306954581, RMSE (Test): 0.7691389918327332\n",
      "Epoch [4211/5000], Train Loss: 0.005693847740379472, R2 Score (Train): 0.9925899719768715, RMSE (Train): 0.14458951996400168\n",
      "Test Loss: 0.6144030094146729, R2 Score (Test): 0.7524123944131547, RMSE (Test): 0.7804867029190063\n",
      "Epoch [4216/5000], Train Loss: 0.0048562886465030415, R2 Score (Train): 0.9941606849972962, RMSE (Train): 0.12835353752087664\n",
      "Test Loss: 0.5764470398426056, R2 Score (Test): 0.7507446409726662, RMSE (Test): 0.7831109762191772\n",
      "Epoch [4221/5000], Train Loss: 0.004929154839677115, R2 Score (Train): 0.9949561576127198, RMSE (Train): 0.11929099037770012\n",
      "Test Loss: 0.5914795994758606, R2 Score (Test): 0.7596682924600971, RMSE (Test): 0.7689650058746338\n",
      "Epoch [4226/5000], Train Loss: 0.0057571837096475065, R2 Score (Train): 0.9930395668134951, RMSE (Train): 0.1401344860881\n",
      "Test Loss: 0.6181827187538147, R2 Score (Test): 0.7567277267527267, RMSE (Test): 0.7736549973487854\n",
      "Epoch [4231/5000], Train Loss: 0.007089389177660148, R2 Score (Train): 0.9940646004561966, RMSE (Train): 0.12940524226863476\n",
      "Test Loss: 0.5875540673732758, R2 Score (Test): 0.7580747112307953, RMSE (Test): 0.7715101838111877\n",
      "Epoch [4236/5000], Train Loss: 0.00683815290297692, R2 Score (Train): 0.9923754548839657, RMSE (Train): 0.14666749007912894\n",
      "Test Loss: 0.5660068243741989, R2 Score (Test): 0.7583111020542919, RMSE (Test): 0.7711331844329834\n",
      "Epoch [4241/5000], Train Loss: 0.00740390884069105, R2 Score (Train): 0.9906362658963591, RMSE (Train): 0.16253669591947217\n",
      "Test Loss: 0.5719462037086487, R2 Score (Test): 0.7582457543027545, RMSE (Test): 0.7712373733520508\n",
      "Epoch [4246/5000], Train Loss: 0.011898364871740341, R2 Score (Train): 0.9966618678135989, RMSE (Train): 0.0970462270951078\n",
      "Test Loss: 0.5947130024433136, R2 Score (Test): 0.755700864209553, RMSE (Test): 0.7752861380577087\n",
      "Epoch [4251/5000], Train Loss: 0.0075034585121708615, R2 Score (Train): 0.9892014240947764, RMSE (Train): 0.1745460935025224\n",
      "Test Loss: 0.5970445275306702, R2 Score (Test): 0.7629867727518338, RMSE (Test): 0.7636376619338989\n",
      "Epoch [4256/5000], Train Loss: 0.007087735342793167, R2 Score (Train): 0.9920171509181922, RMSE (Train): 0.15007413544070136\n",
      "Test Loss: 0.5478608310222626, R2 Score (Test): 0.7634508238835268, RMSE (Test): 0.7628897428512573\n",
      "Epoch [4261/5000], Train Loss: 0.006019120492661993, R2 Score (Train): 0.990628662597584, RMSE (Train): 0.16260267196494882\n",
      "Test Loss: 0.6023501753807068, R2 Score (Test): 0.7577016451523034, RMSE (Test): 0.7721048593521118\n",
      "Epoch [4266/5000], Train Loss: 0.005384848394896835, R2 Score (Train): 0.9920614308561352, RMSE (Train): 0.1496573347394244\n",
      "Test Loss: 0.494023397564888, R2 Score (Test): 0.7698704311145718, RMSE (Test): 0.75246661901474\n",
      "Epoch [4271/5000], Train Loss: 0.006513893449058135, R2 Score (Train): 0.9946991825177441, RMSE (Train): 0.1222920755971588\n",
      "Test Loss: 0.6217932999134064, R2 Score (Test): 0.7557067993593534, RMSE (Test): 0.7752767205238342\n",
      "Epoch [4276/5000], Train Loss: 0.005762832549711068, R2 Score (Train): 0.994265403486022, RMSE (Train): 0.12719742627940156\n",
      "Test Loss: 0.5693329274654388, R2 Score (Test): 0.7672380619348242, RMSE (Test): 0.7567580342292786\n",
      "Epoch [4281/5000], Train Loss: 0.006319130188785493, R2 Score (Train): 0.9928017131096345, RMSE (Train): 0.14250872892021896\n",
      "Test Loss: 0.5420668721199036, R2 Score (Test): 0.7548172220614072, RMSE (Test): 0.7766869068145752\n",
      "Epoch [4286/5000], Train Loss: 0.006947635653583954, R2 Score (Train): 0.9923861734786941, RMSE (Train): 0.14656436115438545\n",
      "Test Loss: 0.5893992185592651, R2 Score (Test): 0.7550714117526713, RMSE (Test): 0.7762842774391174\n",
      "Epoch [4291/5000], Train Loss: 0.008349461286949614, R2 Score (Train): 0.9940047401115399, RMSE (Train): 0.13005615122616845\n",
      "Test Loss: 0.6398622989654541, R2 Score (Test): 0.7596412290293344, RMSE (Test): 0.7690082788467407\n",
      "Epoch [4296/5000], Train Loss: 0.006714265948782365, R2 Score (Train): 0.9931484263272317, RMSE (Train): 0.13903433264449935\n",
      "Test Loss: 0.6556543409824371, R2 Score (Test): 0.7535842282208421, RMSE (Test): 0.7786374688148499\n",
      "Epoch [4301/5000], Train Loss: 0.006062134207847218, R2 Score (Train): 0.9946958411209658, RMSE (Train): 0.12233061323537782\n",
      "Test Loss: 0.6103272438049316, R2 Score (Test): 0.7572885384819595, RMSE (Test): 0.7727627158164978\n",
      "Epoch [4306/5000], Train Loss: 0.007009074421754728, R2 Score (Train): 0.990950810663493, RMSE (Train): 0.15978342576174887\n",
      "Test Loss: 0.6421588957309723, R2 Score (Test): 0.7529623286919113, RMSE (Test): 0.7796193957328796\n",
      "Epoch [4311/5000], Train Loss: 0.0053518373557987315, R2 Score (Train): 0.9923680457328315, RMSE (Train): 0.146738734836451\n",
      "Test Loss: 0.6264003217220306, R2 Score (Test): 0.7594779566055359, RMSE (Test): 0.7692694664001465\n",
      "Epoch [4316/5000], Train Loss: 0.005837713630171493, R2 Score (Train): 0.9924055148341133, RMSE (Train): 0.14637808451938888\n",
      "Test Loss: 0.5834676623344421, R2 Score (Test): 0.7500619017911241, RMSE (Test): 0.7841827273368835\n",
      "Epoch [4321/5000], Train Loss: 0.008479898368629316, R2 Score (Train): 0.9923779818605549, RMSE (Train): 0.1466431833157717\n",
      "Test Loss: 0.6443895101547241, R2 Score (Test): 0.7528607605528547, RMSE (Test): 0.7797796726226807\n",
      "Epoch [4326/5000], Train Loss: 0.006688143650535494, R2 Score (Train): 0.9908596543347173, RMSE (Train): 0.1605861923415068\n",
      "Test Loss: 0.5483955144882202, R2 Score (Test): 0.7582293069642978, RMSE (Test): 0.7712637186050415\n",
      "Epoch [4331/5000], Train Loss: 0.00855157058686018, R2 Score (Train): 0.9932389802383165, RMSE (Train): 0.13811250214954907\n",
      "Test Loss: 0.6180979907512665, R2 Score (Test): 0.749808713687335, RMSE (Test): 0.7845797538757324\n",
      "Epoch [4336/5000], Train Loss: 0.00540815342295294, R2 Score (Train): 0.9937032146064746, RMSE (Train): 0.13328655302878364\n",
      "Test Loss: 0.5955065488815308, R2 Score (Test): 0.7521476024044234, RMSE (Test): 0.7809039354324341\n",
      "Epoch [4341/5000], Train Loss: 0.005916416315206637, R2 Score (Train): 0.9926450330647262, RMSE (Train): 0.14405132356963607\n",
      "Test Loss: 0.5466144531965256, R2 Score (Test): 0.7592620402173894, RMSE (Test): 0.7696146368980408\n",
      "Epoch [4346/5000], Train Loss: 0.008368407026864588, R2 Score (Train): 0.9932515431275868, RMSE (Train): 0.13798412663141704\n",
      "Test Loss: 0.5721797496080399, R2 Score (Test): 0.7545427312188369, RMSE (Test): 0.7771216034889221\n",
      "Epoch [4351/5000], Train Loss: 0.006213820481207222, R2 Score (Train): 0.9954794544320428, RMSE (Train): 0.11293337739002036\n",
      "Test Loss: 0.5800631642341614, R2 Score (Test): 0.7530756399985012, RMSE (Test): 0.7794405817985535\n",
      "Epoch [4356/5000], Train Loss: 0.008293024768742422, R2 Score (Train): 0.9929704020291685, RMSE (Train): 0.14082901272370071\n",
      "Test Loss: 0.5642693936824799, R2 Score (Test): 0.7567339247189995, RMSE (Test): 0.7736451029777527\n",
      "Epoch [4361/5000], Train Loss: 0.006608510952598105, R2 Score (Train): 0.9914920915142128, RMSE (Train): 0.15493098985229012\n",
      "Test Loss: 0.6535303890705109, R2 Score (Test): 0.7586834475552438, RMSE (Test): 0.7705389261245728\n",
      "Epoch [4366/5000], Train Loss: 0.008022982510738075, R2 Score (Train): 0.9914710740265619, RMSE (Train): 0.1551222384786868\n",
      "Test Loss: 0.6482461094856262, R2 Score (Test): 0.7458896742756467, RMSE (Test): 0.7907007932662964\n",
      "Epoch [4371/5000], Train Loss: 0.006447546067647636, R2 Score (Train): 0.9919188759527405, RMSE (Train): 0.15099507332754603\n",
      "Test Loss: 0.6256940066814423, R2 Score (Test): 0.7542806702371938, RMSE (Test): 0.7775363326072693\n",
      "Epoch [4376/5000], Train Loss: 0.0048130186429868145, R2 Score (Train): 0.9956881894319228, RMSE (Train): 0.11029522906535363\n",
      "Test Loss: 0.5985982716083527, R2 Score (Test): 0.7559891273010333, RMSE (Test): 0.7748285531997681\n",
      "Epoch [4381/5000], Train Loss: 0.006045485846698284, R2 Score (Train): 0.9907631706009509, RMSE (Train): 0.16143152590865836\n",
      "Test Loss: 0.5985115766525269, R2 Score (Test): 0.7633195383462436, RMSE (Test): 0.7631013989448547\n",
      "Epoch [4386/5000], Train Loss: 0.005556854923876624, R2 Score (Train): 0.993961554158246, RMSE (Train): 0.1305237306690609\n",
      "Test Loss: 0.6583749949932098, R2 Score (Test): 0.7553664023113204, RMSE (Test): 0.7758166790008545\n",
      "Epoch [4391/5000], Train Loss: 0.008046805936222276, R2 Score (Train): 0.9963683462714914, RMSE (Train): 0.10122297725615588\n",
      "Test Loss: 0.6307208240032196, R2 Score (Test): 0.7584340976729838, RMSE (Test): 0.7709369659423828\n",
      "Epoch [4396/5000], Train Loss: 0.0056516754169327514, R2 Score (Train): 0.9901695209604063, RMSE (Train): 0.16653834000208034\n",
      "Test Loss: 0.5691705495119095, R2 Score (Test): 0.752454930377007, RMSE (Test): 0.780419647693634\n",
      "Epoch [4401/5000], Train Loss: 0.006014747583928208, R2 Score (Train): 0.9944039875682071, RMSE (Train): 0.1256510800530677\n",
      "Test Loss: 0.6204577684402466, R2 Score (Test): 0.7511779545331534, RMSE (Test): 0.7824299931526184\n",
      "Epoch [4406/5000], Train Loss: 0.005322637560311705, R2 Score (Train): 0.9942417012283852, RMSE (Train): 0.1274600216789406\n",
      "Test Loss: 0.6081016659736633, R2 Score (Test): 0.7536064009752462, RMSE (Test): 0.7786024808883667\n",
      "Epoch [4411/5000], Train Loss: 0.006667219063577552, R2 Score (Train): 0.9891701277047178, RMSE (Train): 0.17479884490380354\n",
      "Test Loss: 0.5745769441127777, R2 Score (Test): 0.7644087644990643, RMSE (Test): 0.7613434791564941\n",
      "Epoch [4416/5000], Train Loss: 0.0060133352332438035, R2 Score (Train): 0.9926564300938043, RMSE (Train): 0.1439396715639114\n",
      "Test Loss: 0.589380294084549, R2 Score (Test): 0.7595085614220842, RMSE (Test): 0.7692205309867859\n",
      "Epoch [4421/5000], Train Loss: 0.007018869548725585, R2 Score (Train): 0.9945459886050717, RMSE (Train): 0.1240466131359393\n",
      "Test Loss: 0.5634004473686218, R2 Score (Test): 0.7586249108870327, RMSE (Test): 0.7706323862075806\n",
      "Epoch [4426/5000], Train Loss: 0.006538889041015257, R2 Score (Train): 0.9907871261929387, RMSE (Train): 0.1612220547669415\n",
      "Test Loss: 0.6097245514392853, R2 Score (Test): 0.755480659199839, RMSE (Test): 0.7756354212760925\n",
      "Epoch [4431/5000], Train Loss: 0.004261050043472399, R2 Score (Train): 0.9895742575479181, RMSE (Train): 0.17150642205524358\n",
      "Test Loss: 0.5715213716030121, R2 Score (Test): 0.7570804673691002, RMSE (Test): 0.7730939388275146\n",
      "Epoch [4436/5000], Train Loss: 0.004742914228700101, R2 Score (Train): 0.9931461256271892, RMSE (Train): 0.13905767395715776\n",
      "Test Loss: 0.5391577780246735, R2 Score (Test): 0.7561872068612623, RMSE (Test): 0.7745140194892883\n",
      "Epoch [4441/5000], Train Loss: 0.005011964763980359, R2 Score (Train): 0.9925924794835203, RMSE (Train): 0.14456505380418092\n",
      "Test Loss: 0.5247002840042114, R2 Score (Test): 0.7679470569630875, RMSE (Test): 0.7556045651435852\n",
      "Epoch [4446/5000], Train Loss: 0.004674594209063798, R2 Score (Train): 0.993140178299041, RMSE (Train): 0.13911799329210783\n",
      "Test Loss: 0.6373175084590912, R2 Score (Test): 0.7555494942819903, RMSE (Test): 0.7755262851715088\n",
      "Epoch [4451/5000], Train Loss: 0.004777413152623922, R2 Score (Train): 0.9950395570464265, RMSE (Train): 0.11830064715865857\n",
      "Test Loss: 0.6158522367477417, R2 Score (Test): 0.7471033479265852, RMSE (Test): 0.7888103127479553\n",
      "Epoch [4456/5000], Train Loss: 0.006663083486879866, R2 Score (Train): 0.9932985348755478, RMSE (Train): 0.13750287271969933\n",
      "Test Loss: 0.5879743993282318, R2 Score (Test): 0.7552584837042722, RMSE (Test): 0.7759877443313599\n",
      "Epoch [4461/5000], Train Loss: 0.00834722036961466, R2 Score (Train): 0.9919728248286994, RMSE (Train): 0.15049021437940785\n",
      "Test Loss: 0.6599413156509399, R2 Score (Test): 0.7462140160260593, RMSE (Test): 0.7901960611343384\n",
      "Epoch [4466/5000], Train Loss: 0.006568163536333789, R2 Score (Train): 0.9935736490090126, RMSE (Train): 0.13465085382786812\n",
      "Test Loss: 0.6919286847114563, R2 Score (Test): 0.7438606522730349, RMSE (Test): 0.793851375579834\n",
      "Epoch [4471/5000], Train Loss: 0.006108999853798498, R2 Score (Train): 0.9902024761983207, RMSE (Train): 0.16625895798979473\n",
      "Test Loss: 0.5421739369630814, R2 Score (Test): 0.7573401177189513, RMSE (Test): 0.7726806402206421\n",
      "Epoch [4476/5000], Train Loss: 0.005485092134525378, R2 Score (Train): 0.9932455435440244, RMSE (Train): 0.13804544904927038\n",
      "Test Loss: 0.5953299701213837, R2 Score (Test): 0.7547313749495856, RMSE (Test): 0.7768229246139526\n",
      "Epoch [4481/5000], Train Loss: 0.008464839231843749, R2 Score (Train): 0.9893586684817843, RMSE (Train): 0.17327059923245655\n",
      "Test Loss: 0.5294619649648666, R2 Score (Test): 0.7558116365781354, RMSE (Test): 0.7751103639602661\n",
      "Epoch [4486/5000], Train Loss: 0.007023796516781052, R2 Score (Train): 0.9910238203394376, RMSE (Train): 0.1591375468240318\n",
      "Test Loss: 0.5866824388504028, R2 Score (Test): 0.757807609625059, RMSE (Test): 0.7719359993934631\n",
      "Epoch [4491/5000], Train Loss: 0.006600890036982794, R2 Score (Train): 0.9942280664983365, RMSE (Train): 0.1276108349239142\n",
      "Test Loss: 0.6241589784622192, R2 Score (Test): 0.7491448160966977, RMSE (Test): 0.7856200933456421\n",
      "Epoch [4496/5000], Train Loss: 0.004995180856591712, R2 Score (Train): 0.9946670304842887, RMSE (Train): 0.12266239533113654\n",
      "Test Loss: 0.5904293358325958, R2 Score (Test): 0.748920359788469, RMSE (Test): 0.785971462726593\n",
      "Epoch [4501/5000], Train Loss: 0.007358799688518047, R2 Score (Train): 0.9910527737822082, RMSE (Train): 0.15888068357205715\n",
      "Test Loss: 0.6241061091423035, R2 Score (Test): 0.7588317527928854, RMSE (Test): 0.7703021168708801\n",
      "Epoch [4506/5000], Train Loss: 0.006821706231373052, R2 Score (Train): 0.9950667930748047, RMSE (Train): 0.1179754267332039\n",
      "Test Loss: 0.607387125492096, R2 Score (Test): 0.755946396244514, RMSE (Test): 0.7748964428901672\n",
      "Epoch [4511/5000], Train Loss: 0.00627851296061029, R2 Score (Train): 0.9926724275699564, RMSE (Train): 0.14378280460993845\n",
      "Test Loss: 0.6269712746143341, R2 Score (Test): 0.7613656735936535, RMSE (Test): 0.7662447094917297\n",
      "Epoch [4516/5000], Train Loss: 0.006459235795773566, R2 Score (Train): 0.9948486604615879, RMSE (Train): 0.12055548618380448\n",
      "Test Loss: 0.5920254290103912, R2 Score (Test): 0.7562703910619635, RMSE (Test): 0.7743819355964661\n",
      "Epoch [4521/5000], Train Loss: 0.007862218655645847, R2 Score (Train): 0.990639564816429, RMSE (Train): 0.16250806189392414\n",
      "Test Loss: 0.5746268033981323, R2 Score (Test): 0.752509332949536, RMSE (Test): 0.7803338766098022\n",
      "Epoch [4526/5000], Train Loss: 0.006400101410690695, R2 Score (Train): 0.992045252863709, RMSE (Train): 0.14980975030214483\n",
      "Test Loss: 0.5621354877948761, R2 Score (Test): 0.7604981259802103, RMSE (Test): 0.7676362991333008\n",
      "Epoch [4531/5000], Train Loss: 0.00782573886681348, R2 Score (Train): 0.9906346630865488, RMSE (Train): 0.16255060619610914\n",
      "Test Loss: 0.5818490535020828, R2 Score (Test): 0.7492500458543381, RMSE (Test): 0.7854553461074829\n",
      "Epoch [4536/5000], Train Loss: 0.006219964338621746, R2 Score (Train): 0.9941227436631301, RMSE (Train): 0.12876985513869094\n",
      "Test Loss: 0.5953412652015686, R2 Score (Test): 0.7573934256554045, RMSE (Test): 0.7725957632064819\n",
      "Epoch [4541/5000], Train Loss: 0.00668180244974792, R2 Score (Train): 0.9913709048656951, RMSE (Train): 0.1560305068267342\n",
      "Test Loss: 0.6358296275138855, R2 Score (Test): 0.7509731299929151, RMSE (Test): 0.7827519178390503\n",
      "Epoch [4546/5000], Train Loss: 0.005551116812663774, R2 Score (Train): 0.9933044385491657, RMSE (Train): 0.13744229261303825\n",
      "Test Loss: 0.578250914812088, R2 Score (Test): 0.7618191130417342, RMSE (Test): 0.7655164003372192\n",
      "Epoch [4551/5000], Train Loss: 0.007754517835564911, R2 Score (Train): 0.9916009693101434, RMSE (Train): 0.1539364527954506\n",
      "Test Loss: 0.55195252597332, R2 Score (Test): 0.7597029750124306, RMSE (Test): 0.7689095139503479\n",
      "Epoch [4556/5000], Train Loss: 0.0069542940861235065, R2 Score (Train): 0.9944851765875482, RMSE (Train): 0.12473625358417059\n",
      "Test Loss: 0.6253704726696014, R2 Score (Test): 0.7599604706441341, RMSE (Test): 0.7684974670410156\n",
      "Epoch [4561/5000], Train Loss: 0.0071487446160366135, R2 Score (Train): 0.9935516576851648, RMSE (Train): 0.1348810483470961\n",
      "Test Loss: 0.6026128232479095, R2 Score (Test): 0.7578748628261696, RMSE (Test): 0.7718287706375122\n",
      "Epoch [4566/5000], Train Loss: 0.007636840416428943, R2 Score (Train): 0.9944272134722519, RMSE (Train): 0.12539005540537657\n",
      "Test Loss: 0.6133707165718079, R2 Score (Test): 0.7490509664268534, RMSE (Test): 0.7857670783996582\n",
      "Epoch [4571/5000], Train Loss: 0.006336758960969746, R2 Score (Train): 0.9947313393096584, RMSE (Train): 0.1219205760081993\n",
      "Test Loss: 0.5873350203037262, R2 Score (Test): 0.7542502157482109, RMSE (Test): 0.7775845527648926\n",
      "Epoch [4576/5000], Train Loss: 0.006234767225881417, R2 Score (Train): 0.99440956258437, RMSE (Train): 0.12558847463829204\n",
      "Test Loss: 0.5847789645195007, R2 Score (Test): 0.7544595223135991, RMSE (Test): 0.7772533893585205\n",
      "Epoch [4581/5000], Train Loss: 0.006228401248032848, R2 Score (Train): 0.9937443495814153, RMSE (Train): 0.13285047951603515\n",
      "Test Loss: 0.5975806415081024, R2 Score (Test): 0.7501283174161697, RMSE (Test): 0.7840785384178162\n",
      "Epoch [4586/5000], Train Loss: 0.004982649368078758, R2 Score (Train): 0.9935421349812691, RMSE (Train): 0.1349806055952727\n",
      "Test Loss: 0.5880696177482605, R2 Score (Test): 0.7577796387713716, RMSE (Test): 0.7719805836677551\n",
      "Epoch [4591/5000], Train Loss: 0.006318921339698136, R2 Score (Train): 0.9934686422906448, RMSE (Train): 0.13574649523774593\n",
      "Test Loss: 0.5685741603374481, R2 Score (Test): 0.762505598378965, RMSE (Test): 0.7644124031066895\n",
      "Epoch [4596/5000], Train Loss: 0.007753398773881297, R2 Score (Train): 0.9929974387315524, RMSE (Train): 0.14055792892514954\n",
      "Test Loss: 0.6182754337787628, R2 Score (Test): 0.7573744764550719, RMSE (Test): 0.7726259827613831\n",
      "Epoch [4601/5000], Train Loss: 0.007565441696594159, R2 Score (Train): 0.9893952829167526, RMSE (Train): 0.17297224973868588\n",
      "Test Loss: 0.6024382412433624, R2 Score (Test): 0.7623170397185169, RMSE (Test): 0.7647157907485962\n",
      "Epoch [4606/5000], Train Loss: 0.004838258939950417, R2 Score (Train): 0.9946435802505549, RMSE (Train): 0.12293178620779316\n",
      "Test Loss: 0.5687488615512848, R2 Score (Test): 0.7562604438546835, RMSE (Test): 0.7743977308273315\n",
      "Epoch [4611/5000], Train Loss: 0.006945739539029698, R2 Score (Train): 0.9952229447433648, RMSE (Train): 0.11609326437332397\n",
      "Test Loss: 0.5795199275016785, R2 Score (Test): 0.756768933962469, RMSE (Test): 0.773589551448822\n",
      "Epoch [4616/5000], Train Loss: 0.005733200639951974, R2 Score (Train): 0.9932011968620851, RMSE (Train): 0.13849787939522346\n",
      "Test Loss: 0.6218493282794952, R2 Score (Test): 0.7532337145703749, RMSE (Test): 0.7791910767555237\n",
      "Epoch [4621/5000], Train Loss: 0.004878753835024933, R2 Score (Train): 0.9909401126686765, RMSE (Train): 0.15987784622812198\n",
      "Test Loss: 0.534226268529892, R2 Score (Test): 0.7600855690708046, RMSE (Test): 0.7682971358299255\n",
      "Epoch [4626/5000], Train Loss: 0.0061386352754198015, R2 Score (Train): 0.992857533068833, RMSE (Train): 0.1419551032236307\n",
      "Test Loss: 0.6408566236495972, R2 Score (Test): 0.7537392375940283, RMSE (Test): 0.7783925533294678\n",
      "Epoch [4631/5000], Train Loss: 0.008630729435632626, R2 Score (Train): 0.9932929430738353, RMSE (Train): 0.1375602279740136\n",
      "Test Loss: 0.5721435695886612, R2 Score (Test): 0.7530586107924336, RMSE (Test): 0.7794674634933472\n",
      "Epoch [4636/5000], Train Loss: 0.0069119207716236515, R2 Score (Train): 0.9887035720070101, RMSE (Train): 0.17852434973001627\n",
      "Test Loss: 0.5684533566236496, R2 Score (Test): 0.7517509165473582, RMSE (Test): 0.7815285921096802\n",
      "Epoch [4641/5000], Train Loss: 0.005956932456077387, R2 Score (Train): 0.9930581420785168, RMSE (Train): 0.13994737314949557\n",
      "Test Loss: 0.6235267817974091, R2 Score (Test): 0.7538144431415442, RMSE (Test): 0.7782737016677856\n",
      "Epoch [4646/5000], Train Loss: 0.006471321200175832, R2 Score (Train): 0.9941696506580263, RMSE (Train): 0.12825496291637467\n",
      "Test Loss: 0.6336959600448608, R2 Score (Test): 0.7511947901453577, RMSE (Test): 0.7824035286903381\n",
      "Epoch [4651/5000], Train Loss: 0.004943590902257711, R2 Score (Train): 0.9900157535408423, RMSE (Train): 0.16783577455086873\n",
      "Test Loss: 0.6265958249568939, R2 Score (Test): 0.7534364464727681, RMSE (Test): 0.7788709402084351\n",
      "Epoch [4656/5000], Train Loss: 0.005271591420751065, R2 Score (Train): 0.9947803117870532, RMSE (Train): 0.12135262406759154\n",
      "Test Loss: 0.6239532828330994, R2 Score (Test): 0.7573253661547054, RMSE (Test): 0.7727041244506836\n",
      "Epoch [4661/5000], Train Loss: 0.0046593722072429955, R2 Score (Train): 0.9955009079926941, RMSE (Train): 0.11266507963494878\n",
      "Test Loss: 0.552630826830864, R2 Score (Test): 0.7563963920527299, RMSE (Test): 0.7741817235946655\n",
      "Epoch [4666/5000], Train Loss: 0.007950699034457406, R2 Score (Train): 0.9932299089275377, RMSE (Train): 0.1382051243805048\n",
      "Test Loss: 0.5953185856342316, R2 Score (Test): 0.7534777478216148, RMSE (Test): 0.7788057327270508\n",
      "Epoch [4671/5000], Train Loss: 0.005347520501042406, R2 Score (Train): 0.9941246358991387, RMSE (Train): 0.1287491241582583\n",
      "Test Loss: 0.5987994968891144, R2 Score (Test): 0.7619987419327782, RMSE (Test): 0.7652276754379272\n",
      "Epoch [4676/5000], Train Loss: 0.004586558633794387, R2 Score (Train): 0.9946562722311019, RMSE (Train): 0.12278605704012788\n",
      "Test Loss: 0.5825841426849365, R2 Score (Test): 0.7569775291337942, RMSE (Test): 0.7732577323913574\n",
      "Epoch [4681/5000], Train Loss: 0.005121071958759178, R2 Score (Train): 0.9926991385684609, RMSE (Train): 0.14352050159610485\n",
      "Test Loss: 0.5348168760538101, R2 Score (Test): 0.75625178777181, RMSE (Test): 0.7744114398956299\n",
      "Epoch [4686/5000], Train Loss: 0.0053313600753123564, R2 Score (Train): 0.9912663910802743, RMSE (Train): 0.1569725672440628\n",
      "Test Loss: 0.6469082534313202, R2 Score (Test): 0.7560550754172434, RMSE (Test): 0.7747238874435425\n",
      "Epoch [4691/5000], Train Loss: 0.00968519365414977, R2 Score (Train): 0.9903903442568185, RMSE (Train): 0.16465723018179737\n",
      "Test Loss: 0.626026451587677, R2 Score (Test): 0.7594411137755204, RMSE (Test): 0.7693283557891846\n",
      "Epoch [4696/5000], Train Loss: 0.006119997104785095, R2 Score (Train): 0.9937489676886653, RMSE (Train): 0.1328014333743364\n",
      "Test Loss: 0.577001690864563, R2 Score (Test): 0.7645684260763782, RMSE (Test): 0.7610853910446167\n",
      "Epoch [4701/5000], Train Loss: 0.0062854897502499325, R2 Score (Train): 0.9924422555750428, RMSE (Train): 0.1460235799387966\n",
      "Test Loss: 0.6528841853141785, R2 Score (Test): 0.7530892346558975, RMSE (Test): 0.7794191241264343\n",
      "Epoch [4706/5000], Train Loss: 0.004854941643619289, R2 Score (Train): 0.9946807230696706, RMSE (Train): 0.12250482413293746\n",
      "Test Loss: 0.5961742103099823, R2 Score (Test): 0.7576520771397962, RMSE (Test): 0.7721838355064392\n",
      "Epoch [4711/5000], Train Loss: 0.0041135846404358745, R2 Score (Train): 0.9937325356068466, RMSE (Train): 0.13297586630818672\n",
      "Test Loss: 0.5948894321918488, R2 Score (Test): 0.7578197223228406, RMSE (Test): 0.7719166874885559\n",
      "Epoch [4716/5000], Train Loss: 0.005639936910786976, R2 Score (Train): 0.9926726295585543, RMSE (Train): 0.14378082286990682\n",
      "Test Loss: 0.5929363667964935, R2 Score (Test): 0.753556339346689, RMSE (Test): 0.7786815166473389\n",
      "Epoch [4721/5000], Train Loss: 0.003843916095017145, R2 Score (Train): 0.994832178717956, RMSE (Train): 0.12074819118925717\n",
      "Test Loss: 0.6391317248344421, R2 Score (Test): 0.7571631590012885, RMSE (Test): 0.7729623913764954\n",
      "Epoch [4726/5000], Train Loss: 0.005878916068468243, R2 Score (Train): 0.9930812865004149, RMSE (Train): 0.13971388340274007\n",
      "Test Loss: 0.6499283164739609, R2 Score (Test): 0.762279445452408, RMSE (Test): 0.7647762894630432\n",
      "Epoch [4731/5000], Train Loss: 0.006800048518925905, R2 Score (Train): 0.9944922318454706, RMSE (Train): 0.12465643887186918\n",
      "Test Loss: 0.6160100102424622, R2 Score (Test): 0.7580185107111841, RMSE (Test): 0.7715998888015747\n",
      "Epoch [4736/5000], Train Loss: 0.0062486117240041494, R2 Score (Train): 0.992947359708683, RMSE (Train): 0.14105963561201268\n",
      "Test Loss: 0.5432810038328171, R2 Score (Test): 0.7603267276030277, RMSE (Test): 0.767910897731781\n",
      "Epoch [4741/5000], Train Loss: 0.00491153885377571, R2 Score (Train): 0.992928361831993, RMSE (Train): 0.1412494958058751\n",
      "Test Loss: 0.569488525390625, R2 Score (Test): 0.7643791258325833, RMSE (Test): 0.7613913416862488\n",
      "Epoch [4746/5000], Train Loss: 0.006364107267775883, R2 Score (Train): 0.9936780626712463, RMSE (Train): 0.133552488222693\n",
      "Test Loss: 0.6002549231052399, R2 Score (Test): 0.7559685655844298, RMSE (Test): 0.7748612761497498\n",
      "Epoch [4751/5000], Train Loss: 0.005465622777895381, R2 Score (Train): 0.9922599027120524, RMSE (Train): 0.14777470479108418\n",
      "Test Loss: 0.5895534157752991, R2 Score (Test): 0.7573635080192365, RMSE (Test): 0.7726433873176575\n",
      "Epoch [4756/5000], Train Loss: 0.006530888029374182, R2 Score (Train): 0.9929532261526913, RMSE (Train): 0.14100095612633537\n",
      "Test Loss: 0.5581998974084854, R2 Score (Test): 0.7564625827934541, RMSE (Test): 0.774076521396637\n",
      "Epoch [4761/5000], Train Loss: 0.006931208442741384, R2 Score (Train): 0.991603966403617, RMSE (Train): 0.1539089851554167\n",
      "Test Loss: 0.645329475402832, R2 Score (Test): 0.7630149337719535, RMSE (Test): 0.7635922431945801\n",
      "Epoch [4766/5000], Train Loss: 0.005271335248835385, R2 Score (Train): 0.992627229133854, RMSE (Train): 0.14422556837900877\n",
      "Test Loss: 0.5767693221569061, R2 Score (Test): 0.7570731317191746, RMSE (Test): 0.7731056213378906\n",
      "Epoch [4771/5000], Train Loss: 0.00543950276914984, R2 Score (Train): 0.9911160094387527, RMSE (Train): 0.15831823331754777\n",
      "Test Loss: 0.6095646321773529, R2 Score (Test): 0.7589263590327753, RMSE (Test): 0.7701510190963745\n",
      "Epoch [4776/5000], Train Loss: 0.0052604151812071604, R2 Score (Train): 0.9945257161998236, RMSE (Train): 0.1242769381731807\n",
      "Test Loss: 0.5555250644683838, R2 Score (Test): 0.7576436484592706, RMSE (Test): 0.7721972465515137\n",
      "Epoch [4781/5000], Train Loss: 0.005049556862407674, R2 Score (Train): 0.9938584843786684, RMSE (Train): 0.13163296721694748\n",
      "Test Loss: 0.6022393703460693, R2 Score (Test): 0.7604066472751759, RMSE (Test): 0.7677828669548035\n",
      "Epoch [4786/5000], Train Loss: 0.005924945949421574, R2 Score (Train): 0.9934580137189952, RMSE (Train): 0.13585690144449195\n",
      "Test Loss: 0.6012214422225952, R2 Score (Test): 0.7567110087559149, RMSE (Test): 0.773681640625\n",
      "Epoch [4791/5000], Train Loss: 0.007055260085811217, R2 Score (Train): 0.9955037094505846, RMSE (Train): 0.11262999748681106\n",
      "Test Loss: 0.590697318315506, R2 Score (Test): 0.7534909098030931, RMSE (Test): 0.7787848711013794\n",
      "Epoch [4796/5000], Train Loss: 0.00442807498620823, R2 Score (Train): 0.9926567224792634, RMSE (Train): 0.14393680604438822\n",
      "Test Loss: 0.5568254142999649, R2 Score (Test): 0.7633987759340088, RMSE (Test): 0.7629736661911011\n",
      "Epoch [4801/5000], Train Loss: 0.0062448402944331365, R2 Score (Train): 0.9935116847154396, RMSE (Train): 0.13529846308989454\n",
      "Test Loss: 0.5918352901935577, R2 Score (Test): 0.7549497562551731, RMSE (Test): 0.776477038860321\n",
      "Epoch [4806/5000], Train Loss: 0.0073850594538574415, R2 Score (Train): 0.9930297259673178, RMSE (Train): 0.14023351403632517\n",
      "Test Loss: 0.6366589963436127, R2 Score (Test): 0.74571820642365, RMSE (Test): 0.7909675240516663\n",
      "Epoch [4811/5000], Train Loss: 0.005617515145180126, R2 Score (Train): 0.9932826192749349, RMSE (Train): 0.13766605666153872\n",
      "Test Loss: 0.6176451444625854, R2 Score (Test): 0.7536639064673905, RMSE (Test): 0.7785115242004395\n",
      "Epoch [4816/5000], Train Loss: 0.006574524915777147, R2 Score (Train): 0.9927778231031426, RMSE (Train): 0.14274501530385317\n",
      "Test Loss: 0.6035984456539154, R2 Score (Test): 0.7523071261345176, RMSE (Test): 0.780652642250061\n",
      "Epoch [4821/5000], Train Loss: 0.008778949695018431, R2 Score (Train): 0.9928021263438186, RMSE (Train): 0.14250463834114\n",
      "Test Loss: 0.6492465138435364, R2 Score (Test): 0.7537954879510076, RMSE (Test): 0.7783036828041077\n",
      "Epoch [4826/5000], Train Loss: 0.0051174806818986935, R2 Score (Train): 0.9939500909253819, RMSE (Train): 0.13064756340127112\n",
      "Test Loss: 0.596884697675705, R2 Score (Test): 0.7559404517353135, RMSE (Test): 0.7749058604240417\n",
      "Epoch [4831/5000], Train Loss: 0.006196603567029039, R2 Score (Train): 0.9895027652163118, RMSE (Train): 0.17209345198792186\n",
      "Test Loss: 0.5951134264469147, R2 Score (Test): 0.7556897721250317, RMSE (Test): 0.7753037214279175\n",
      "Epoch [4836/5000], Train Loss: 0.005186189354086916, R2 Score (Train): 0.9934605289724426, RMSE (Train): 0.13583078190091336\n",
      "Test Loss: 0.6188929080963135, R2 Score (Test): 0.752398168196155, RMSE (Test): 0.7805091142654419\n",
      "Epoch [4841/5000], Train Loss: 0.006176219438202679, R2 Score (Train): 0.9929866119173072, RMSE (Train): 0.14066654681245574\n",
      "Test Loss: 0.5740143209695816, R2 Score (Test): 0.7535306708911143, RMSE (Test): 0.7787220478057861\n",
      "Epoch [4846/5000], Train Loss: 0.007969039375893772, R2 Score (Train): 0.9920156688702789, RMSE (Train): 0.1500880657264169\n",
      "Test Loss: 0.526232123374939, R2 Score (Test): 0.7631354480851684, RMSE (Test): 0.7633980512619019\n",
      "Epoch [4851/5000], Train Loss: 0.007214525132440031, R2 Score (Train): 0.9951127438066907, RMSE (Train): 0.11742469572447645\n",
      "Test Loss: 0.5728852897882462, R2 Score (Test): 0.7545644177609648, RMSE (Test): 0.7770873308181763\n",
      "Epoch [4856/5000], Train Loss: 0.007101905493376155, R2 Score (Train): 0.9920699773159485, RMSE (Train): 0.14957675442294244\n",
      "Test Loss: 0.6021097898483276, R2 Score (Test): 0.7549020593881721, RMSE (Test): 0.7765526175498962\n",
      "Epoch [4861/5000], Train Loss: 0.005414949303182463, R2 Score (Train): 0.9939709952652211, RMSE (Train): 0.13042165386123147\n",
      "Test Loss: 0.5718564391136169, R2 Score (Test): 0.7519578642845383, RMSE (Test): 0.7812027335166931\n",
      "Epoch [4866/5000], Train Loss: 0.005196371135146667, R2 Score (Train): 0.9938576664856652, RMSE (Train): 0.13164173199956103\n",
      "Test Loss: 0.6256369650363922, R2 Score (Test): 0.7497208387368797, RMSE (Test): 0.7847176194190979\n",
      "Epoch [4871/5000], Train Loss: 0.007721142998586099, R2 Score (Train): 0.9961932153902809, RMSE (Train): 0.1036349024877493\n",
      "Test Loss: 0.5565268099308014, R2 Score (Test): 0.7563893697852664, RMSE (Test): 0.7741929292678833\n",
      "Epoch [4876/5000], Train Loss: 0.005355674889869988, R2 Score (Train): 0.9961135673745011, RMSE (Train): 0.10471344849355424\n",
      "Test Loss: 0.6926751136779785, R2 Score (Test): 0.7466011403849437, RMSE (Test): 0.7895931005477905\n",
      "Epoch [4881/5000], Train Loss: 0.004609040877160926, R2 Score (Train): 0.9948852431376412, RMSE (Train): 0.12012665596219325\n",
      "Test Loss: 0.6322641372680664, R2 Score (Test): 0.752809716587868, RMSE (Test): 0.7798601388931274\n",
      "Epoch [4886/5000], Train Loss: 0.005764769099187106, R2 Score (Train): 0.9940253777091795, RMSE (Train): 0.12983211086846377\n",
      "Test Loss: 0.6704505085945129, R2 Score (Test): 0.747209876595939, RMSE (Test): 0.7886441946029663\n",
      "Epoch [4891/5000], Train Loss: 0.004938658191046367, R2 Score (Train): 0.9949771846141051, RMSE (Train): 0.11904207781237168\n",
      "Test Loss: 0.6195421516895294, R2 Score (Test): 0.7526987036778074, RMSE (Test): 0.7800352573394775\n",
      "Epoch [4896/5000], Train Loss: 0.016131700288193922, R2 Score (Train): 0.9954961453853389, RMSE (Train): 0.11272469584298858\n",
      "Test Loss: 0.6524641513824463, R2 Score (Test): 0.7359941876553373, RMSE (Test): 0.8059494495391846\n",
      "Epoch [4901/5000], Train Loss: 0.004998913733288646, R2 Score (Train): 0.993448670972375, RMSE (Train): 0.13595387686644503\n",
      "Test Loss: 0.6087454855442047, R2 Score (Test): 0.7545368189228814, RMSE (Test): 0.7771309614181519\n",
      "Epoch [4906/5000], Train Loss: 0.004998359984407823, R2 Score (Train): 0.992106778451755, RMSE (Train): 0.1492292777588926\n",
      "Test Loss: 0.5736941993236542, R2 Score (Test): 0.7519413140006044, RMSE (Test): 0.7812288403511047\n",
      "Epoch [4911/5000], Train Loss: 0.004037361436833938, R2 Score (Train): 0.9928677883463921, RMSE (Train): 0.14185315581975969\n",
      "Test Loss: 0.6184671223163605, R2 Score (Test): 0.7538166847916102, RMSE (Test): 0.7782701253890991\n",
      "Epoch [4916/5000], Train Loss: 0.006178854149766266, R2 Score (Train): 0.988210774009895, RMSE (Train): 0.18237677725507206\n",
      "Test Loss: 0.5916736423969269, R2 Score (Test): 0.7572266167919683, RMSE (Test): 0.7728613018989563\n",
      "Epoch [4921/5000], Train Loss: 0.005564861426440378, R2 Score (Train): 0.9934825170802191, RMSE (Train): 0.13560223308917949\n",
      "Test Loss: 0.625952810049057, R2 Score (Test): 0.7506435412809022, RMSE (Test): 0.7832697629928589\n",
      "Epoch [4926/5000], Train Loss: 0.006339642025219898, R2 Score (Train): 0.9937194990401325, RMSE (Train): 0.1331140919220973\n",
      "Test Loss: 0.5742011070251465, R2 Score (Test): 0.7587931083048931, RMSE (Test): 0.7703638076782227\n",
      "Epoch [4931/5000], Train Loss: 0.0067179475057249265, R2 Score (Train): 0.9907496679996028, RMSE (Train): 0.16154947490537985\n",
      "Test Loss: 0.6251173615455627, R2 Score (Test): 0.757959370922824, RMSE (Test): 0.7716941237449646\n",
      "Epoch [4936/5000], Train Loss: 0.006618346514490743, R2 Score (Train): 0.9891612164004169, RMSE (Train): 0.17487074627781302\n",
      "Test Loss: 0.5600135773420334, R2 Score (Test): 0.7595456939064283, RMSE (Test): 0.7691611051559448\n",
      "Epoch [4941/5000], Train Loss: 0.004894991036659728, R2 Score (Train): 0.9946253529899144, RMSE (Train): 0.12314076971096091\n",
      "Test Loss: 0.5911589860916138, R2 Score (Test): 0.7588959261530426, RMSE (Test): 0.7701996564865112\n",
      "Epoch [4946/5000], Train Loss: 0.006899024922555934, R2 Score (Train): 0.9918129518319153, RMSE (Train): 0.15198144293430985\n",
      "Test Loss: 0.6187498271465302, R2 Score (Test): 0.7533105142331358, RMSE (Test): 0.7790697813034058\n",
      "Epoch [4951/5000], Train Loss: 0.0066602275473997, R2 Score (Train): 0.9905590373994162, RMSE (Train): 0.16320558980244582\n",
      "Test Loss: 0.6481897234916687, R2 Score (Test): 0.749573886921031, RMSE (Test): 0.78494793176651\n",
      "Epoch [4956/5000], Train Loss: 0.007870736493108174, R2 Score (Train): 0.9883281235957341, RMSE (Train): 0.18146682081131585\n",
      "Test Loss: 0.6009369194507599, R2 Score (Test): 0.7638219760664777, RMSE (Test): 0.7622910141944885\n",
      "Epoch [4961/5000], Train Loss: 0.0062864570451589925, R2 Score (Train): 0.9916257784082593, RMSE (Train): 0.15370893553552342\n",
      "Test Loss: 0.5469212085008621, R2 Score (Test): 0.766346722203004, RMSE (Test): 0.7582055926322937\n",
      "Epoch [4966/5000], Train Loss: 0.005100720057574411, R2 Score (Train): 0.9933933869204676, RMSE (Train): 0.13652630204942476\n",
      "Test Loss: 0.568602591753006, R2 Score (Test): 0.7565806319350417, RMSE (Test): 0.7738888263702393\n",
      "Epoch [4971/5000], Train Loss: 0.009418464168750992, R2 Score (Train): 0.9890333215567555, RMSE (Train): 0.17589943540376396\n",
      "Test Loss: 0.6245409250259399, R2 Score (Test): 0.7631360951912174, RMSE (Test): 0.7633970379829407\n",
      "Epoch [4976/5000], Train Loss: 0.006533351241766165, R2 Score (Train): 0.9883248102437775, RMSE (Train): 0.1814925759152993\n",
      "Test Loss: 0.5743427574634552, R2 Score (Test): 0.7624406438439296, RMSE (Test): 0.7645169496536255\n",
      "Epoch [4981/5000], Train Loss: 0.005154885797916601, R2 Score (Train): 0.9956721366901403, RMSE (Train): 0.1105003512462691\n",
      "Test Loss: 0.6239407956600189, R2 Score (Test): 0.7510948288409176, RMSE (Test): 0.7825606465339661\n",
      "Epoch [4986/5000], Train Loss: 0.0052385947201400995, R2 Score (Train): 0.9932782485747267, RMSE (Train): 0.13771083596202774\n",
      "Test Loss: 0.6281408667564392, R2 Score (Test): 0.7634858457720091, RMSE (Test): 0.762833297252655\n",
      "Epoch [4991/5000], Train Loss: 0.007226652931421995, R2 Score (Train): 0.994594121110769, RMSE (Train): 0.12349803472602326\n",
      "Test Loss: 0.5388844162225723, R2 Score (Test): 0.7585152801446625, RMSE (Test): 0.7708073854446411\n",
      "Epoch [4996/5000], Train Loss: 0.006652672891505063, R2 Score (Train): 0.9925548756366211, RMSE (Train): 0.14493152733964668\n",
      "Test Loss: 0.5762761533260345, R2 Score (Test): 0.7644850162837944, RMSE (Test): 0.7612202167510986\n",
      "=========================================\n",
      "average R2 on training dataset:0.9923775748224993, average R2 on testing dataset:0.7550701433994962\n",
      "average rmse on training dataset:0.14552565161730846, average rmse on testing dataset:0.7762269973754883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device', device)\n",
    "model = prediction_model(x_data.shape[1], 1024, 512, 1)\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-5)\n",
    "\n",
    "model = model.to(device)\n",
    "loss = loss_func.to(device)\n",
    "epochs = 5000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 64)\n",
    "\n",
    "\n",
    "final_R2_test, final_R2_train, final_rmse_test, final_rmse_train = train(model, x_data, y_data, optimizer, loss, epochs, device)\n",
    "\n",
    "# for plot\n",
    "final_R2_train = final_R2_train[5:]\n",
    "final_R2_test = final_R2_test[5:]\n",
    "final_rmse_train = final_rmse_train[5:]\n",
    "final_rmse_test = final_rmse_test[5:]\n",
    "\n",
    "# for calculation\n",
    "print('=========================================')\n",
    "final_R2_train_sub = final_R2_train[200:]\n",
    "final_R2_test_sub = final_R2_test[200:]\n",
    "final_rmse_train_sub = final_rmse_train[200:]\n",
    "final_rmse_test_sub = final_rmse_test[200:]\n",
    "print(f'average R2 on training dataset:{np.mean(final_R2_train_sub)}, average R2 on testing dataset:{np.mean(final_R2_test_sub)}')\n",
    "print(f'average rmse on training dataset:{np.mean(final_rmse_train_sub)}, average rmse on testing dataset:{np.mean(final_rmse_test_sub)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(995)]\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, final_R2_train, label = 'r2_train')\n",
    "plt.plot(x, final_R2_test, label = 'r2_test')\n",
    "plt.legend()\n",
    "plt.title('R2 score')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('R2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, final_rmse_train, label = 'rmse_train')\n",
    "plt.plot(x, final_rmse_test, label = 'rmse_test')\n",
    "plt.legend()\n",
    "plt.title('RMSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'ANN_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Drug_Discovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
