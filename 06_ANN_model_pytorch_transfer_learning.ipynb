{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden1_size)\n",
    "        # self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.batch1 = nn.BatchNorm1d(hidden1_size)\n",
    "\n",
    "        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        # self.dropout2 = nn.Dropout(p=0.01)\n",
    "        self.batch2 = nn.BatchNorm1d(hidden2_size)\n",
    "\n",
    "        # self.hidden3 = nn.Linear(hidden2_size, hidden3_size)\n",
    "        # self.batch3 = nn.BatchNorm1d(hidden3_size)\n",
    "\n",
    "        self.predict = nn.Linear(hidden2_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        result = self.hidden1(input)\n",
    "        # result = self.dropout1(result)\n",
    "        result = self.batch1(result)\n",
    "        result = F.leaky_relu(result)\n",
    "\n",
    "        result = self.hidden2(result)\n",
    "        # result = self.dropout2(result)\n",
    "        result = self.batch2(result)\n",
    "        result = F.leaky_relu(result)\n",
    "\n",
    "        # result = self.hidden3(result)\n",
    "        # result = self.batch3(result)\n",
    "        # result = F.leaky_relu(result)\n",
    "\n",
    "        result = self.predict(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_data, y_data, optimizer, loss, epochs, device):\n",
    "\n",
    "    final_R2_train = []\n",
    "    final_R2_test = []\n",
    "    final_rmse_train = []\n",
    "    final_rmse_test = []\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 64)\n",
    "\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1) # convert 1D to 2D, from (N,) to (N,1)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "    test_load = DataLoader(test_dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        mse_loss = []\n",
    "        # define batch: batch_sizee = 100\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()   # initialize gradient, gradient computed from batch 1 does not interfere batch2 and model update\n",
    "\n",
    "            output = model(batch_x)\n",
    "            l = loss(output, batch_y)\n",
    "\n",
    "            mse_loss.append(l.item())   # l.item() 返回tensor中的每一个值，节约内存\n",
    "\n",
    "            l.backward()     # back propagation\n",
    "            optimizer.step()    # update model\n",
    "        \n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                train_pred = model(x_train_tensor.to(device)).cpu().numpy().flatten()\n",
    "                r2_train = r2_score(y_train, train_pred)\n",
    "                rmse_train = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "\n",
    "                final_R2_train.append(r2_train)\n",
    "                final_rmse_train.append(rmse_train)\n",
    "\n",
    "                # final_R2_test.append()\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {np.mean(mse_loss)}, R2 Score (Train): {r2_train}, RMSE (Train): {rmse_train}\")\n",
    "\n",
    "                test_loss = []\n",
    "                test_preds_list = []\n",
    "                test_target_list = []\n",
    "                for batch_x, batch_y in test_load:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    test_output = model(batch_x)\n",
    "                    l = loss(test_output, batch_y)\n",
    "                    test_loss.append(l.item())\n",
    "\n",
    "                    test_preds_list.append(test_output.cpu().numpy())\n",
    "                    test_target_list.append(batch_y.cpu().numpy())\n",
    "                    \n",
    "                test_preds = np.concatenate(test_preds_list).flatten()\n",
    "                test_target = np.concatenate(test_target_list).flatten()\n",
    "\n",
    "                r2_test = r2_score(test_target, test_preds)\n",
    "                rmse_test = np.sqrt(mean_squared_error(test_target, test_preds))\n",
    "\n",
    "                final_R2_test.append(r2_test)\n",
    "                final_rmse_test.append(rmse_test)\n",
    "\n",
    "                print(f\"Test Loss: {np.mean(test_loss)}, R2 Score (Test): {r2_test}, RMSE (Test): {rmse_test}\")\n",
    "\n",
    "    return final_R2_test, final_R2_train, final_rmse_test, final_rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 1.  , 0.  , ..., 0.  , 0.  , 3.22],\n",
       "       [0.  , 1.  , 0.  , ..., 0.  , 0.  , 3.56],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 2.46],\n",
       "       ...,\n",
       "       [0.  , 1.  , 0.  , ..., 0.  , 0.  , 2.68],\n",
       "       [0.  , 1.  , 0.  , ..., 0.  , 0.  , 2.32],\n",
       "       [0.  , 1.  , 0.  , ..., 0.  , 0.  , 2.32]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = pd.read_csv('./data/x_value_train_with_ALogP.csv')\n",
    "x_data = x_data.values\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.read_csv('./data/y_value_train_with_ALogP.csv')\n",
    "y_data = y_data.values\n",
    "# y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Epoch [1/5000], Train Loss: 47.70114008585612, R2 Score (Train): -16.037393177334916, RMSE (Train): 6.933115952804351\n",
      "Test Loss: 45.754600524902344, R2 Score (Test): -17.43677610507256, RMSE (Test): 6.7350921630859375\n",
      "Epoch [6/5000], Train Loss: 33.52093760172526, R2 Score (Train): -10.247355194568785, RMSE (Train): 5.633160118600557\n",
      "Test Loss: 32.73237991333008, R2 Score (Test): -12.297561361974212, RMSE (Test): 5.719881534576416\n",
      "Epoch [11/5000], Train Loss: 25.673515637715656, R2 Score (Train): -7.41373217929725, RMSE (Train): 4.8721565289276825\n",
      "Test Loss: 25.436023712158203, R2 Score (Test): -9.338565923703461, RMSE (Test): 5.043490409851074\n",
      "Epoch [16/5000], Train Loss: 19.4802143573761, R2 Score (Train): -5.359817592097046, RMSE (Train): 4.235934348864079\n",
      "Test Loss: 20.32651138305664, R2 Score (Test): -7.1910759785909395, RMSE (Test): 4.489226818084717\n",
      "Epoch [21/5000], Train Loss: 14.72818374633789, R2 Score (Train): -3.7835761283878417, RMSE (Train): 3.6736961713629404\n",
      "Test Loss: 16.220832347869873, R2 Score (Test): -5.539309680766197, RMSE (Test): 4.01113224029541\n",
      "Epoch [26/5000], Train Loss: 10.901440779368082, R2 Score (Train): -2.4315606593026287, RMSE (Train): 3.1115209597249476\n",
      "Test Loss: 12.556291103363037, R2 Score (Test): -4.047108281279546, RMSE (Test): 3.5238897800445557\n",
      "Epoch [31/5000], Train Loss: 7.629840175310771, R2 Score (Train): -1.418043227821085, RMSE (Train): 2.611914286982529\n",
      "Test Loss: 9.714557647705078, R2 Score (Test): -2.9116187235655406, RMSE (Test): 3.1022677421569824\n",
      "Epoch [36/5000], Train Loss: 5.1387433608373, R2 Score (Train): -0.6814414231800925, RMSE (Train): 2.178049581149698\n",
      "Test Loss: 7.550700902938843, R2 Score (Test): -2.071551079006926, RMSE (Test): 2.749032974243164\n",
      "Epoch [41/5000], Train Loss: 3.5829933087031045, R2 Score (Train): -0.1384955538778163, RMSE (Train): 1.7922251199724983\n",
      "Test Loss: 5.756721019744873, R2 Score (Test): -1.4017727197632261, RMSE (Test): 2.4308993816375732\n",
      "Epoch [46/5000], Train Loss: 2.3130493760108948, R2 Score (Train): 0.36906833995704147, RMSE (Train): 1.3341911362200098\n",
      "Test Loss: 4.345295190811157, R2 Score (Test): -0.7370696398508123, RMSE (Test): 2.0673294067382812\n",
      "Epoch [51/5000], Train Loss: 1.5012625207503636, R2 Score (Train): 0.5650713935873717, RMSE (Train): 1.107734998551554\n",
      "Test Loss: 3.582211494445801, R2 Score (Test): -0.45188624309670344, RMSE (Test): 1.890023946762085\n",
      "Epoch [56/5000], Train Loss: 0.9504607692360878, R2 Score (Train): 0.7252944825804042, RMSE (Train): 0.8803605677047245\n",
      "Test Loss: 2.9526411294937134, R2 Score (Test): -0.22067298301218785, RMSE (Test): 1.7330085039138794\n",
      "Epoch [61/5000], Train Loss: 0.6771488835414251, R2 Score (Train): 0.8556423027750548, RMSE (Train): 0.6381853985827483\n",
      "Test Loss: 2.4513832330703735, R2 Score (Test): 0.06124352941952238, RMSE (Test): 1.5197688341140747\n",
      "Epoch [66/5000], Train Loss: 0.40154190299411613, R2 Score (Train): 0.9582314931372873, RMSE (Train): 0.3432822640881586\n",
      "Test Loss: 1.7891375422477722, R2 Score (Test): 0.28616955785207354, RMSE (Test): 1.3252524137496948\n",
      "Epoch [71/5000], Train Loss: 0.23405497210721174, R2 Score (Train): 0.9536538154193315, RMSE (Train): 0.3616045480574107\n",
      "Test Loss: 1.6962815523147583, R2 Score (Test): 0.2748088447810124, RMSE (Test): 1.335756540298462\n",
      "Epoch [76/5000], Train Loss: 0.22804297196368375, R2 Score (Train): 0.9721929265226474, RMSE (Train): 0.28009446898143453\n",
      "Test Loss: 1.5151761174201965, R2 Score (Test): 0.36542251552599636, RMSE (Test): 1.2495205402374268\n",
      "Epoch [81/5000], Train Loss: 0.14462341740727425, R2 Score (Train): 0.9861592774183612, RMSE (Train): 0.1976087962474754\n",
      "Test Loss: 1.4918738007545471, R2 Score (Test): 0.43463457478625245, RMSE (Test): 1.1794122457504272\n",
      "Epoch [86/5000], Train Loss: 0.12092940943936507, R2 Score (Train): 0.9907120740215276, RMSE (Train): 0.16187741607339448\n",
      "Test Loss: 1.414308786392212, R2 Score (Test): 0.4254091251618052, RMSE (Test): 1.1889961957931519\n",
      "Epoch [91/5000], Train Loss: 0.15373542066663504, R2 Score (Train): 0.9900163630539952, RMSE (Train): 0.16783065149657342\n",
      "Test Loss: 1.2166129350662231, R2 Score (Test): 0.48663913323153984, RMSE (Test): 1.1238605976104736\n",
      "Epoch [96/5000], Train Loss: 0.11928700997183721, R2 Score (Train): 0.9927585307096117, RMSE (Train): 0.14293554347856963\n",
      "Test Loss: 1.1574489772319794, R2 Score (Test): 0.4894362872589413, RMSE (Test): 1.120794653892517\n",
      "Epoch [101/5000], Train Loss: 0.16233664378523827, R2 Score (Train): 0.9908446334438351, RMSE (Train): 0.16071808874709917\n",
      "Test Loss: 1.0019451975822449, R2 Score (Test): 0.5444788632825608, RMSE (Test): 1.0586570501327515\n",
      "Epoch [106/5000], Train Loss: 0.0859747997795542, R2 Score (Train): 0.9894952642834003, RMSE (Train): 0.17215492679278416\n",
      "Test Loss: 1.1564668416976929, R2 Score (Test): 0.5444539978725883, RMSE (Test): 1.0586860179901123\n",
      "Epoch [111/5000], Train Loss: 0.14364801223079363, R2 Score (Train): 0.9864435172718266, RMSE (Train): 0.19556917477819938\n",
      "Test Loss: 1.2655897736549377, R2 Score (Test): 0.485979549481789, RMSE (Test): 1.124582290649414\n",
      "Epoch [116/5000], Train Loss: 0.12068148112545411, R2 Score (Train): 0.9887175314401998, RMSE (Train): 0.1784140109084831\n",
      "Test Loss: 1.0231503546237946, R2 Score (Test): 0.5502692536369471, RMSE (Test): 1.051906943321228\n",
      "Epoch [121/5000], Train Loss: 0.07979413882518809, R2 Score (Train): 0.9906223472113359, RMSE (Train): 0.1626574520718168\n",
      "Test Loss: 1.1755178570747375, R2 Score (Test): 0.5348340383236643, RMSE (Test): 1.0698059797286987\n",
      "Epoch [126/5000], Train Loss: 0.10668715989838044, R2 Score (Train): 0.9901210978638433, RMSE (Train): 0.1669480044579275\n",
      "Test Loss: 1.0467323064804077, R2 Score (Test): 0.5702752618472748, RMSE (Test): 1.0282440185546875\n",
      "Epoch [131/5000], Train Loss: 0.11508709254364173, R2 Score (Train): 0.9933505976680596, RMSE (Train): 0.13696771051404122\n",
      "Test Loss: 1.1620523035526276, R2 Score (Test): 0.5653501038171984, RMSE (Test): 1.034119725227356\n",
      "Epoch [136/5000], Train Loss: 0.08735668038328488, R2 Score (Train): 0.9902484581201649, RMSE (Train): 0.165868354329007\n",
      "Test Loss: 1.1254310011863708, R2 Score (Test): 0.5589551585628816, RMSE (Test): 1.0416994094848633\n",
      "Epoch [141/5000], Train Loss: 0.06988831330090761, R2 Score (Train): 0.9929199688016005, RMSE (Train): 0.14133329249719678\n",
      "Test Loss: 1.1620301306247711, R2 Score (Test): 0.5589319658579246, RMSE (Test): 1.04172682762146\n",
      "Epoch [146/5000], Train Loss: 0.0684788345048825, R2 Score (Train): 0.9945939005939761, RMSE (Train): 0.12350055356832283\n",
      "Test Loss: 1.1065237522125244, R2 Score (Test): 0.5532914706190708, RMSE (Test): 1.0483665466308594\n",
      "Epoch [151/5000], Train Loss: 0.0945687925753494, R2 Score (Train): 0.9911899963757136, RMSE (Train): 0.1576576084007579\n",
      "Test Loss: 1.042931705713272, R2 Score (Test): 0.563264691906165, RMSE (Test): 1.0365976095199585\n",
      "Epoch [156/5000], Train Loss: 0.08429174746076266, R2 Score (Train): 0.994324600329487, RMSE (Train): 0.12653920918950728\n",
      "Test Loss: 1.0457053780555725, R2 Score (Test): 0.5671135761764514, RMSE (Test): 1.0320197343826294\n",
      "Epoch [161/5000], Train Loss: 0.10840606161703666, R2 Score (Train): 0.9925958864968928, RMSE (Train): 0.14453180437088076\n",
      "Test Loss: 1.0018214285373688, R2 Score (Test): 0.5676912787749351, RMSE (Test): 1.031330943107605\n",
      "Epoch [166/5000], Train Loss: 0.07518257821599643, R2 Score (Train): 0.9905407896492584, RMSE (Train): 0.16336323777156572\n",
      "Test Loss: 0.8886342644691467, R2 Score (Test): 0.6003052343857117, RMSE (Test): 0.9916657209396362\n",
      "Epoch [171/5000], Train Loss: 0.09927821873376767, R2 Score (Train): 0.9915091237429279, RMSE (Train): 0.1547758317388219\n",
      "Test Loss: 0.9491636455059052, R2 Score (Test): 0.5857038654706119, RMSE (Test): 1.0096166133880615\n",
      "Epoch [176/5000], Train Loss: 0.09227524247641365, R2 Score (Train): 0.9934138403263048, RMSE (Train): 0.13631480241639135\n",
      "Test Loss: 1.02096888422966, R2 Score (Test): 0.5804145184521164, RMSE (Test): 1.0160410404205322\n",
      "Epoch [181/5000], Train Loss: 0.13516929559409618, R2 Score (Train): 0.9880062804797061, RMSE (Train): 0.18395171234133093\n",
      "Test Loss: 0.9848063588142395, R2 Score (Test): 0.6400348781077861, RMSE (Test): 0.9410902261734009\n",
      "Epoch [186/5000], Train Loss: 0.048744432628154755, R2 Score (Train): 0.9929001581112226, RMSE (Train): 0.14153088726353447\n",
      "Test Loss: 0.9287529289722443, R2 Score (Test): 0.6191528604165094, RMSE (Test): 0.9680023193359375\n",
      "Epoch [191/5000], Train Loss: 0.08885359515746434, R2 Score (Train): 0.9933780769433632, RMSE (Train): 0.13668440159177192\n",
      "Test Loss: 1.0133326053619385, R2 Score (Test): 0.6155271123741027, RMSE (Test): 0.9725991487503052\n",
      "Epoch [196/5000], Train Loss: 0.0979554879789551, R2 Score (Train): 0.9929511823888164, RMSE (Train): 0.14102140177814523\n",
      "Test Loss: 0.9381000101566315, R2 Score (Test): 0.6258738045644767, RMSE (Test): 0.9594230055809021\n",
      "Epoch [201/5000], Train Loss: 0.07860976457595825, R2 Score (Train): 0.9923006506882046, RMSE (Train): 0.1473852092866332\n",
      "Test Loss: 0.7918294221162796, R2 Score (Test): 0.6300531385863581, RMSE (Test): 0.9540491104125977\n",
      "Epoch [206/5000], Train Loss: 0.11611347443734606, R2 Score (Train): 0.993387423814584, RMSE (Train): 0.13658790223415224\n",
      "Test Loss: 0.9840060770511627, R2 Score (Test): 0.6188980304561045, RMSE (Test): 0.9683261513710022\n",
      "Epoch [211/5000], Train Loss: 0.06620875947798292, R2 Score (Train): 0.9933622542202267, RMSE (Train): 0.1368476041117345\n",
      "Test Loss: 0.78249391913414, R2 Score (Test): 0.6380131956782931, RMSE (Test): 0.9437292814254761\n",
      "Epoch [216/5000], Train Loss: 0.060345046843091645, R2 Score (Train): 0.9914171299158295, RMSE (Train): 0.15561202707887378\n",
      "Test Loss: 0.9587869942188263, R2 Score (Test): 0.6251005695413014, RMSE (Test): 0.9604138731956482\n",
      "Epoch [221/5000], Train Loss: 0.08173016520837943, R2 Score (Train): 0.9923685467278491, RMSE (Train): 0.14673391846969974\n",
      "Test Loss: 0.9026831686496735, R2 Score (Test): 0.6354508161548728, RMSE (Test): 0.9470635056495667\n",
      "Epoch [226/5000], Train Loss: 0.07557173740739624, R2 Score (Train): 0.9934356039102386, RMSE (Train): 0.1360893938574098\n",
      "Test Loss: 0.8517533242702484, R2 Score (Test): 0.621390476660534, RMSE (Test): 0.9651543498039246\n",
      "Epoch [231/5000], Train Loss: 0.06936688410739104, R2 Score (Train): 0.9929755426513138, RMSE (Train): 0.14077751040836897\n",
      "Test Loss: 0.9056262075901031, R2 Score (Test): 0.6304333633105865, RMSE (Test): 0.9535587430000305\n",
      "Epoch [236/5000], Train Loss: 0.08936366609608133, R2 Score (Train): 0.9921177050685308, RMSE (Train): 0.1491259526582217\n",
      "Test Loss: 0.8557270765304565, R2 Score (Test): 0.6216680763901006, RMSE (Test): 0.9648005962371826\n",
      "Epoch [241/5000], Train Loss: 0.06460212419430415, R2 Score (Train): 0.9904052142625842, RMSE (Train): 0.1645297853519752\n",
      "Test Loss: 0.8862938284873962, R2 Score (Test): 0.6266669820575236, RMSE (Test): 0.9584053754806519\n",
      "Epoch [246/5000], Train Loss: 0.061377277597784996, R2 Score (Train): 0.9951092840086879, RMSE (Train): 0.11746625215572983\n",
      "Test Loss: 0.763803243637085, R2 Score (Test): 0.6556510424861932, RMSE (Test): 0.9204504489898682\n",
      "Epoch [251/5000], Train Loss: 0.07282328295210998, R2 Score (Train): 0.9917667254675157, RMSE (Train): 0.15240990385595476\n",
      "Test Loss: 0.8420743346214294, R2 Score (Test): 0.6358310973772556, RMSE (Test): 0.9465695023536682\n",
      "Epoch [256/5000], Train Loss: 0.04635243924955527, R2 Score (Train): 0.9920890488227803, RMSE (Train): 0.14939678196803227\n",
      "Test Loss: 0.7962113618850708, R2 Score (Test): 0.6368203107556302, RMSE (Test): 0.9452828764915466\n",
      "Epoch [261/5000], Train Loss: 0.034627145156264305, R2 Score (Train): 0.9949658449473306, RMSE (Train): 0.11917637863341433\n",
      "Test Loss: 0.9154598116874695, R2 Score (Test): 0.6464254804691458, RMSE (Test): 0.9326989650726318\n",
      "Epoch [266/5000], Train Loss: 0.05880476844807466, R2 Score (Train): 0.9940997222400104, RMSE (Train): 0.12902180671634608\n",
      "Test Loss: 0.7467282265424728, R2 Score (Test): 0.6488591570185755, RMSE (Test): 0.9294836521148682\n",
      "Epoch [271/5000], Train Loss: 0.05227955741186937, R2 Score (Train): 0.994616904262662, RMSE (Train): 0.12323751785424686\n",
      "Test Loss: 0.9128079414367676, R2 Score (Test): 0.6515008300841556, RMSE (Test): 0.9259806871414185\n",
      "Epoch [276/5000], Train Loss: 0.05442649017398556, R2 Score (Train): 0.9949739665508209, RMSE (Train): 0.11908020618963742\n",
      "Test Loss: 0.9089126288890839, R2 Score (Test): 0.6315417849076502, RMSE (Test): 0.9521276354789734\n",
      "Epoch [281/5000], Train Loss: 0.06352866720408201, R2 Score (Train): 0.9941835464571878, RMSE (Train): 0.12810203311094287\n",
      "Test Loss: 1.0282771289348602, R2 Score (Test): 0.6367632492030038, RMSE (Test): 0.9453571438789368\n",
      "Epoch [286/5000], Train Loss: 0.03963008445377151, R2 Score (Train): 0.991058136880226, RMSE (Train): 0.1588330587335953\n",
      "Test Loss: 0.8096777498722076, R2 Score (Test): 0.658227542934982, RMSE (Test): 0.9170004725456238\n",
      "Epoch [291/5000], Train Loss: 0.05388955189846456, R2 Score (Train): 0.9946252565852678, RMSE (Train): 0.12314187408945625\n",
      "Test Loss: 0.958891898393631, R2 Score (Test): 0.6571385547863162, RMSE (Test): 0.9184602499008179\n",
      "Epoch [296/5000], Train Loss: 0.040745535089323916, R2 Score (Train): 0.9929644444507408, RMSE (Train): 0.14088867632068264\n",
      "Test Loss: 0.7532225251197815, R2 Score (Test): 0.6596077997487624, RMSE (Test): 0.9151469469070435\n",
      "Epoch [301/5000], Train Loss: 0.05093487072736025, R2 Score (Train): 0.9913323072275646, RMSE (Train): 0.15637907695257106\n",
      "Test Loss: 0.8415998816490173, R2 Score (Test): 0.6432313028623801, RMSE (Test): 0.9369025230407715\n",
      "Epoch [306/5000], Train Loss: 0.047696130350232124, R2 Score (Train): 0.9954085198702618, RMSE (Train): 0.11381598080799196\n",
      "Test Loss: 0.7480704635381699, R2 Score (Test): 0.6587853956956684, RMSE (Test): 0.9162518382072449\n",
      "Epoch [311/5000], Train Loss: 0.0502823474816978, R2 Score (Train): 0.9938461583422835, RMSE (Train): 0.13176499484355914\n",
      "Test Loss: 0.8571280539035797, R2 Score (Test): 0.6648576480732784, RMSE (Test): 0.9080623984336853\n",
      "Epoch [316/5000], Train Loss: 0.04926637001335621, R2 Score (Train): 0.9937070882623815, RMSE (Train): 0.13324554911639644\n",
      "Test Loss: 0.8648201525211334, R2 Score (Test): 0.6581499709381473, RMSE (Test): 0.9171045422554016\n",
      "Epoch [321/5000], Train Loss: 0.06626071160038312, R2 Score (Train): 0.993015335555407, RMSE (Train): 0.1403781982714862\n",
      "Test Loss: 0.8278931081295013, R2 Score (Test): 0.6619705332775425, RMSE (Test): 0.9119653105735779\n",
      "Epoch [326/5000], Train Loss: 0.0746805272065103, R2 Score (Train): 0.9922726220504611, RMSE (Train): 0.1476532354328144\n",
      "Test Loss: 0.9435319900512695, R2 Score (Test): 0.6481710503067195, RMSE (Test): 0.9303939342498779\n",
      "Epoch [331/5000], Train Loss: 0.041159129391113915, R2 Score (Train): 0.9924385982479992, RMSE (Train): 0.14605890737008573\n",
      "Test Loss: 0.9212498366832733, R2 Score (Test): 0.6604882921905286, RMSE (Test): 0.9139625430107117\n",
      "Epoch [336/5000], Train Loss: 0.04177536147957047, R2 Score (Train): 0.9933485556357476, RMSE (Train): 0.13698874030036584\n",
      "Test Loss: 0.762589156627655, R2 Score (Test): 0.6780044874975479, RMSE (Test): 0.8900737166404724\n",
      "Epoch [341/5000], Train Loss: 0.05323743990932902, R2 Score (Train): 0.9940971603733776, RMSE (Train): 0.12904981393935203\n",
      "Test Loss: 0.8942093849182129, R2 Score (Test): 0.6844503219659679, RMSE (Test): 0.8811196684837341\n",
      "Epoch [346/5000], Train Loss: 0.05046013401200374, R2 Score (Train): 0.9934366296074659, RMSE (Train): 0.1360787613509976\n",
      "Test Loss: 0.6906819343566895, R2 Score (Test): 0.6725364786044163, RMSE (Test): 0.8975992798805237\n",
      "Epoch [351/5000], Train Loss: 0.04517910350114107, R2 Score (Train): 0.9913741539852207, RMSE (Train): 0.15600112892041385\n",
      "Test Loss: 0.870650053024292, R2 Score (Test): 0.6572306606620166, RMSE (Test): 0.9183369278907776\n",
      "Epoch [356/5000], Train Loss: 0.060639438995470606, R2 Score (Train): 0.9900619343523099, RMSE (Train): 0.16744717358595812\n",
      "Test Loss: 0.7991832196712494, R2 Score (Test): 0.6700016343849273, RMSE (Test): 0.9010666608810425\n",
      "Epoch [361/5000], Train Loss: 0.052719190095861755, R2 Score (Train): 0.9903668150591304, RMSE (Train): 0.16485868817362095\n",
      "Test Loss: 0.9237619042396545, R2 Score (Test): 0.6665408923279248, RMSE (Test): 0.9057791829109192\n",
      "Epoch [366/5000], Train Loss: 0.039447850392510496, R2 Score (Train): 0.9921666002347426, RMSE (Train): 0.14866270679107232\n",
      "Test Loss: 0.888224720954895, R2 Score (Test): 0.6642307946392458, RMSE (Test): 0.9089112877845764\n",
      "Epoch [371/5000], Train Loss: 0.04351484915241599, R2 Score (Train): 0.9937330553527364, RMSE (Train): 0.13297035250798528\n",
      "Test Loss: 0.8460525572299957, R2 Score (Test): 0.6670985448738898, RMSE (Test): 0.9050214290618896\n",
      "Epoch [376/5000], Train Loss: 0.03741480177268386, R2 Score (Train): 0.9927455036727878, RMSE (Train): 0.14306405261416055\n",
      "Test Loss: 0.7307269871234894, R2 Score (Test): 0.6835413636432912, RMSE (Test): 0.8823878765106201\n",
      "Epoch [381/5000], Train Loss: 0.05250084912404418, R2 Score (Train): 0.991667681822342, RMSE (Train): 0.1533238844176977\n",
      "Test Loss: 0.7277452349662781, R2 Score (Test): 0.6797698517269053, RMSE (Test): 0.8876304030418396\n",
      "Epoch [386/5000], Train Loss: 0.05884923196087281, R2 Score (Train): 0.9906656310507459, RMSE (Train): 0.16228163404871399\n",
      "Test Loss: 0.8020564019680023, R2 Score (Test): 0.6729635639367271, RMSE (Test): 0.897013783454895\n",
      "Epoch [391/5000], Train Loss: 0.046695786993950605, R2 Score (Train): 0.9930883579859809, RMSE (Train): 0.1396424656970423\n",
      "Test Loss: 0.7033893764019012, R2 Score (Test): 0.6984817158456273, RMSE (Test): 0.8613068461418152\n",
      "Epoch [396/5000], Train Loss: 0.030792716114471357, R2 Score (Train): 0.9956934177134836, RMSE (Train): 0.11022833959806519\n",
      "Test Loss: 0.7386338114738464, R2 Score (Test): 0.6828154582862469, RMSE (Test): 0.8833993077278137\n",
      "Epoch [401/5000], Train Loss: 0.057599375334878765, R2 Score (Train): 0.9945195623383876, RMSE (Train): 0.12434677089240273\n",
      "Test Loss: 0.7551777362823486, R2 Score (Test): 0.6933263475099016, RMSE (Test): 0.8686389327049255\n",
      "Epoch [406/5000], Train Loss: 0.046393840573728085, R2 Score (Train): 0.9920984215186617, RMSE (Train): 0.14930825496521233\n",
      "Test Loss: 0.7798411846160889, R2 Score (Test): 0.6788278548795024, RMSE (Test): 0.8889349102973938\n",
      "Epoch [411/5000], Train Loss: 0.05875865494211515, R2 Score (Train): 0.9928656513917662, RMSE (Train): 0.14187440526308803\n",
      "Test Loss: 0.7734663188457489, R2 Score (Test): 0.684673356915042, RMSE (Test): 0.8808082938194275\n",
      "Epoch [416/5000], Train Loss: 0.032636886152128376, R2 Score (Train): 0.9940892056316634, RMSE (Train): 0.129136739248634\n",
      "Test Loss: 0.8586553633213043, R2 Score (Test): 0.6788792465328579, RMSE (Test): 0.8888638019561768\n",
      "Epoch [421/5000], Train Loss: 0.029555876351272065, R2 Score (Train): 0.994221571437064, RMSE (Train): 0.12768261391726904\n",
      "Test Loss: 0.7955439984798431, R2 Score (Test): 0.6782631568456032, RMSE (Test): 0.8897161483764648\n",
      "Epoch [426/5000], Train Loss: 0.04230497016881903, R2 Score (Train): 0.992981793151235, RMSE (Train): 0.14071486317415174\n",
      "Test Loss: 0.8403739929199219, R2 Score (Test): 0.6784918144971164, RMSE (Test): 0.8893998265266418\n",
      "Epoch [431/5000], Train Loss: 0.045509876062472664, R2 Score (Train): 0.9919325431334693, RMSE (Train): 0.15086733427602295\n",
      "Test Loss: 0.8276344835758209, R2 Score (Test): 0.7070700230078351, RMSE (Test): 0.8489516377449036\n",
      "Epoch [436/5000], Train Loss: 0.04989788665746649, R2 Score (Train): 0.9936298823063034, RMSE (Train): 0.1340604333368812\n",
      "Test Loss: 0.770283579826355, R2 Score (Test): 0.7061644844137274, RMSE (Test): 0.8502628207206726\n",
      "Epoch [441/5000], Train Loss: 0.038076914536456265, R2 Score (Train): 0.9933518433243053, RMSE (Train): 0.13695488058837535\n",
      "Test Loss: 0.7587383389472961, R2 Score (Test): 0.6923265938322813, RMSE (Test): 0.8700537085533142\n",
      "Epoch [446/5000], Train Loss: 0.03659884554023544, R2 Score (Train): 0.9915547458173793, RMSE (Train): 0.15435946098118747\n",
      "Test Loss: 0.7465424239635468, R2 Score (Test): 0.6960960964776555, RMSE (Test): 0.8647074103355408\n",
      "Epoch [451/5000], Train Loss: 0.052906656016906105, R2 Score (Train): 0.9964934967225506, RMSE (Train): 0.09946356362153727\n",
      "Test Loss: 0.6781340837478638, R2 Score (Test): 0.7013741998933244, RMSE (Test): 0.857165515422821\n",
      "Epoch [456/5000], Train Loss: 0.035111849661916494, R2 Score (Train): 0.99250966485006, RMSE (Train): 0.1453709122708613\n",
      "Test Loss: 0.9092799127101898, R2 Score (Test): 0.6765517614130563, RMSE (Test): 0.8920791745185852\n",
      "Epoch [461/5000], Train Loss: 0.051517650950700045, R2 Score (Train): 0.9921897068019856, RMSE (Train): 0.1484432859880381\n",
      "Test Loss: 0.8024713397026062, R2 Score (Test): 0.6854953681950691, RMSE (Test): 0.8796594738960266\n",
      "Epoch [466/5000], Train Loss: 0.02988169559588035, R2 Score (Train): 0.9911862229553009, RMSE (Train): 0.1576913680208054\n",
      "Test Loss: 0.6750214993953705, R2 Score (Test): 0.6963042055461587, RMSE (Test): 0.8644112944602966\n",
      "Epoch [471/5000], Train Loss: 0.031209707725793123, R2 Score (Train): 0.992494573765556, RMSE (Train): 0.1455172809447798\n",
      "Test Loss: 0.8159626722335815, R2 Score (Test): 0.6826341915259004, RMSE (Test): 0.8836517333984375\n",
      "Epoch [476/5000], Train Loss: 0.044492420429984726, R2 Score (Train): 0.9937449757173116, RMSE (Train): 0.1328438307641026\n",
      "Test Loss: 0.7866663932800293, R2 Score (Test): 0.683805091946792, RMSE (Test): 0.882020115852356\n",
      "Epoch [481/5000], Train Loss: 0.040278888307511806, R2 Score (Train): 0.9922123457026476, RMSE (Train): 0.14822799115160165\n",
      "Test Loss: 0.7830407619476318, R2 Score (Test): 0.6903791106871853, RMSE (Test): 0.8728029131889343\n",
      "Epoch [486/5000], Train Loss: 0.03512916015461087, R2 Score (Train): 0.9940985532484631, RMSE (Train): 0.12903458729531136\n",
      "Test Loss: 0.7634043991565704, R2 Score (Test): 0.6922405822572701, RMSE (Test): 0.870175302028656\n",
      "Epoch [491/5000], Train Loss: 0.034979619551450014, R2 Score (Train): 0.988531919577446, RMSE (Train): 0.17987560000725808\n",
      "Test Loss: 0.7503831684589386, R2 Score (Test): 0.697570566710064, RMSE (Test): 0.8626072406768799\n",
      "Epoch [496/5000], Train Loss: 0.03637928057772418, R2 Score (Train): 0.9923076403832115, RMSE (Train): 0.14731829378947445\n",
      "Test Loss: 0.715540885925293, R2 Score (Test): 0.6964850737399212, RMSE (Test): 0.8641538619995117\n",
      "Epoch [501/5000], Train Loss: 0.03507813912195464, R2 Score (Train): 0.9916451817213773, RMSE (Train): 0.15353075824525828\n",
      "Test Loss: 0.6879082322120667, R2 Score (Test): 0.6948587829297738, RMSE (Test): 0.8664658665657043\n",
      "Epoch [506/5000], Train Loss: 0.03532035633300742, R2 Score (Train): 0.991952802737794, RMSE (Train): 0.15067778049855407\n",
      "Test Loss: 0.777841329574585, R2 Score (Test): 0.684111296498356, RMSE (Test): 0.881592869758606\n",
      "Epoch [511/5000], Train Loss: 0.03514091957670947, R2 Score (Train): 0.9936240198588954, RMSE (Train): 0.13412210735445482\n",
      "Test Loss: 0.7891876101493835, R2 Score (Test): 0.7039217314229397, RMSE (Test): 0.853501558303833\n",
      "Epoch [516/5000], Train Loss: 0.050904938175032534, R2 Score (Train): 0.9930993917066585, RMSE (Train): 0.1395309588096958\n",
      "Test Loss: 0.8240976333618164, R2 Score (Test): 0.6756964019184617, RMSE (Test): 0.8932580351829529\n",
      "Epoch [521/5000], Train Loss: 0.030729322073360283, R2 Score (Train): 0.9921079065617191, RMSE (Train): 0.14921861335211853\n",
      "Test Loss: 0.8003127872943878, R2 Score (Test): 0.7059684140424687, RMSE (Test): 0.8505464792251587\n",
      "Epoch [526/5000], Train Loss: 0.0335428478817145, R2 Score (Train): 0.9923686039743803, RMSE (Train): 0.14673336811426385\n",
      "Test Loss: 0.6633365303277969, R2 Score (Test): 0.6981650810659735, RMSE (Test): 0.8617589473724365\n",
      "Epoch [531/5000], Train Loss: 0.05942657946919402, R2 Score (Train): 0.992955843916095, RMSE (Train): 0.14097476389734176\n",
      "Test Loss: 0.7108798325061798, R2 Score (Test): 0.7061591030900947, RMSE (Test): 0.8502706289291382\n",
      "Epoch [536/5000], Train Loss: 0.0606428412720561, R2 Score (Train): 0.9917012957105246, RMSE (Train): 0.1530143054907789\n",
      "Test Loss: 0.6993897259235382, R2 Score (Test): 0.7017854019228065, RMSE (Test): 0.8565752506256104\n",
      "Epoch [541/5000], Train Loss: 0.03552180978779992, R2 Score (Train): 0.9942323882037509, RMSE (Train): 0.1275630520009759\n",
      "Test Loss: 0.7223106026649475, R2 Score (Test): 0.7174471153226714, RMSE (Test): 0.8337789177894592\n",
      "Epoch [546/5000], Train Loss: 0.03400848705011109, R2 Score (Train): 0.991902302717212, RMSE (Train): 0.15114982872773797\n",
      "Test Loss: 0.7885852456092834, R2 Score (Test): 0.7056709025929233, RMSE (Test): 0.8509766459465027\n",
      "Epoch [551/5000], Train Loss: 0.03132891375571489, R2 Score (Train): 0.9937586488706518, RMSE (Train): 0.13269855652470447\n",
      "Test Loss: 0.7045458257198334, R2 Score (Test): 0.7095465626808067, RMSE (Test): 0.8453553318977356\n",
      "Epoch [556/5000], Train Loss: 0.03351082854593793, R2 Score (Train): 0.9904913248106655, RMSE (Train): 0.16378981669924356\n",
      "Test Loss: 0.8095675706863403, R2 Score (Test): 0.6998411433153018, RMSE (Test): 0.8593629002571106\n",
      "Epoch [561/5000], Train Loss: 0.03355507226660848, R2 Score (Train): 0.9934376781469088, RMSE (Train): 0.13606789120051244\n",
      "Test Loss: 0.770401120185852, R2 Score (Test): 0.7081445592410488, RMSE (Test): 0.8473930954933167\n",
      "Epoch [566/5000], Train Loss: 0.03800647628183166, R2 Score (Train): 0.9951555743246462, RMSE (Train): 0.11690902518676663\n",
      "Test Loss: 0.741553783416748, R2 Score (Test): 0.6983663909455113, RMSE (Test): 0.8614714741706848\n",
      "Epoch [571/5000], Train Loss: 0.033486398247381054, R2 Score (Train): 0.9916541996318716, RMSE (Train): 0.1534478779003806\n",
      "Test Loss: 0.7095912098884583, R2 Score (Test): 0.7034033041580089, RMSE (Test): 0.8542484045028687\n",
      "Epoch [576/5000], Train Loss: 0.02949683430294196, R2 Score (Train): 0.9942954960820343, RMSE (Train): 0.12686324971725563\n",
      "Test Loss: 0.7701537907123566, R2 Score (Test): 0.7108451625319226, RMSE (Test): 0.8434634804725647\n",
      "Epoch [581/5000], Train Loss: 0.03477699433763822, R2 Score (Train): 0.9910873337429574, RMSE (Train): 0.15857353682740433\n",
      "Test Loss: 0.7761088013648987, R2 Score (Test): 0.6936804276438399, RMSE (Test): 0.8681373000144958\n",
      "Epoch [586/5000], Train Loss: 0.032905177058031164, R2 Score (Train): 0.9926483141691376, RMSE (Train): 0.1440191888121298\n",
      "Test Loss: 0.7312455773353577, R2 Score (Test): 0.7015077356670684, RMSE (Test): 0.8569739460945129\n",
      "Epoch [591/5000], Train Loss: 0.027804505700866382, R2 Score (Train): 0.9951213792310637, RMSE (Train): 0.11732090943245693\n",
      "Test Loss: 0.7490087449550629, R2 Score (Test): 0.7084833826543296, RMSE (Test): 0.8469011187553406\n",
      "Epoch [596/5000], Train Loss: 0.030321790215869743, R2 Score (Train): 0.9920961433164174, RMSE (Train): 0.14932977787277765\n",
      "Test Loss: 0.6464332640171051, R2 Score (Test): 0.7237135654113213, RMSE (Test): 0.8244813084602356\n",
      "Epoch [601/5000], Train Loss: 0.03581448085606098, R2 Score (Train): 0.988624372681132, RMSE (Train): 0.17914907431052765\n",
      "Test Loss: 0.7073330581188202, R2 Score (Test): 0.7090359826339805, RMSE (Test): 0.846098005771637\n",
      "Epoch [606/5000], Train Loss: 0.03608875007679065, R2 Score (Train): 0.9943736693444047, RMSE (Train): 0.12599099813907982\n",
      "Test Loss: 0.6603021621704102, R2 Score (Test): 0.7178751501012282, RMSE (Test): 0.8331471681594849\n",
      "Epoch [611/5000], Train Loss: 0.03186229042087992, R2 Score (Train): 0.9914585609785134, RMSE (Train): 0.1552359890615333\n",
      "Test Loss: 0.7278925180435181, R2 Score (Test): 0.6980990758809287, RMSE (Test): 0.8618531823158264\n",
      "Epoch [616/5000], Train Loss: 0.024883070029318333, R2 Score (Train): 0.9929874975050488, RMSE (Train): 0.14065766547716094\n",
      "Test Loss: 0.6918718814849854, R2 Score (Test): 0.7077046646554971, RMSE (Test): 0.8480314612388611\n",
      "Epoch [621/5000], Train Loss: 0.025923562546571095, R2 Score (Train): 0.9935282635823278, RMSE (Train): 0.13512549600772528\n",
      "Test Loss: 0.7302996218204498, R2 Score (Test): 0.71390920326473, RMSE (Test): 0.8389827013015747\n",
      "Epoch [626/5000], Train Loss: 0.03807201577971379, R2 Score (Train): 0.9883454955198566, RMSE (Train): 0.18133172677882042\n",
      "Test Loss: 0.6787202060222626, R2 Score (Test): 0.7101735629285308, RMSE (Test): 0.8444424867630005\n",
      "Epoch [631/5000], Train Loss: 0.03939714034398397, R2 Score (Train): 0.9931091968943048, RMSE (Train): 0.13943179265508907\n",
      "Test Loss: 0.6561806201934814, R2 Score (Test): 0.7214730399192484, RMSE (Test): 0.8278176188468933\n",
      "Epoch [636/5000], Train Loss: 0.030900465169300634, R2 Score (Train): 0.9918501045412111, RMSE (Train): 0.15163620599494992\n",
      "Test Loss: 0.6275974214076996, R2 Score (Test): 0.7188514056477937, RMSE (Test): 0.8317044377326965\n",
      "Epoch [641/5000], Train Loss: 0.029828389020015795, R2 Score (Train): 0.9894260726525604, RMSE (Train): 0.17272096339309853\n",
      "Test Loss: 0.8228356540203094, R2 Score (Test): 0.7009263633789942, RMSE (Test): 0.8578081130981445\n",
      "Epoch [646/5000], Train Loss: 0.025287137056390446, R2 Score (Train): 0.9944325179074949, RMSE (Train): 0.12533036517905516\n",
      "Test Loss: 0.7049828469753265, R2 Score (Test): 0.7178975636952781, RMSE (Test): 0.8331140875816345\n",
      "Epoch [651/5000], Train Loss: 0.02131054092509051, R2 Score (Train): 0.9931427408321131, RMSE (Train): 0.13909200662851615\n",
      "Test Loss: 0.6384529769420624, R2 Score (Test): 0.7170767969876423, RMSE (Test): 0.8343251943588257\n",
      "Epoch [656/5000], Train Loss: 0.030114831713338692, R2 Score (Train): 0.9902165664649312, RMSE (Train): 0.16613936267367269\n",
      "Test Loss: 0.6563551127910614, R2 Score (Test): 0.7244771474968514, RMSE (Test): 0.8233411908149719\n",
      "Epoch [661/5000], Train Loss: 0.03039364842697978, R2 Score (Train): 0.987887806870756, RMSE (Train): 0.1848580145388383\n",
      "Test Loss: 0.6512216627597809, R2 Score (Test): 0.7205183580726008, RMSE (Test): 0.8292350769042969\n",
      "Epoch [666/5000], Train Loss: 0.0339869213445733, R2 Score (Train): 0.9899288539557224, RMSE (Train): 0.1685645857409337\n",
      "Test Loss: 0.6828400790691376, R2 Score (Test): 0.7166486685089953, RMSE (Test): 0.8349561095237732\n",
      "Epoch [671/5000], Train Loss: 0.024229952444632847, R2 Score (Train): 0.9942155896418434, RMSE (Train): 0.12774868496761874\n",
      "Test Loss: 0.7587291598320007, R2 Score (Test): 0.7200046696601627, RMSE (Test): 0.8299968838691711\n",
      "Epoch [676/5000], Train Loss: 0.03636289838080605, R2 Score (Train): 0.993344704578299, RMSE (Train): 0.1370283914754637\n",
      "Test Loss: 0.7302275002002716, R2 Score (Test): 0.7261985067089713, RMSE (Test): 0.8207652568817139\n",
      "Epoch [681/5000], Train Loss: 0.030577392938236397, R2 Score (Train): 0.9941670328167642, RMSE (Train): 0.12828375308266574\n",
      "Test Loss: 0.7166356146335602, R2 Score (Test): 0.7201160220770136, RMSE (Test): 0.8298317790031433\n",
      "Epoch [686/5000], Train Loss: 0.029458431371798117, R2 Score (Train): 0.9924030214096928, RMSE (Train): 0.14640211200312847\n",
      "Test Loss: 0.619349017739296, R2 Score (Test): 0.7123753097464092, RMSE (Test): 0.8412287831306458\n",
      "Epoch [691/5000], Train Loss: 0.028177466088285048, R2 Score (Train): 0.987106410627858, RMSE (Train): 0.19072771603944327\n",
      "Test Loss: 0.7188433110713959, R2 Score (Test): 0.7099286329957379, RMSE (Test): 0.8447991609573364\n",
      "Epoch [696/5000], Train Loss: 0.03286816583325466, R2 Score (Train): 0.9879355411004594, RMSE (Train): 0.18449339163685644\n",
      "Test Loss: 0.6219252347946167, R2 Score (Test): 0.7299802527903201, RMSE (Test): 0.8150773048400879\n",
      "Epoch [701/5000], Train Loss: 0.022664471296593547, R2 Score (Train): 0.9939168759197687, RMSE (Train): 0.13100571091257443\n",
      "Test Loss: 0.6865501701831818, R2 Score (Test): 0.7185325871572369, RMSE (Test): 0.8321758508682251\n",
      "Epoch [706/5000], Train Loss: 0.02367223030887544, R2 Score (Train): 0.9949372817049104, RMSE (Train): 0.11951399724440011\n",
      "Test Loss: 0.6992256045341492, R2 Score (Test): 0.7196117220939935, RMSE (Test): 0.8305789828300476\n",
      "Epoch [711/5000], Train Loss: 0.029687140913059313, R2 Score (Train): 0.9942925498971509, RMSE (Train): 0.12689600579451085\n",
      "Test Loss: 0.6890057027339935, R2 Score (Test): 0.7301330138746799, RMSE (Test): 0.8148466944694519\n",
      "Epoch [716/5000], Train Loss: 0.044923984135190644, R2 Score (Train): 0.9905509965777067, RMSE (Train): 0.1632750757120206\n",
      "Test Loss: 0.7145315110683441, R2 Score (Test): 0.7212772608819338, RMSE (Test): 0.8281084895133972\n",
      "Epoch [721/5000], Train Loss: 0.028223999387895066, R2 Score (Train): 0.9937644784125876, RMSE (Train): 0.13263657054593248\n",
      "Test Loss: 0.6896211206912994, R2 Score (Test): 0.7326049802513331, RMSE (Test): 0.8111062049865723\n",
      "Epoch [726/5000], Train Loss: 0.030895827958981197, R2 Score (Train): 0.9928931095475801, RMSE (Train): 0.1416011241792708\n",
      "Test Loss: 0.6467524468898773, R2 Score (Test): 0.736391641331438, RMSE (Test): 0.8053424954414368\n",
      "Epoch [731/5000], Train Loss: 0.02614314714446664, R2 Score (Train): 0.9942732052494148, RMSE (Train): 0.12711087250246716\n",
      "Test Loss: 0.6923105716705322, R2 Score (Test): 0.721891164876218, RMSE (Test): 0.8271960020065308\n",
      "Epoch [736/5000], Train Loss: 0.022497081585849326, R2 Score (Train): 0.9935231824264793, RMSE (Train): 0.1351785311552134\n",
      "Test Loss: 0.6456268131732941, R2 Score (Test): 0.7311390488544218, RMSE (Test): 0.8133264780044556\n",
      "Epoch [741/5000], Train Loss: 0.023643597106759746, R2 Score (Train): 0.9919971619400227, RMSE (Train): 0.15026191007155348\n",
      "Test Loss: 0.6311653256416321, R2 Score (Test): 0.7304991878613544, RMSE (Test): 0.8142937421798706\n",
      "Epoch [746/5000], Train Loss: 0.027411888198306162, R2 Score (Train): 0.9925416021515129, RMSE (Train): 0.14506066486006872\n",
      "Test Loss: 0.5921762138605118, R2 Score (Test): 0.7265265522213387, RMSE (Test): 0.8202733993530273\n",
      "Epoch [751/5000], Train Loss: 0.027813877289493878, R2 Score (Train): 0.9899786723218841, RMSE (Train): 0.16814715444858686\n",
      "Test Loss: 0.6518487930297852, R2 Score (Test): 0.7364426073387098, RMSE (Test): 0.8052647113800049\n",
      "Epoch [756/5000], Train Loss: 0.03219435503706336, R2 Score (Train): 0.9897163951013433, RMSE (Train): 0.17033330842795116\n",
      "Test Loss: 0.6957116425037384, R2 Score (Test): 0.7103846779745244, RMSE (Test): 0.8441348671913147\n",
      "Epoch [761/5000], Train Loss: 0.022594368318095803, R2 Score (Train): 0.9909796877072308, RMSE (Train): 0.15952827805171343\n",
      "Test Loss: 0.6462838649749756, R2 Score (Test): 0.7134177225916201, RMSE (Test): 0.8397029638290405\n",
      "Epoch [766/5000], Train Loss: 0.021608560035626095, R2 Score (Train): 0.993470972885764, RMSE (Train): 0.13572227375416673\n",
      "Test Loss: 0.6573760509490967, R2 Score (Test): 0.7200513131762678, RMSE (Test): 0.8299277424812317\n",
      "Epoch [771/5000], Train Loss: 0.03756203952555855, R2 Score (Train): 0.9919215701203028, RMSE (Train): 0.15096990109047878\n",
      "Test Loss: 0.711206316947937, R2 Score (Test): 0.712529090372864, RMSE (Test): 0.8410038352012634\n",
      "Epoch [776/5000], Train Loss: 0.020226135617122054, R2 Score (Train): 0.9902169573388675, RMSE (Train): 0.16613604378797261\n",
      "Test Loss: 0.7153534591197968, R2 Score (Test): 0.713422164086996, RMSE (Test): 0.83969646692276\n",
      "Epoch [781/5000], Train Loss: 0.022649614528442424, R2 Score (Train): 0.9941905244251499, RMSE (Train): 0.12802516839117806\n",
      "Test Loss: 0.63250532746315, R2 Score (Test): 0.7186172597352167, RMSE (Test): 0.8320506811141968\n",
      "Epoch [786/5000], Train Loss: 0.02062613897336026, R2 Score (Train): 0.9935730929264321, RMSE (Train): 0.13465667948029647\n",
      "Test Loss: 0.6649941205978394, R2 Score (Test): 0.7140052707584572, RMSE (Test): 0.8388417959213257\n",
      "Epoch [791/5000], Train Loss: 0.026435631637771923, R2 Score (Train): 0.9939888443168918, RMSE (Train): 0.1302284521261896\n",
      "Test Loss: 0.7036155164241791, R2 Score (Test): 0.7264516029194699, RMSE (Test): 0.8203858137130737\n",
      "Epoch [796/5000], Train Loss: 0.025930301441500585, R2 Score (Train): 0.9933717013707691, RMSE (Train): 0.13675018547833043\n",
      "Test Loss: 0.6047104895114899, R2 Score (Test): 0.7338347064309614, RMSE (Test): 0.8092389106750488\n",
      "Epoch [801/5000], Train Loss: 0.02769015869125724, R2 Score (Train): 0.9911364881205302, RMSE (Train): 0.1581356566229354\n",
      "Test Loss: 0.6455336809158325, R2 Score (Test): 0.7199712626638447, RMSE (Test): 0.8300463557243347\n",
      "Epoch [806/5000], Train Loss: 0.02173045448337992, R2 Score (Train): 0.9921793829049462, RMSE (Train): 0.14854136214631974\n",
      "Test Loss: 0.6594776511192322, R2 Score (Test): 0.7261465380479792, RMSE (Test): 0.8208431005477905\n",
      "Epoch [811/5000], Train Loss: 0.023833331574375432, R2 Score (Train): 0.992464061651133, RMSE (Train): 0.1458127695931778\n",
      "Test Loss: 0.6594512164592743, R2 Score (Test): 0.7252800265012231, RMSE (Test): 0.8221406936645508\n",
      "Epoch [816/5000], Train Loss: 0.025035644803817075, R2 Score (Train): 0.9945927056017347, RMSE (Train): 0.12351420241544449\n",
      "Test Loss: 0.6972382664680481, R2 Score (Test): 0.7164672333667058, RMSE (Test): 0.8352234363555908\n",
      "Epoch [821/5000], Train Loss: 0.02753945447814961, R2 Score (Train): 0.9890691929895902, RMSE (Train): 0.17561152083605192\n",
      "Test Loss: 0.6737987399101257, R2 Score (Test): 0.7337082491553968, RMSE (Test): 0.8094311356544495\n",
      "Epoch [826/5000], Train Loss: 0.018158956818903487, R2 Score (Train): 0.9943803036882081, RMSE (Train): 0.12591669445878212\n",
      "Test Loss: 0.6280943155288696, R2 Score (Test): 0.7255283498013283, RMSE (Test): 0.8217690587043762\n",
      "Epoch [831/5000], Train Loss: 0.028251051747550566, R2 Score (Train): 0.9855700323042531, RMSE (Train): 0.2017713826791972\n",
      "Test Loss: 0.6718192100524902, R2 Score (Test): 0.7298733823510248, RMSE (Test): 0.8152385950088501\n",
      "Epoch [836/5000], Train Loss: 0.019451760531713564, R2 Score (Train): 0.9937918912372176, RMSE (Train): 0.1323446985733186\n",
      "Test Loss: 0.6430560648441315, R2 Score (Test): 0.7358299504831958, RMSE (Test): 0.8062001466751099\n",
      "Epoch [841/5000], Train Loss: 0.02014641642260055, R2 Score (Train): 0.9911133093825284, RMSE (Train): 0.15834228982986864\n",
      "Test Loss: 0.6759221255779266, R2 Score (Test): 0.7310201776275156, RMSE (Test): 0.8135062456130981\n",
      "Epoch [846/5000], Train Loss: 0.023865907297780115, R2 Score (Train): 0.9945073636570024, RMSE (Train): 0.1244850831576166\n",
      "Test Loss: 0.6763243675231934, R2 Score (Test): 0.7285435209370864, RMSE (Test): 0.8172428607940674\n",
      "Epoch [851/5000], Train Loss: 0.022956308055048186, R2 Score (Train): 0.9943572679582374, RMSE (Train): 0.12617450346117823\n",
      "Test Loss: 0.7468317747116089, R2 Score (Test): 0.7243217854679651, RMSE (Test): 0.8235733509063721\n",
      "Epoch [856/5000], Train Loss: 0.03089461786051591, R2 Score (Train): 0.9903474186494079, RMSE (Train): 0.16502457612632956\n",
      "Test Loss: 0.7472741603851318, R2 Score (Test): 0.7041106841264412, RMSE (Test): 0.8532291650772095\n",
      "Epoch [861/5000], Train Loss: 0.03363775104905168, R2 Score (Train): 0.990819781786516, RMSE (Train): 0.16093607041254168\n",
      "Test Loss: 0.7641407549381256, R2 Score (Test): 0.7120530176155973, RMSE (Test): 0.8416999578475952\n",
      "Epoch [866/5000], Train Loss: 0.017204473571230967, R2 Score (Train): 0.9930030540739294, RMSE (Train): 0.14050156103640907\n",
      "Test Loss: 0.6552340090274811, R2 Score (Test): 0.7249673019058382, RMSE (Test): 0.8226085901260376\n",
      "Epoch [871/5000], Train Loss: 0.01886412687599659, R2 Score (Train): 0.9939917966570675, RMSE (Train): 0.1301964677681343\n",
      "Test Loss: 0.7137660086154938, R2 Score (Test): 0.7238772270802402, RMSE (Test): 0.8242371082305908\n",
      "Epoch [876/5000], Train Loss: 0.023774205862234037, R2 Score (Train): 0.992860421910848, RMSE (Train): 0.14192639274134147\n",
      "Test Loss: 0.641127198934555, R2 Score (Test): 0.7292419274566397, RMSE (Test): 0.8161908984184265\n",
      "Epoch [881/5000], Train Loss: 0.020467622283225257, R2 Score (Train): 0.9936499873774816, RMSE (Train): 0.1338487084726111\n",
      "Test Loss: 0.7109813094139099, R2 Score (Test): 0.731467744260107, RMSE (Test): 0.8128291964530945\n",
      "Epoch [886/5000], Train Loss: 0.024659805077438552, R2 Score (Train): 0.9935467340935817, RMSE (Train): 0.13493253231329275\n",
      "Test Loss: 0.6251883506774902, R2 Score (Test): 0.7296397458104202, RMSE (Test): 0.8155910968780518\n",
      "Epoch [891/5000], Train Loss: 0.028345647267997265, R2 Score (Train): 0.9915898835394311, RMSE (Train): 0.15403800865088801\n",
      "Test Loss: 0.6339436173439026, R2 Score (Test): 0.7314026606539261, RMSE (Test): 0.8129276633262634\n",
      "Epoch [896/5000], Train Loss: 0.02222150455539425, R2 Score (Train): 0.994563899428528, RMSE (Train): 0.12384276280522799\n",
      "Test Loss: 0.6496508121490479, R2 Score (Test): 0.7250342614224587, RMSE (Test): 0.8225083947181702\n",
      "Epoch [901/5000], Train Loss: 0.02719599800184369, R2 Score (Train): 0.9913334741301291, RMSE (Train): 0.1563685502015216\n",
      "Test Loss: 0.6426030099391937, R2 Score (Test): 0.7311323328016706, RMSE (Test): 0.8133366107940674\n",
      "Epoch [906/5000], Train Loss: 0.019680901352937024, R2 Score (Train): 0.9934270349036743, RMSE (Train): 0.13617818881852442\n",
      "Test Loss: 0.7526670694351196, R2 Score (Test): 0.7246612336056248, RMSE (Test): 0.8230661153793335\n",
      "Epoch [911/5000], Train Loss: 0.020332407671958208, R2 Score (Train): 0.9922514212872988, RMSE (Train): 0.14785564672469895\n",
      "Test Loss: 0.6727075278759003, R2 Score (Test): 0.7323162940372467, RMSE (Test): 0.8115439414978027\n",
      "Epoch [916/5000], Train Loss: 0.022953414513419073, R2 Score (Train): 0.9920639274254015, RMSE (Train): 0.14963380031681106\n",
      "Test Loss: 0.6568352580070496, R2 Score (Test): 0.7316621249502832, RMSE (Test): 0.8125349283218384\n",
      "Epoch [921/5000], Train Loss: 0.023722077642257016, R2 Score (Train): 0.9937960896219176, RMSE (Train): 0.1322999403425211\n",
      "Test Loss: 0.6997148096561432, R2 Score (Test): 0.7361868066794617, RMSE (Test): 0.8056553602218628\n",
      "Epoch [926/5000], Train Loss: 0.01665130560286343, R2 Score (Train): 0.9921266257719676, RMSE (Train): 0.14904154291374389\n",
      "Test Loss: 0.6886054575443268, R2 Score (Test): 0.7256199208054268, RMSE (Test): 0.8216319680213928\n",
      "Epoch [931/5000], Train Loss: 0.019200981128960848, R2 Score (Train): 0.9901841155043123, RMSE (Train): 0.16641467085594985\n",
      "Test Loss: 0.6460057497024536, R2 Score (Test): 0.7251520450873726, RMSE (Test): 0.8223322629928589\n",
      "Epoch [936/5000], Train Loss: 0.019620936131104827, R2 Score (Train): 0.9905647783033269, RMSE (Train): 0.16315596085423553\n",
      "Test Loss: 0.6985052227973938, R2 Score (Test): 0.7235545978227449, RMSE (Test): 0.8247184753417969\n",
      "Epoch [941/5000], Train Loss: 0.02483264380134642, R2 Score (Train): 0.9941047826023918, RMSE (Train): 0.12896646719058996\n",
      "Test Loss: 0.6040998697280884, R2 Score (Test): 0.7395658700445469, RMSE (Test): 0.8004790544509888\n",
      "Epoch [946/5000], Train Loss: 0.022313306108117104, R2 Score (Train): 0.9910323667332657, RMSE (Train): 0.1590617698183002\n",
      "Test Loss: 0.714735597372055, R2 Score (Test): 0.7160770150698202, RMSE (Test): 0.8357980251312256\n",
      "Epoch [951/5000], Train Loss: 0.020858652501677472, R2 Score (Train): 0.9943952081861777, RMSE (Train): 0.12574960614803427\n",
      "Test Loss: 0.6992325186729431, R2 Score (Test): 0.7335351695505624, RMSE (Test): 0.8096941113471985\n",
      "Epoch [956/5000], Train Loss: 0.017367854171122115, R2 Score (Train): 0.9934647565146009, RMSE (Train): 0.13578686984828117\n",
      "Test Loss: 0.757704883813858, R2 Score (Test): 0.7155708779006721, RMSE (Test): 0.8365426063537598\n",
      "Epoch [961/5000], Train Loss: 0.02113922033458948, R2 Score (Train): 0.9928091618419909, RMSE (Train): 0.14243497636291125\n",
      "Test Loss: 0.6741227507591248, R2 Score (Test): 0.7275183336261921, RMSE (Test): 0.8187846541404724\n",
      "Epoch [966/5000], Train Loss: 0.01725738433500131, R2 Score (Train): 0.994921902421474, RMSE (Train): 0.11969538654655994\n",
      "Test Loss: 0.7598689049482346, R2 Score (Test): 0.7368342344413432, RMSE (Test): 0.8046661615371704\n",
      "Epoch [971/5000], Train Loss: 0.02043009006107847, R2 Score (Train): 0.9896462151433775, RMSE (Train): 0.17091353583820051\n",
      "Test Loss: 0.7305447161197662, R2 Score (Test): 0.7262734544798215, RMSE (Test): 0.8206529021263123\n",
      "Epoch [976/5000], Train Loss: 0.027088869207849104, R2 Score (Train): 0.9924070451458542, RMSE (Train): 0.14636333596290518\n",
      "Test Loss: 0.672534167766571, R2 Score (Test): 0.7354355456587256, RMSE (Test): 0.8068016767501831\n",
      "Epoch [981/5000], Train Loss: 0.029263028719772894, R2 Score (Train): 0.9941147773648802, RMSE (Train): 0.12885709582021768\n",
      "Test Loss: 0.6555245220661163, R2 Score (Test): 0.7397664337954866, RMSE (Test): 0.8001708388328552\n",
      "Epoch [986/5000], Train Loss: 0.019715687027201056, R2 Score (Train): 0.9929611046144071, RMSE (Train): 0.14092211286220582\n",
      "Test Loss: 0.6150738000869751, R2 Score (Test): 0.7279648624140025, RMSE (Test): 0.8181135058403015\n",
      "Epoch [991/5000], Train Loss: 0.017229481289784115, R2 Score (Train): 0.9926437900973509, RMSE (Train): 0.1440634951764559\n",
      "Test Loss: 0.622434601187706, R2 Score (Test): 0.7252764697379206, RMSE (Test): 0.8221460580825806\n",
      "Epoch [996/5000], Train Loss: 0.018472964176908135, R2 Score (Train): 0.9948330178792879, RMSE (Train): 0.12073838712278782\n",
      "Test Loss: 0.6150824725627899, R2 Score (Test): 0.7290187924526373, RMSE (Test): 0.8165271282196045\n",
      "Epoch [1001/5000], Train Loss: 0.020846082363277674, R2 Score (Train): 0.9946353563923095, RMSE (Train): 0.12302612028583676\n",
      "Test Loss: 0.6433440148830414, R2 Score (Test): 0.7371903829870992, RMSE (Test): 0.8041215538978577\n",
      "Epoch [1006/5000], Train Loss: 0.027837459075575072, R2 Score (Train): 0.991970051904126, RMSE (Train): 0.15051620496593104\n",
      "Test Loss: 0.699903666973114, R2 Score (Test): 0.7340270446815016, RMSE (Test): 0.808946430683136\n",
      "Epoch [1011/5000], Train Loss: 0.02048126367541651, R2 Score (Train): 0.9897642867788963, RMSE (Train): 0.1699362167585469\n",
      "Test Loss: 0.6261193156242371, R2 Score (Test): 0.7447056174483565, RMSE (Test): 0.7925409078598022\n",
      "Epoch [1016/5000], Train Loss: 0.020951882392788928, R2 Score (Train): 0.990861150816353, RMSE (Train): 0.16057304600472724\n",
      "Test Loss: 0.6529043018817902, R2 Score (Test): 0.7317586252382148, RMSE (Test): 0.8123887777328491\n",
      "Epoch [1021/5000], Train Loss: 0.018775835012396175, R2 Score (Train): 0.993337301883305, RMSE (Train): 0.13710457873735074\n",
      "Test Loss: 0.6195059418678284, R2 Score (Test): 0.7250267764864717, RMSE (Test): 0.8225195407867432\n",
      "Epoch [1026/5000], Train Loss: 0.017906143330037594, R2 Score (Train): 0.9936424340651647, RMSE (Train): 0.13392829104131657\n",
      "Test Loss: 0.6682690978050232, R2 Score (Test): 0.727211580244167, RMSE (Test): 0.8192453384399414\n",
      "Epoch [1031/5000], Train Loss: 0.021860656173278887, R2 Score (Train): 0.9880866261959268, RMSE (Train): 0.18333453234138455\n",
      "Test Loss: 0.6395786106586456, R2 Score (Test): 0.7307641692021751, RMSE (Test): 0.8138933181762695\n",
      "Epoch [1036/5000], Train Loss: 0.02041318950553735, R2 Score (Train): 0.9953070848541091, RMSE (Train): 0.11506632466486622\n",
      "Test Loss: 0.7548265010118484, R2 Score (Test): 0.7381080205250714, RMSE (Test): 0.8027163743972778\n",
      "Epoch [1041/5000], Train Loss: 0.02031942813967665, R2 Score (Train): 0.9892766182902972, RMSE (Train): 0.17393731965533818\n",
      "Test Loss: 0.5791420936584473, R2 Score (Test): 0.7352886905383887, RMSE (Test): 0.8070256114006042\n",
      "Epoch [1046/5000], Train Loss: 0.01661200464392702, R2 Score (Train): 0.9937388641002869, RMSE (Train): 0.1329087139970607\n",
      "Test Loss: 0.6339746713638306, R2 Score (Test): 0.7334632203247184, RMSE (Test): 0.8098034262657166\n",
      "Epoch [1051/5000], Train Loss: 0.025999560796966154, R2 Score (Train): 0.9914815043803586, RMSE (Train): 0.15502735696550649\n",
      "Test Loss: 0.6649216115474701, R2 Score (Test): 0.7442941004325746, RMSE (Test): 0.7931793928146362\n",
      "Epoch [1056/5000], Train Loss: 0.020651659773041803, R2 Score (Train): 0.9925496352929039, RMSE (Train): 0.14498252429982988\n",
      "Test Loss: 0.6342328488826752, R2 Score (Test): 0.7377994820107742, RMSE (Test): 0.8031890988349915\n",
      "Epoch [1061/5000], Train Loss: 0.017942300687233608, R2 Score (Train): 0.9917374227503692, RMSE (Train): 0.15268088095194915\n",
      "Test Loss: 0.6065456867218018, R2 Score (Test): 0.7437266147300982, RMSE (Test): 0.7940590381622314\n",
      "Epoch [1066/5000], Train Loss: 0.019880465775107343, R2 Score (Train): 0.9897238371382093, RMSE (Train): 0.17027166388815768\n",
      "Test Loss: 0.5994142144918442, R2 Score (Test): 0.7368116475034492, RMSE (Test): 0.8047007322311401\n",
      "Epoch [1071/5000], Train Loss: 0.026671201921999454, R2 Score (Train): 0.9908990937129514, RMSE (Train): 0.16023936375554967\n",
      "Test Loss: 0.6733851730823517, R2 Score (Test): 0.7287806756877775, RMSE (Test): 0.8168858289718628\n",
      "Epoch [1076/5000], Train Loss: 0.020935222739353776, R2 Score (Train): 0.9909417401214682, RMSE (Train): 0.1598634859318228\n",
      "Test Loss: 0.6783273220062256, R2 Score (Test): 0.7416023984568679, RMSE (Test): 0.7973431944847107\n",
      "Epoch [1081/5000], Train Loss: 0.018837100670983393, R2 Score (Train): 0.9942074590133252, RMSE (Train): 0.12783843589288882\n",
      "Test Loss: 0.6410002112388611, R2 Score (Test): 0.7278772520453938, RMSE (Test): 0.8182451725006104\n",
      "Epoch [1086/5000], Train Loss: 0.015160216328998407, R2 Score (Train): 0.9936098875591038, RMSE (Train): 0.1342706652731109\n",
      "Test Loss: 0.6687764525413513, R2 Score (Test): 0.7280334625389419, RMSE (Test): 0.8180103302001953\n",
      "Epoch [1091/5000], Train Loss: 0.02276873456624647, R2 Score (Train): 0.9923333681880221, RMSE (Train): 0.14707172765399357\n",
      "Test Loss: 0.6655513644218445, R2 Score (Test): 0.7338839148852455, RMSE (Test): 0.8091641068458557\n",
      "Epoch [1096/5000], Train Loss: 0.0163414539808097, R2 Score (Train): 0.9946241975844419, RMSE (Train): 0.12315400498699888\n",
      "Test Loss: 0.6356797814369202, R2 Score (Test): 0.7364945313048354, RMSE (Test): 0.8051853179931641\n",
      "Epoch [1101/5000], Train Loss: 0.015520114451646805, R2 Score (Train): 0.9914740172825341, RMSE (Train): 0.15509547052149408\n",
      "Test Loss: 0.5974667072296143, R2 Score (Test): 0.7464580182262351, RMSE (Test): 0.78981614112854\n",
      "Epoch [1106/5000], Train Loss: 0.015394632006064057, R2 Score (Train): 0.9944286543020162, RMSE (Train): 0.12537384471775653\n",
      "Test Loss: 0.683447927236557, R2 Score (Test): 0.7347001472630825, RMSE (Test): 0.8079221844673157\n",
      "Epoch [1111/5000], Train Loss: 0.020997683595245082, R2 Score (Train): 0.9916425582010123, RMSE (Train): 0.1535548616692975\n",
      "Test Loss: 0.6160047948360443, R2 Score (Test): 0.7416260287689298, RMSE (Test): 0.7973067164421082\n",
      "Epoch [1116/5000], Train Loss: 0.014260824886150658, R2 Score (Train): 0.9922822630238866, RMSE (Train): 0.147561097762356\n",
      "Test Loss: 0.6687082052230835, R2 Score (Test): 0.7340182275466267, RMSE (Test): 0.8089599013328552\n",
      "Epoch [1121/5000], Train Loss: 0.020595033187419176, R2 Score (Train): 0.9924634575338444, RMSE (Train): 0.14581861400469115\n",
      "Test Loss: 0.6259624063968658, R2 Score (Test): 0.73774808064618, RMSE (Test): 0.8032678365707397\n",
      "Epoch [1126/5000], Train Loss: 0.02522112902564307, R2 Score (Train): 0.9923164684950947, RMSE (Train): 0.14723373483748092\n",
      "Test Loss: 0.6806537508964539, R2 Score (Test): 0.7309078341680342, RMSE (Test): 0.8136761784553528\n",
      "Epoch [1131/5000], Train Loss: 0.016808215373506147, R2 Score (Train): 0.9930894689516654, RMSE (Train): 0.13963124229876528\n",
      "Test Loss: 0.637697845697403, R2 Score (Test): 0.738276665779859, RMSE (Test): 0.8024579882621765\n",
      "Epoch [1136/5000], Train Loss: 0.01955478172749281, R2 Score (Train): 0.9914648520032078, RMSE (Train): 0.15517881056227853\n",
      "Test Loss: 0.6418222486972809, R2 Score (Test): 0.7464004341111588, RMSE (Test): 0.7899057865142822\n",
      "Epoch [1141/5000], Train Loss: 0.02763723023235798, R2 Score (Train): 0.992311112980606, RMSE (Train): 0.14728503774727053\n",
      "Test Loss: 0.664059191942215, R2 Score (Test): 0.7300618209615266, RMSE (Test): 0.8149541616439819\n",
      "Epoch [1146/5000], Train Loss: 0.02058169967494905, R2 Score (Train): 0.9890944950798543, RMSE (Train): 0.175408154660407\n",
      "Test Loss: 0.682057112455368, R2 Score (Test): 0.7433585491220827, RMSE (Test): 0.7946290373802185\n",
      "Epoch [1151/5000], Train Loss: 0.024216934262464445, R2 Score (Train): 0.9873650437981204, RMSE (Train): 0.18880511730522948\n",
      "Test Loss: 0.6953770816326141, R2 Score (Test): 0.7397242822423794, RMSE (Test): 0.8002356290817261\n",
      "Epoch [1156/5000], Train Loss: 0.01736275781877339, R2 Score (Train): 0.9939847737899509, RMSE (Train): 0.13027253755173154\n",
      "Test Loss: 0.6269357800483704, R2 Score (Test): 0.7493601486640669, RMSE (Test): 0.7852827906608582\n",
      "Epoch [1161/5000], Train Loss: 0.02021290307554106, R2 Score (Train): 0.9943639789986757, RMSE (Train): 0.12609944991377015\n",
      "Test Loss: 0.664668083190918, R2 Score (Test): 0.7350559337552814, RMSE (Test): 0.8073802590370178\n",
      "Epoch [1166/5000], Train Loss: 0.021716004198727507, R2 Score (Train): 0.9944490073262515, RMSE (Train): 0.12514462975052904\n",
      "Test Loss: 0.5937697142362595, R2 Score (Test): 0.7391293548125843, RMSE (Test): 0.8011496663093567\n",
      "Epoch [1171/5000], Train Loss: 0.01780743113098045, R2 Score (Train): 0.993490846274738, RMSE (Train): 0.13551555710732566\n",
      "Test Loss: 0.5927174836397171, R2 Score (Test): 0.738809872203749, RMSE (Test): 0.8016400933265686\n",
      "Epoch [1176/5000], Train Loss: 0.015507606246198216, R2 Score (Train): 0.995249735444556, RMSE (Train): 0.11576726930452289\n",
      "Test Loss: 0.604361355304718, R2 Score (Test): 0.7337488605536173, RMSE (Test): 0.8093694448471069\n",
      "Epoch [1181/5000], Train Loss: 0.022003174759447575, R2 Score (Train): 0.9936769280579442, RMSE (Train): 0.13356447217820508\n",
      "Test Loss: 0.5975385755300522, R2 Score (Test): 0.7388456176555168, RMSE (Test): 0.8015852570533752\n",
      "Epoch [1186/5000], Train Loss: 0.034778752752269305, R2 Score (Train): 0.9914357839097657, RMSE (Train): 0.15544283165096423\n",
      "Test Loss: 0.6621223986148834, R2 Score (Test): 0.7446476363019026, RMSE (Test): 0.7926308512687683\n",
      "Epoch [1191/5000], Train Loss: 0.02223543372626106, R2 Score (Train): 0.9898388936018178, RMSE (Train): 0.16931576225984604\n",
      "Test Loss: 0.6609656512737274, R2 Score (Test): 0.7478170391106919, RMSE (Test): 0.7876965403556824\n",
      "Epoch [1196/5000], Train Loss: 0.01776002914023896, R2 Score (Train): 0.9932643022850741, RMSE (Train): 0.13785362315581956\n",
      "Test Loss: 0.6507945954799652, R2 Score (Test): 0.7459241351701762, RMSE (Test): 0.7906472086906433\n",
      "Epoch [1201/5000], Train Loss: 0.016604524105787277, R2 Score (Train): 0.9924046163831529, RMSE (Train): 0.14638674275219601\n",
      "Test Loss: 0.6747763454914093, R2 Score (Test): 0.7458192374259501, RMSE (Test): 0.7908104658126831\n",
      "Epoch [1206/5000], Train Loss: 0.023095448967069387, R2 Score (Train): 0.9920492343974925, RMSE (Train): 0.14977225399789706\n",
      "Test Loss: 0.6969329118728638, R2 Score (Test): 0.733570735083973, RMSE (Test): 0.809640109539032\n",
      "Epoch [1211/5000], Train Loss: 0.025230523198843002, R2 Score (Train): 0.9921451082009519, RMSE (Train): 0.14886650561967107\n",
      "Test Loss: 0.6305231750011444, R2 Score (Test): 0.7510803590102288, RMSE (Test): 0.7825834155082703\n",
      "Epoch [1216/5000], Train Loss: 0.014168845334400734, R2 Score (Train): 0.9933294088837459, RMSE (Train): 0.13718576552172726\n",
      "Test Loss: 0.6331034898757935, R2 Score (Test): 0.7439701999557489, RMSE (Test): 0.7936816215515137\n",
      "Epoch [1221/5000], Train Loss: 0.01842349162325263, R2 Score (Train): 0.9948752152790825, RMSE (Train): 0.12024435689034595\n",
      "Test Loss: 0.6615537703037262, R2 Score (Test): 0.7536587648465121, RMSE (Test): 0.7785196900367737\n",
      "Epoch [1226/5000], Train Loss: 0.019138738978654146, R2 Score (Train): 0.9938551862074099, RMSE (Train): 0.13166830782536235\n",
      "Test Loss: 0.5702545791864395, R2 Score (Test): 0.7463771128666499, RMSE (Test): 0.7899420857429504\n",
      "Epoch [1231/5000], Train Loss: 0.0211227946759512, R2 Score (Train): 0.992035457537214, RMSE (Train): 0.14990195838406217\n",
      "Test Loss: 0.6475768983364105, R2 Score (Test): 0.7440183709984287, RMSE (Test): 0.7936068773269653\n",
      "Epoch [1236/5000], Train Loss: 0.02323680293435852, R2 Score (Train): 0.9892734067286115, RMSE (Train): 0.17396336407994833\n",
      "Test Loss: 0.5928862541913986, R2 Score (Test): 0.7433049885846084, RMSE (Test): 0.7947119474411011\n",
      "Epoch [1241/5000], Train Loss: 0.015998925315216184, R2 Score (Train): 0.9926484549350623, RMSE (Train): 0.1440178100065199\n",
      "Test Loss: 0.6124630719423294, R2 Score (Test): 0.7328018385497653, RMSE (Test): 0.8108075857162476\n",
      "Epoch [1246/5000], Train Loss: 0.01713129171791176, R2 Score (Train): 0.9933999244285914, RMSE (Train): 0.13645873620980511\n",
      "Test Loss: 0.5841455012559891, R2 Score (Test): 0.7467346923673069, RMSE (Test): 0.7893850803375244\n",
      "Epoch [1251/5000], Train Loss: 0.016172813174004357, R2 Score (Train): 0.9927614626338289, RMSE (Train): 0.14290660469611635\n",
      "Test Loss: 0.5986556112766266, R2 Score (Test): 0.744348777213352, RMSE (Test): 0.7930946350097656\n",
      "Epoch [1256/5000], Train Loss: 0.01817284516679744, R2 Score (Train): 0.9918972269801103, RMSE (Train): 0.1511971926005997\n",
      "Test Loss: 0.5805641412734985, R2 Score (Test): 0.749746028023234, RMSE (Test): 0.7846781015396118\n",
      "Epoch [1261/5000], Train Loss: 0.016129073376456898, R2 Score (Train): 0.9926976478017691, RMSE (Train): 0.14353515361111624\n",
      "Test Loss: 0.6354792714118958, R2 Score (Test): 0.743133909908357, RMSE (Test): 0.7949767708778381\n",
      "Epoch [1266/5000], Train Loss: 0.016176701212922733, R2 Score (Train): 0.9904976847415788, RMSE (Train): 0.16373503166655384\n",
      "Test Loss: 0.6063933968544006, R2 Score (Test): 0.73349626826596, RMSE (Test): 0.8097532391548157\n",
      "Epoch [1271/5000], Train Loss: 0.025045114414145548, R2 Score (Train): 0.9910846101000708, RMSE (Train): 0.15859776440880552\n",
      "Test Loss: 0.5737256705760956, R2 Score (Test): 0.747277628931418, RMSE (Test): 0.7885385155677795\n",
      "Epoch [1276/5000], Train Loss: 0.01590945610466103, R2 Score (Train): 0.991348335137749, RMSE (Train): 0.15623442544997446\n",
      "Test Loss: 0.6499519050121307, R2 Score (Test): 0.7372163168702448, RMSE (Test): 0.8040818572044373\n",
      "Epoch [1281/5000], Train Loss: 0.017580481556554634, R2 Score (Train): 0.9939252842404365, RMSE (Train): 0.1309151391173798\n",
      "Test Loss: 0.577339306473732, R2 Score (Test): 0.7498022783124875, RMSE (Test): 0.7845898866653442\n",
      "Epoch [1286/5000], Train Loss: 0.016989789437502623, R2 Score (Train): 0.9921999848850334, RMSE (Train): 0.1483455806484519\n",
      "Test Loss: 0.6499086916446686, R2 Score (Test): 0.746821137233204, RMSE (Test): 0.7892503142356873\n",
      "Epoch [1291/5000], Train Loss: 0.012936454343919953, R2 Score (Train): 0.9916469143431591, RMSE (Train): 0.1535148378194754\n",
      "Test Loss: 0.6405981779098511, R2 Score (Test): 0.7424961084594099, RMSE (Test): 0.7959631681442261\n",
      "Epoch [1296/5000], Train Loss: 0.0147674431403478, R2 Score (Train): 0.9880612685363042, RMSE (Train): 0.18352954273706573\n",
      "Test Loss: 0.6470425426959991, R2 Score (Test): 0.7505878492080996, RMSE (Test): 0.7833572030067444\n",
      "Epoch [1301/5000], Train Loss: 0.015156620686563352, R2 Score (Train): 0.99295950719797, RMSE (Train): 0.1409381024825077\n",
      "Test Loss: 0.6450622081756592, R2 Score (Test): 0.7400688992689496, RMSE (Test): 0.7997056245803833\n",
      "Epoch [1306/5000], Train Loss: 0.011123277014121413, R2 Score (Train): 0.9934019537648918, RMSE (Train): 0.1364377559987838\n",
      "Test Loss: 0.639299601316452, R2 Score (Test): 0.7402164876166517, RMSE (Test): 0.7994785904884338\n",
      "Epoch [1311/5000], Train Loss: 0.016385126548508804, R2 Score (Train): 0.9928845143358087, RMSE (Train): 0.14168672588414488\n",
      "Test Loss: 0.6098765134811401, R2 Score (Test): 0.7529163194283427, RMSE (Test): 0.7796919941902161\n",
      "Epoch [1316/5000], Train Loss: 0.014485449142133197, R2 Score (Train): 0.9953190754367105, RMSE (Train): 0.1149192311568804\n",
      "Test Loss: 0.6451357901096344, R2 Score (Test): 0.7472872426224797, RMSE (Test): 0.7885234951972961\n",
      "Epoch [1321/5000], Train Loss: 0.014772838990514478, R2 Score (Train): 0.991121060716888, RMSE (Train): 0.15827321846007375\n",
      "Test Loss: 0.5624580532312393, R2 Score (Test): 0.7483216011951654, RMSE (Test): 0.7869080901145935\n",
      "Epoch [1326/5000], Train Loss: 0.016215514624491334, R2 Score (Train): 0.9931291473623063, RMSE (Train): 0.13922980271048194\n",
      "Test Loss: 0.6270051896572113, R2 Score (Test): 0.7455361275070969, RMSE (Test): 0.7912507057189941\n",
      "Epoch [1331/5000], Train Loss: 0.015812870813533664, R2 Score (Train): 0.993658285161767, RMSE (Train): 0.13376122748291286\n",
      "Test Loss: 0.6691490113735199, R2 Score (Test): 0.746083784199747, RMSE (Test): 0.7903987765312195\n",
      "Epoch [1336/5000], Train Loss: 0.018147501551235717, R2 Score (Train): 0.9934984325007254, RMSE (Train): 0.13543656439951424\n",
      "Test Loss: 0.6132974028587341, R2 Score (Test): 0.7452934192739389, RMSE (Test): 0.7916279435157776\n",
      "Epoch [1341/5000], Train Loss: 0.0237700204209735, R2 Score (Train): 0.9913494653140492, RMSE (Train): 0.15622422058105256\n",
      "Test Loss: 0.6020088493824005, R2 Score (Test): 0.7441163864921184, RMSE (Test): 0.7934549450874329\n",
      "Epoch [1346/5000], Train Loss: 0.016515180584974587, R2 Score (Train): 0.9931312717600715, RMSE (Train): 0.13920827682687886\n",
      "Test Loss: 0.5671376883983612, R2 Score (Test): 0.7459275450224168, RMSE (Test): 0.7906419634819031\n",
      "Epoch [1351/5000], Train Loss: 0.013842770674576363, R2 Score (Train): 0.9955547277003276, RMSE (Train): 0.11198918273789386\n",
      "Test Loss: 0.6782693564891815, R2 Score (Test): 0.74027659845101, RMSE (Test): 0.7993860840797424\n",
      "Epoch [1356/5000], Train Loss: 0.015584667834142843, R2 Score (Train): 0.9937524272933563, RMSE (Train): 0.1327646791211683\n",
      "Test Loss: 0.5960457921028137, R2 Score (Test): 0.7464088847269175, RMSE (Test): 0.7898926734924316\n",
      "Epoch [1361/5000], Train Loss: 0.014212012213344375, R2 Score (Train): 0.9918970487316561, RMSE (Train): 0.15119885564346042\n",
      "Test Loss: 0.6559994220733643, R2 Score (Test): 0.7377179460949488, RMSE (Test): 0.8033139705657959\n",
      "Epoch [1366/5000], Train Loss: 0.01774284018514057, R2 Score (Train): 0.9914342911107593, RMSE (Train): 0.15545637841294116\n",
      "Test Loss: 0.5931656360626221, R2 Score (Test): 0.7502765985512092, RMSE (Test): 0.783845841884613\n",
      "Epoch [1371/5000], Train Loss: 0.013639989192597568, R2 Score (Train): 0.9936243694960014, RMSE (Train): 0.13411842990351144\n",
      "Test Loss: 0.5974603593349457, R2 Score (Test): 0.7510941067877279, RMSE (Test): 0.7825617790222168\n",
      "Epoch [1376/5000], Train Loss: 0.01440717070363462, R2 Score (Train): 0.9929503958113253, RMSE (Train): 0.14102926984710276\n",
      "Test Loss: 0.6232943534851074, R2 Score (Test): 0.7464407212123455, RMSE (Test): 0.7898430228233337\n",
      "Epoch [1381/5000], Train Loss: 0.018080379425858457, R2 Score (Train): 0.9930168259404467, RMSE (Train): 0.14036322054920997\n",
      "Test Loss: 0.6552101671695709, R2 Score (Test): 0.742854019097631, RMSE (Test): 0.7954097390174866\n",
      "Epoch [1386/5000], Train Loss: 0.018696167157031596, R2 Score (Train): 0.9922366826484329, RMSE (Train): 0.14799619867506034\n",
      "Test Loss: 0.6672656536102295, R2 Score (Test): 0.7443411109952949, RMSE (Test): 0.7931064963340759\n",
      "Epoch [1391/5000], Train Loss: 0.01728260579208533, R2 Score (Train): 0.9944674619943563, RMSE (Train): 0.12493643047279043\n",
      "Test Loss: 0.61549311876297, R2 Score (Test): 0.7391738251427953, RMSE (Test): 0.8010813593864441\n",
      "Epoch [1396/5000], Train Loss: 0.015235839178785682, R2 Score (Train): 0.9935815169244844, RMSE (Train): 0.13456840065932502\n",
      "Test Loss: 0.6438112854957581, R2 Score (Test): 0.7495236861444435, RMSE (Test): 0.7850265502929688\n",
      "Epoch [1401/5000], Train Loss: 0.015776921917373937, R2 Score (Train): 0.9898843574498174, RMSE (Train): 0.16893655277209355\n",
      "Test Loss: 0.6521109938621521, R2 Score (Test): 0.7456620445207722, RMSE (Test): 0.791054904460907\n",
      "Epoch [1406/5000], Train Loss: 0.01917558489367366, R2 Score (Train): 0.9944315379675793, RMSE (Train): 0.1253413944757491\n",
      "Test Loss: 0.6576659083366394, R2 Score (Test): 0.7397788816738539, RMSE (Test): 0.8001516461372375\n",
      "Epoch [1411/5000], Train Loss: 0.014481632116561135, R2 Score (Train): 0.9923041996629876, RMSE (Train): 0.14735123715080553\n",
      "Test Loss: 0.6088354587554932, R2 Score (Test): 0.7346911831652702, RMSE (Test): 0.8079358339309692\n",
      "Epoch [1416/5000], Train Loss: 0.017486040558045108, R2 Score (Train): 0.9948909698511452, RMSE (Train): 0.12005938750457688\n",
      "Test Loss: 0.5746766179800034, R2 Score (Test): 0.7459683583439698, RMSE (Test): 0.7905784249305725\n",
      "Epoch [1421/5000], Train Loss: 0.01439379877410829, R2 Score (Train): 0.9934910899420114, RMSE (Train): 0.1355130206012974\n",
      "Test Loss: 0.6775985062122345, R2 Score (Test): 0.7455434931330714, RMSE (Test): 0.7912392616271973\n",
      "Epoch [1426/5000], Train Loss: 0.014035926898941398, R2 Score (Train): 0.9929371797177974, RMSE (Train): 0.14116140373948305\n",
      "Test Loss: 0.6578778624534607, R2 Score (Test): 0.7458102548482113, RMSE (Test): 0.7908244132995605\n",
      "Epoch [1431/5000], Train Loss: 0.018837664742022753, R2 Score (Train): 0.9890054496803691, RMSE (Train): 0.1761228182753841\n",
      "Test Loss: 0.6516779959201813, R2 Score (Test): 0.7478306885939822, RMSE (Test): 0.7876752018928528\n",
      "Epoch [1436/5000], Train Loss: 0.013639779839043817, R2 Score (Train): 0.9937188165889385, RMSE (Train): 0.13312132394099813\n",
      "Test Loss: 0.5864342600107193, R2 Score (Test): 0.7470323671709487, RMSE (Test): 0.7889209985733032\n",
      "Epoch [1441/5000], Train Loss: 0.016264116624370217, R2 Score (Train): 0.9921847041007431, RMSE (Train): 0.14849081932090627\n",
      "Test Loss: 0.6579495370388031, R2 Score (Test): 0.7485389485038794, RMSE (Test): 0.786568284034729\n",
      "Epoch [1446/5000], Train Loss: 0.01394479845960935, R2 Score (Train): 0.9943546533081978, RMSE (Train): 0.12620373256159698\n",
      "Test Loss: 0.5783451050519943, R2 Score (Test): 0.745345634844832, RMSE (Test): 0.7915467619895935\n",
      "Epoch [1451/5000], Train Loss: 0.015339958171049753, R2 Score (Train): 0.9914647461615462, RMSE (Train): 0.1551797727208855\n",
      "Test Loss: 0.6394343972206116, R2 Score (Test): 0.7499557585272982, RMSE (Test): 0.7843492031097412\n",
      "Epoch [1456/5000], Train Loss: 0.017593841378887493, R2 Score (Train): 0.9926259896449398, RMSE (Train): 0.14423769126222563\n",
      "Test Loss: 0.587570995092392, R2 Score (Test): 0.7477032901568841, RMSE (Test): 0.787874162197113\n",
      "Epoch [1461/5000], Train Loss: 0.013449146800364057, R2 Score (Train): 0.9939767791300345, RMSE (Train): 0.1303590795017724\n",
      "Test Loss: 0.6416527628898621, R2 Score (Test): 0.747935161218449, RMSE (Test): 0.7875120043754578\n",
      "Epoch [1466/5000], Train Loss: 0.014633896915862957, R2 Score (Train): 0.9941431812951204, RMSE (Train): 0.12854576734449497\n",
      "Test Loss: 0.593430370092392, R2 Score (Test): 0.7526740116954425, RMSE (Test): 0.7800742387771606\n",
      "Epoch [1471/5000], Train Loss: 0.013456959975883365, R2 Score (Train): 0.99467217680832, RMSE (Train): 0.12260319634300008\n",
      "Test Loss: 0.5974912196397781, R2 Score (Test): 0.7411315177343675, RMSE (Test): 0.7980693578720093\n",
      "Epoch [1476/5000], Train Loss: 0.013794956418375174, R2 Score (Train): 0.9943250341937214, RMSE (Train): 0.1265343723595405\n",
      "Test Loss: 0.6449840366840363, R2 Score (Test): 0.753972025596239, RMSE (Test): 0.7780245542526245\n",
      "Epoch [1481/5000], Train Loss: 0.012148593630020818, R2 Score (Train): 0.9932592349205577, RMSE (Train): 0.1379054680614398\n",
      "Test Loss: 0.6579895913600922, R2 Score (Test): 0.7445105808942605, RMSE (Test): 0.7928435206413269\n",
      "Epoch [1486/5000], Train Loss: 0.016303904820233583, R2 Score (Train): 0.9946391970763048, RMSE (Train): 0.12298207364588333\n",
      "Test Loss: 0.6399236023426056, R2 Score (Test): 0.7505446639181134, RMSE (Test): 0.7834250926971436\n",
      "Epoch [1491/5000], Train Loss: 0.01358780451118946, R2 Score (Train): 0.9889527516483089, RMSE (Train): 0.1765444012704328\n",
      "Test Loss: 0.6231708228588104, R2 Score (Test): 0.7584802299525273, RMSE (Test): 0.7708633542060852\n",
      "Epoch [1496/5000], Train Loss: 0.015048520096267263, R2 Score (Train): 0.9924943583513502, RMSE (Train): 0.1455193691849146\n",
      "Test Loss: 0.5306592136621475, R2 Score (Test): 0.7538713806350855, RMSE (Test): 0.77818363904953\n",
      "Epoch [1501/5000], Train Loss: 0.012726752863576015, R2 Score (Train): 0.9940726966923301, RMSE (Train): 0.1293169539475118\n",
      "Test Loss: 0.6221449077129364, R2 Score (Test): 0.7437044289551983, RMSE (Test): 0.7940934300422668\n",
      "Epoch [1506/5000], Train Loss: 0.013935568432013193, R2 Score (Train): 0.9908325081306697, RMSE (Train): 0.16082448057432797\n",
      "Test Loss: 0.6503081023693085, R2 Score (Test): 0.7441618780672786, RMSE (Test): 0.7933844923973083\n",
      "Epoch [1511/5000], Train Loss: 0.015566372700656453, R2 Score (Train): 0.9937404121114063, RMSE (Train): 0.13289228272251996\n",
      "Test Loss: 0.6059098839759827, R2 Score (Test): 0.7551772548029788, RMSE (Test): 0.7761165499687195\n",
      "Epoch [1516/5000], Train Loss: 0.01551100192591548, R2 Score (Train): 0.994465693000854, RMSE (Train): 0.12495640268712768\n",
      "Test Loss: 0.6448091268539429, R2 Score (Test): 0.7454637754069, RMSE (Test): 0.7913631200790405\n",
      "Epoch [1521/5000], Train Loss: 0.012787768811297914, R2 Score (Train): 0.9921873771148089, RMSE (Train): 0.14846542348187347\n",
      "Test Loss: 0.6562541425228119, R2 Score (Test): 0.7487114515582267, RMSE (Test): 0.786298394203186\n",
      "Epoch [1526/5000], Train Loss: 0.011493458490197858, R2 Score (Train): 0.9914167238443353, RMSE (Train): 0.15561570818206757\n",
      "Test Loss: 0.5832049548625946, R2 Score (Test): 0.7461508492334894, RMSE (Test): 0.7902944087982178\n",
      "Epoch [1531/5000], Train Loss: 0.017440908394443493, R2 Score (Train): 0.9926925369015814, RMSE (Train): 0.14358537479384745\n",
      "Test Loss: 0.5708762109279633, R2 Score (Test): 0.7579771588641563, RMSE (Test): 0.7716657519340515\n",
      "Epoch [1536/5000], Train Loss: 0.013491040716568628, R2 Score (Train): 0.9940171736640633, RMSE (Train): 0.12992121968926304\n",
      "Test Loss: 0.6094854474067688, R2 Score (Test): 0.7381423508963434, RMSE (Test): 0.8026638031005859\n",
      "Epoch [1541/5000], Train Loss: 0.013210020416105786, R2 Score (Train): 0.9921173361868215, RMSE (Train): 0.14912944207299342\n",
      "Test Loss: 0.6029245853424072, R2 Score (Test): 0.7464717384492232, RMSE (Test): 0.7897947430610657\n",
      "Epoch [1546/5000], Train Loss: 0.01410657629215469, R2 Score (Train): 0.990554808314405, RMSE (Train): 0.16324213973199397\n",
      "Test Loss: 0.619639664888382, R2 Score (Test): 0.748419166734095, RMSE (Test): 0.7867555618286133\n",
      "Epoch [1551/5000], Train Loss: 0.01429232112908115, R2 Score (Train): 0.9917515890959954, RMSE (Train): 0.1525499376665481\n",
      "Test Loss: 0.6425497531890869, R2 Score (Test): 0.7462258833192801, RMSE (Test): 0.790177583694458\n",
      "Epoch [1556/5000], Train Loss: 0.016557372407987714, R2 Score (Train): 0.9955675378391836, RMSE (Train): 0.11182770421511555\n",
      "Test Loss: 0.6831050515174866, R2 Score (Test): 0.754723233568052, RMSE (Test): 0.7768358588218689\n",
      "Epoch [1561/5000], Train Loss: 0.018250449405362208, R2 Score (Train): 0.9903280934156379, RMSE (Train): 0.16518968967287576\n",
      "Test Loss: 0.6581532061100006, R2 Score (Test): 0.7450610752808886, RMSE (Test): 0.7919889688491821\n",
      "Epoch [1566/5000], Train Loss: 0.012433579075150192, R2 Score (Train): 0.9952243652757854, RMSE (Train): 0.11607600201201773\n",
      "Test Loss: 0.6049487590789795, R2 Score (Test): 0.7485612859616083, RMSE (Test): 0.7865332961082458\n",
      "Epoch [1571/5000], Train Loss: 0.011583295534364879, R2 Score (Train): 0.9931884928017555, RMSE (Train): 0.13862721570743547\n",
      "Test Loss: 0.6036725044250488, R2 Score (Test): 0.7473723915208628, RMSE (Test): 0.788390576839447\n",
      "Epoch [1576/5000], Train Loss: 0.014227122611676654, R2 Score (Train): 0.992321982365196, RMSE (Train): 0.1471808962835877\n",
      "Test Loss: 0.6310285925865173, R2 Score (Test): 0.7375931031009042, RMSE (Test): 0.8035051226615906\n",
      "Epoch [1581/5000], Train Loss: 0.016631440065490704, R2 Score (Train): 0.9892750810375657, RMSE (Train): 0.17394978661981347\n",
      "Test Loss: 0.6644220650196075, R2 Score (Test): 0.7470308800913535, RMSE (Test): 0.7889233231544495\n",
      "Epoch [1586/5000], Train Loss: 0.012573656781266132, R2 Score (Train): 0.9935604209029296, RMSE (Train): 0.13478936634631733\n",
      "Test Loss: 0.6875619292259216, R2 Score (Test): 0.7543089074024877, RMSE (Test): 0.7774916887283325\n",
      "Epoch [1591/5000], Train Loss: 0.019435343410198886, R2 Score (Train): 0.9924994048347093, RMSE (Train): 0.14547044035314888\n",
      "Test Loss: 0.6022771298885345, R2 Score (Test): 0.7455701915487383, RMSE (Test): 0.7911977171897888\n",
      "Epoch [1596/5000], Train Loss: 0.014311382236580053, R2 Score (Train): 0.9914915336839397, RMSE (Train): 0.1549360688784649\n",
      "Test Loss: 0.6747974753379822, R2 Score (Test): 0.7449137834932293, RMSE (Test): 0.7922177314758301\n",
      "Epoch [1601/5000], Train Loss: 0.016441098026310403, R2 Score (Train): 0.9878937679545137, RMSE (Train): 0.184812519486341\n",
      "Test Loss: 0.6446645259857178, R2 Score (Test): 0.746646157431763, RMSE (Test): 0.7895230054855347\n",
      "Epoch [1606/5000], Train Loss: 0.015575007147466144, R2 Score (Train): 0.9938946235204034, RMSE (Train): 0.1312451051977098\n",
      "Test Loss: 0.6587222218513489, R2 Score (Test): 0.7455789381393516, RMSE (Test): 0.79118412733078\n",
      "Epoch [1611/5000], Train Loss: 0.012597299724196395, R2 Score (Train): 0.9944345282957114, RMSE (Train): 0.12530773507132856\n",
      "Test Loss: 0.561370238661766, R2 Score (Test): 0.7586652693461355, RMSE (Test): 0.7705680131912231\n",
      "Epoch [1616/5000], Train Loss: 0.014205990514407555, R2 Score (Train): 0.9912362185799085, RMSE (Train): 0.15724348452946343\n",
      "Test Loss: 0.6046575605869293, R2 Score (Test): 0.7557605004683189, RMSE (Test): 0.7751914858818054\n",
      "Epoch [1621/5000], Train Loss: 0.01228908721047143, R2 Score (Train): 0.9933469464398783, RMSE (Train): 0.13700531026409232\n",
      "Test Loss: 0.6318504214286804, R2 Score (Test): 0.7555697028174766, RMSE (Test): 0.7754942178726196\n",
      "Epoch [1626/5000], Train Loss: 0.012351077321606377, R2 Score (Train): 0.9920229043289508, RMSE (Train): 0.15002004486681766\n",
      "Test Loss: 0.5846325755119324, R2 Score (Test): 0.7549908374491398, RMSE (Test): 0.7764120101928711\n",
      "Epoch [1631/5000], Train Loss: 0.016888027389844257, R2 Score (Train): 0.9897586177249704, RMSE (Train): 0.16998326986569737\n",
      "Test Loss: 0.6832157671451569, R2 Score (Test): 0.7415700551975852, RMSE (Test): 0.7973931431770325\n",
      "Epoch [1636/5000], Train Loss: 0.013561369579595825, R2 Score (Train): 0.9931895847973, RMSE (Train): 0.13861610316177125\n",
      "Test Loss: 0.5636431574821472, R2 Score (Test): 0.7472321275283179, RMSE (Test): 0.7886094450950623\n",
      "Epoch [1641/5000], Train Loss: 0.012033886043354869, R2 Score (Train): 0.995144327134615, RMSE (Train): 0.11704465898562666\n",
      "Test Loss: 0.6364358067512512, R2 Score (Test): 0.751279114296426, RMSE (Test): 0.7822709679603577\n",
      "Epoch [1646/5000], Train Loss: 0.01289695311182489, R2 Score (Train): 0.991331535343061, RMSE (Train): 0.15638603981467125\n",
      "Test Loss: 0.6514672040939331, R2 Score (Test): 0.7494218409138811, RMSE (Test): 0.7851861715316772\n",
      "Epoch [1651/5000], Train Loss: 0.01427768551123639, R2 Score (Train): 0.9943227323104405, RMSE (Train): 0.1265600322345172\n",
      "Test Loss: 0.6226826906204224, R2 Score (Test): 0.7470815127284001, RMSE (Test): 0.7888444066047668\n",
      "Epoch [1656/5000], Train Loss: 0.012539310031570494, R2 Score (Train): 0.994147275185777, RMSE (Train): 0.1285008330263464\n",
      "Test Loss: 0.660289853811264, R2 Score (Test): 0.7546539383991898, RMSE (Test): 0.7769455909729004\n",
      "Epoch [1661/5000], Train Loss: 0.021428943378850818, R2 Score (Train): 0.9884757854912336, RMSE (Train): 0.18031529114567282\n",
      "Test Loss: 0.6352768540382385, R2 Score (Test): 0.7486139674557721, RMSE (Test): 0.7864509224891663\n",
      "Epoch [1666/5000], Train Loss: 0.009808592963963747, R2 Score (Train): 0.9946361067671472, RMSE (Train): 0.12301751589939862\n",
      "Test Loss: 0.634279876947403, R2 Score (Test): 0.7504436867158747, RMSE (Test): 0.7835836410522461\n",
      "Epoch [1671/5000], Train Loss: 0.016078371709833544, R2 Score (Train): 0.9946449773160352, RMSE (Train): 0.12291575358124324\n",
      "Test Loss: 0.7024668902158737, R2 Score (Test): 0.7487697567300295, RMSE (Test): 0.7862071990966797\n",
      "Epoch [1676/5000], Train Loss: 0.015629616798833013, R2 Score (Train): 0.992555736358948, RMSE (Train): 0.14492314941330167\n",
      "Test Loss: 0.6091557145118713, R2 Score (Test): 0.7479995501565577, RMSE (Test): 0.7874114513397217\n",
      "Epoch [1681/5000], Train Loss: 0.01227488562775155, R2 Score (Train): 0.9927637034769538, RMSE (Train): 0.14288448309517207\n",
      "Test Loss: 0.6564177572727203, R2 Score (Test): 0.7455268234351425, RMSE (Test): 0.7912651896476746\n",
      "Epoch [1686/5000], Train Loss: 0.014217537750179568, R2 Score (Train): 0.9920427042411076, RMSE (Train): 0.14983374716414202\n",
      "Test Loss: 0.5997343957424164, R2 Score (Test): 0.757715705906022, RMSE (Test): 0.7720824480056763\n",
      "Epoch [1691/5000], Train Loss: 0.01148465950973332, R2 Score (Train): 0.9934411238584732, RMSE (Train): 0.1360321635816622\n",
      "Test Loss: 0.6212256252765656, R2 Score (Test): 0.7483125363310046, RMSE (Test): 0.7869223356246948\n",
      "Epoch [1696/5000], Train Loss: 0.01751625786225001, R2 Score (Train): 0.9933767196219593, RMSE (Train): 0.13669840924153875\n",
      "Test Loss: 0.6487871408462524, R2 Score (Test): 0.747780287578341, RMSE (Test): 0.7877538800239563\n",
      "Epoch [1701/5000], Train Loss: 0.010438345295066634, R2 Score (Train): 0.9911318814543658, RMSE (Train): 0.15817674550280403\n",
      "Test Loss: 0.5605585873126984, R2 Score (Test): 0.7523764385062406, RMSE (Test): 0.780543327331543\n",
      "Epoch [1706/5000], Train Loss: 0.010785931760134796, R2 Score (Train): 0.9952120897882383, RMSE (Train): 0.1162250895287293\n",
      "Test Loss: 0.6043769121170044, R2 Score (Test): 0.7584748424090015, RMSE (Test): 0.7708719372749329\n",
      "Epoch [1711/5000], Train Loss: 0.01303932451022168, R2 Score (Train): 0.9929723049681106, RMSE (Train): 0.14080994995848548\n",
      "Test Loss: 0.6172564029693604, R2 Score (Test): 0.7475436400051494, RMSE (Test): 0.7881233096122742\n",
      "Epoch [1716/5000], Train Loss: 0.018400012360264856, R2 Score (Train): 0.9934753633554305, RMSE (Train): 0.13567663259465962\n",
      "Test Loss: 0.6574808359146118, R2 Score (Test): 0.7539005554310716, RMSE (Test): 0.7781375646591187\n",
      "Epoch [1721/5000], Train Loss: 0.014486678720762333, R2 Score (Train): 0.9938003182859831, RMSE (Train): 0.13225484399727966\n",
      "Test Loss: 0.6155745387077332, R2 Score (Test): 0.7489786445934375, RMSE (Test): 0.7858802676200867\n",
      "Epoch [1726/5000], Train Loss: 0.012057523165519038, R2 Score (Train): 0.9948031216078768, RMSE (Train): 0.12108718079111698\n",
      "Test Loss: 0.5973478555679321, R2 Score (Test): 0.7506652667678371, RMSE (Test): 0.7832356095314026\n",
      "Epoch [1731/5000], Train Loss: 0.009940982020149628, R2 Score (Train): 0.9933456991002345, RMSE (Train): 0.1370181527990453\n",
      "Test Loss: 0.639814168214798, R2 Score (Test): 0.7519713024644631, RMSE (Test): 0.7811816334724426\n",
      "Epoch [1736/5000], Train Loss: 0.013814477948471904, R2 Score (Train): 0.9953691462252925, RMSE (Train): 0.11430294618275903\n",
      "Test Loss: 0.6044324636459351, R2 Score (Test): 0.7417531968799154, RMSE (Test): 0.7971104979515076\n",
      "Epoch [1741/5000], Train Loss: 0.015513930547361573, R2 Score (Train): 0.9947464890450202, RMSE (Train): 0.12174516196419012\n",
      "Test Loss: 0.5958113670349121, R2 Score (Test): 0.754763866887038, RMSE (Test): 0.7767714262008667\n",
      "Epoch [1746/5000], Train Loss: 0.013235072644116977, R2 Score (Train): 0.9938056620225136, RMSE (Train): 0.1321978340224309\n",
      "Test Loss: 0.5628470331430435, R2 Score (Test): 0.7519112798247523, RMSE (Test): 0.7812761664390564\n",
      "Epoch [1751/5000], Train Loss: 0.00818271015305072, R2 Score (Train): 0.993873543288543, RMSE (Train): 0.13147148707785763\n",
      "Test Loss: 0.5438190549612045, R2 Score (Test): 0.7614960165835191, RMSE (Test): 0.7660354375839233\n",
      "Epoch [1756/5000], Train Loss: 0.0103241380614539, R2 Score (Train): 0.9944195038971527, RMSE (Train): 0.12547675977962458\n",
      "Test Loss: 0.5363461226224899, R2 Score (Test): 0.7593615965918161, RMSE (Test): 0.7694555521011353\n",
      "Epoch [1761/5000], Train Loss: 0.010982967525099715, R2 Score (Train): 0.995098230157089, RMSE (Train): 0.11759892410676445\n",
      "Test Loss: 0.526619553565979, R2 Score (Test): 0.7537787172718073, RMSE (Test): 0.7783300876617432\n",
      "Epoch [1766/5000], Train Loss: 0.011340200124929348, R2 Score (Train): 0.9942086550428649, RMSE (Train): 0.1278252373294841\n",
      "Test Loss: 0.6486233174800873, R2 Score (Test): 0.7545438174043144, RMSE (Test): 0.7771198749542236\n",
      "Epoch [1771/5000], Train Loss: 0.011320100359929105, R2 Score (Train): 0.9919165775628815, RMSE (Train): 0.15101654440450032\n",
      "Test Loss: 0.617056667804718, R2 Score (Test): 0.7715382815172009, RMSE (Test): 0.7497349381446838\n",
      "Epoch [1776/5000], Train Loss: 0.014368048054166138, R2 Score (Train): 0.9912310258384472, RMSE (Train): 0.1572900628160448\n",
      "Test Loss: 0.5792564451694489, R2 Score (Test): 0.7588564156513925, RMSE (Test): 0.7702627778053284\n",
      "Epoch [1781/5000], Train Loss: 0.013080322494109472, R2 Score (Train): 0.9926327186354773, RMSE (Train): 0.14417186577968608\n",
      "Test Loss: 0.5593030452728271, R2 Score (Test): 0.7563454262135219, RMSE (Test): 0.7742626667022705\n",
      "Epoch [1786/5000], Train Loss: 0.01988283482690652, R2 Score (Train): 0.9915665798120823, RMSE (Train): 0.15425127421917487\n",
      "Test Loss: 0.6022928357124329, R2 Score (Test): 0.7596855507306275, RMSE (Test): 0.7689374089241028\n",
      "Epoch [1791/5000], Train Loss: 0.014451955057059726, R2 Score (Train): 0.9930491514839302, RMSE (Train): 0.14003796870672855\n",
      "Test Loss: 0.5749479085206985, R2 Score (Test): 0.7544055882396132, RMSE (Test): 0.7773387432098389\n",
      "Epoch [1796/5000], Train Loss: 0.013857945528191825, R2 Score (Train): 0.9916926927893014, RMSE (Train): 0.15309359672652353\n",
      "Test Loss: 0.6697999536991119, R2 Score (Test): 0.759647761581904, RMSE (Test): 0.768997848033905\n",
      "Epoch [1801/5000], Train Loss: 0.014950972981750965, R2 Score (Train): 0.9917934700297943, RMSE (Train): 0.1521621621028672\n",
      "Test Loss: 0.5991490483283997, R2 Score (Test): 0.7553660161406328, RMSE (Test): 0.7758172750473022\n",
      "Epoch [1806/5000], Train Loss: 0.012952476662273208, R2 Score (Train): 0.9931850423790753, RMSE (Train): 0.13866232261667577\n",
      "Test Loss: 0.6029977202415466, R2 Score (Test): 0.7535385256994286, RMSE (Test): 0.7787096500396729\n",
      "Epoch [1811/5000], Train Loss: 0.012498597226416072, R2 Score (Train): 0.9939457044234271, RMSE (Train): 0.13069491799317334\n",
      "Test Loss: 0.5876196324825287, R2 Score (Test): 0.7540609965611572, RMSE (Test): 0.7778838276863098\n",
      "Epoch [1816/5000], Train Loss: 0.012720521772280335, R2 Score (Train): 0.9920724666949368, RMSE (Train): 0.1495532751422929\n",
      "Test Loss: 0.6311476528644562, R2 Score (Test): 0.756785679137169, RMSE (Test): 0.7735628485679626\n",
      "Epoch [1821/5000], Train Loss: 0.01936080780190726, R2 Score (Train): 0.9917253771045057, RMSE (Train): 0.1527921337702005\n",
      "Test Loss: 0.628620982170105, R2 Score (Test): 0.7609349347352701, RMSE (Test): 0.7669359445571899\n",
      "Epoch [1826/5000], Train Loss: 0.012465273185322681, R2 Score (Train): 0.9890515965663489, RMSE (Train): 0.1757528137899166\n",
      "Test Loss: 0.6256957650184631, R2 Score (Test): 0.7548408747175799, RMSE (Test): 0.776649534702301\n",
      "Epoch [1831/5000], Train Loss: 0.01148862395590792, R2 Score (Train): 0.9939010035475132, RMSE (Train): 0.1311765126874666\n",
      "Test Loss: 0.5832404792308807, R2 Score (Test): 0.7520764794372485, RMSE (Test): 0.7810159921646118\n",
      "Epoch [1836/5000], Train Loss: 0.011765458039008081, R2 Score (Train): 0.9928564188589475, RMSE (Train): 0.1419661751412788\n",
      "Test Loss: 0.6688737273216248, R2 Score (Test): 0.7491063586291253, RMSE (Test): 0.7856802940368652\n",
      "Epoch [1841/5000], Train Loss: 0.009654473707390329, R2 Score (Train): 0.9945803650402866, RMSE (Train): 0.1236550645388019\n",
      "Test Loss: 0.5986951887607574, R2 Score (Test): 0.7516384569636669, RMSE (Test): 0.7817056179046631\n",
      "Epoch [1846/5000], Train Loss: 0.011713268623376885, R2 Score (Train): 0.9934743241013488, RMSE (Train): 0.13568743755496404\n",
      "Test Loss: 0.581026554107666, R2 Score (Test): 0.7521868942349419, RMSE (Test): 0.7808420658111572\n",
      "Epoch [1851/5000], Train Loss: 0.010952421929687262, R2 Score (Train): 0.993051882995485, RMSE (Train): 0.14001045027384307\n",
      "Test Loss: 0.5968594253063202, R2 Score (Test): 0.7566739603537795, RMSE (Test): 0.7737404704093933\n",
      "Epoch [1856/5000], Train Loss: 0.013042973703704774, R2 Score (Train): 0.9943231357556163, RMSE (Train): 0.12655553527009697\n",
      "Test Loss: 0.6441836357116699, R2 Score (Test): 0.75436769668522, RMSE (Test): 0.7773986458778381\n",
      "Epoch [1861/5000], Train Loss: 0.01099208389253666, R2 Score (Train): 0.9931661401174009, RMSE (Train): 0.1388544893707976\n",
      "Test Loss: 0.6184889376163483, R2 Score (Test): 0.7526345304222695, RMSE (Test): 0.7801365256309509\n",
      "Epoch [1866/5000], Train Loss: 0.013204550370573997, R2 Score (Train): 0.9908194586461333, RMSE (Train): 0.160938902833544\n",
      "Test Loss: 0.6363644897937775, R2 Score (Test): 0.7549459737296131, RMSE (Test): 0.7764830589294434\n",
      "Epoch [1871/5000], Train Loss: 0.00900409510359168, R2 Score (Train): 0.9947735497728256, RMSE (Train): 0.12143120372069026\n",
      "Test Loss: 0.607881486415863, R2 Score (Test): 0.7590278534487003, RMSE (Test): 0.7699888944625854\n",
      "Epoch [1876/5000], Train Loss: 0.012553877934503058, R2 Score (Train): 0.9935019520478411, RMSE (Train): 0.13539990094122034\n",
      "Test Loss: 0.64276522397995, R2 Score (Test): 0.7525240146275891, RMSE (Test): 0.7803106904029846\n",
      "Epoch [1881/5000], Train Loss: 0.014286227601890763, R2 Score (Train): 0.9912346417771759, RMSE (Train): 0.1572576297250482\n",
      "Test Loss: 0.6177384853363037, R2 Score (Test): 0.7462449709852729, RMSE (Test): 0.7901478409767151\n",
      "Epoch [1886/5000], Train Loss: 0.010966568331544599, R2 Score (Train): 0.9950884231677677, RMSE (Train): 0.11771650562911028\n",
      "Test Loss: 0.6582722067832947, R2 Score (Test): 0.7481051540178805, RMSE (Test): 0.7872464656829834\n",
      "Epoch [1891/5000], Train Loss: 0.011573592239680389, R2 Score (Train): 0.9939798706759833, RMSE (Train): 0.13032562042567458\n",
      "Test Loss: 0.6123320460319519, R2 Score (Test): 0.7507128167087945, RMSE (Test): 0.7831608653068542\n",
      "Epoch [1896/5000], Train Loss: 0.012014338824277123, R2 Score (Train): 0.993101695886381, RMSE (Train): 0.13950766149769675\n",
      "Test Loss: 0.6155568659305573, R2 Score (Test): 0.7562309056504672, RMSE (Test): 0.7744446396827698\n",
      "Epoch [1901/5000], Train Loss: 0.010894258933452269, R2 Score (Train): 0.9943960988179911, RMSE (Train): 0.12573961460391925\n",
      "Test Loss: 0.6061750054359436, R2 Score (Test): 0.7443162232665123, RMSE (Test): 0.7931450605392456\n",
      "Epoch [1906/5000], Train Loss: 0.009461417833032707, R2 Score (Train): 0.9952574857885639, RMSE (Train): 0.11567279009982676\n",
      "Test Loss: 0.6073094606399536, R2 Score (Test): 0.7579979981548247, RMSE (Test): 0.7716324925422668\n",
      "Epoch [1911/5000], Train Loss: 0.011312694133569797, R2 Score (Train): 0.9930637459953912, RMSE (Train): 0.13989087445752227\n",
      "Test Loss: 0.5964214205741882, R2 Score (Test): 0.7512799126917238, RMSE (Test): 0.7822696566581726\n",
      "Epoch [1916/5000], Train Loss: 0.01622876284333567, R2 Score (Train): 0.9953648279988202, RMSE (Train): 0.11435622696203204\n",
      "Test Loss: 0.5876646637916565, R2 Score (Test): 0.7498929255021567, RMSE (Test): 0.7844477891921997\n",
      "Epoch [1921/5000], Train Loss: 0.009911095956340432, R2 Score (Train): 0.9943248039088561, RMSE (Train): 0.12653693965741414\n",
      "Test Loss: 0.6135792434215546, R2 Score (Test): 0.7488059803816336, RMSE (Test): 0.7861505150794983\n",
      "Epoch [1926/5000], Train Loss: 0.010005467765343687, R2 Score (Train): 0.9943001167450427, RMSE (Train): 0.1268118595275084\n",
      "Test Loss: 0.6163256764411926, R2 Score (Test): 0.753771495407238, RMSE (Test): 0.7783415913581848\n",
      "Epoch [1931/5000], Train Loss: 0.009783723236372074, R2 Score (Train): 0.9927191641806435, RMSE (Train): 0.14332353446557444\n",
      "Test Loss: 0.5970205664634705, R2 Score (Test): 0.7478883414357811, RMSE (Test): 0.7875850796699524\n",
      "Epoch [1936/5000], Train Loss: 0.012937224314858517, R2 Score (Train): 0.9944868886158708, RMSE (Train): 0.12471689044600436\n",
      "Test Loss: 0.5771002471446991, R2 Score (Test): 0.7541955220801526, RMSE (Test): 0.7776710391044617\n",
      "Epoch [1941/5000], Train Loss: 0.011606817599385977, R2 Score (Train): 0.9941171789991149, RMSE (Train): 0.1288308012167568\n",
      "Test Loss: 0.518733873963356, R2 Score (Test): 0.7544754011720625, RMSE (Test): 0.7772281765937805\n",
      "Epoch [1946/5000], Train Loss: 0.015021887918313345, R2 Score (Train): 0.9945794562365046, RMSE (Train): 0.12366543179420227\n",
      "Test Loss: 0.6232421696186066, R2 Score (Test): 0.7548985772878609, RMSE (Test): 0.7765581607818604\n",
      "Epoch [1951/5000], Train Loss: 0.009407639930335185, R2 Score (Train): 0.9938211209814669, RMSE (Train): 0.13203277057668136\n",
      "Test Loss: 0.6348069608211517, R2 Score (Test): 0.7579179659860098, RMSE (Test): 0.771760106086731\n",
      "Epoch [1956/5000], Train Loss: 0.010447410012905797, R2 Score (Train): 0.9934175862810164, RMSE (Train): 0.13627603159022847\n",
      "Test Loss: 0.5635708272457123, R2 Score (Test): 0.757672414053194, RMSE (Test): 0.7721514105796814\n",
      "Epoch [1961/5000], Train Loss: 0.009376372288291654, R2 Score (Train): 0.9934166793540744, RMSE (Train): 0.13628541934318747\n",
      "Test Loss: 0.5799092948436737, R2 Score (Test): 0.7579615080917925, RMSE (Test): 0.7716907262802124\n",
      "Epoch [1966/5000], Train Loss: 0.011481308300668994, R2 Score (Train): 0.9917810018584372, RMSE (Train): 0.1522777081294838\n",
      "Test Loss: 0.5745971202850342, R2 Score (Test): 0.7558075186148375, RMSE (Test): 0.7751169204711914\n",
      "Epoch [1971/5000], Train Loss: 0.010186673258431256, R2 Score (Train): 0.994558934592885, RMSE (Train): 0.12389930320858765\n",
      "Test Loss: 0.5670844912528992, R2 Score (Test): 0.7512324715248297, RMSE (Test): 0.7823442220687866\n",
      "Epoch [1976/5000], Train Loss: 0.013148261156554023, R2 Score (Train): 0.9919373073251659, RMSE (Train): 0.15082278076557076\n",
      "Test Loss: 0.5970920324325562, R2 Score (Test): 0.7619841111316604, RMSE (Test): 0.7652512192726135\n",
      "Epoch [1981/5000], Train Loss: 0.011626978171989322, R2 Score (Train): 0.992693023120887, RMSE (Train): 0.143580597818471\n",
      "Test Loss: 0.6027444899082184, R2 Score (Test): 0.7520804325621038, RMSE (Test): 0.7810097336769104\n",
      "Epoch [1986/5000], Train Loss: 0.012377197621390224, R2 Score (Train): 0.9922255829311427, RMSE (Train): 0.14810196075691306\n",
      "Test Loss: 0.6888458728790283, R2 Score (Test): 0.7493851763568762, RMSE (Test): 0.7852436304092407\n",
      "Epoch [1991/5000], Train Loss: 0.01633297566634913, R2 Score (Train): 0.9893040485837887, RMSE (Train): 0.17371471232382293\n",
      "Test Loss: 0.6168498694896698, R2 Score (Test): 0.7580499864679604, RMSE (Test): 0.771549642086029\n",
      "Epoch [1996/5000], Train Loss: 0.010645056065792838, R2 Score (Train): 0.9949312001881614, RMSE (Train): 0.1195857579263322\n",
      "Test Loss: 0.6217042207717896, R2 Score (Test): 0.7486479385419291, RMSE (Test): 0.7863978147506714\n",
      "Epoch [2001/5000], Train Loss: 0.009501052593501905, R2 Score (Train): 0.993477078738618, RMSE (Train): 0.13565879614442106\n",
      "Test Loss: 0.6184768676757812, R2 Score (Test): 0.7501234188092508, RMSE (Test): 0.7840861678123474\n",
      "Epoch [2006/5000], Train Loss: 0.011352377361617982, R2 Score (Train): 0.9955364065106325, RMSE (Train): 0.11221972714218338\n",
      "Test Loss: 0.5929582417011261, R2 Score (Test): 0.7531362899229983, RMSE (Test): 0.7793448567390442\n",
      "Epoch [2011/5000], Train Loss: 0.008219482784625143, R2 Score (Train): 0.9923850422931532, RMSE (Train): 0.14657524827719065\n",
      "Test Loss: 0.6329522430896759, R2 Score (Test): 0.753008502489323, RMSE (Test): 0.7795464992523193\n",
      "Epoch [2016/5000], Train Loss: 0.010331198262671629, R2 Score (Train): 0.9927195939626153, RMSE (Train): 0.14331930426578532\n",
      "Test Loss: 0.5708543211221695, R2 Score (Test): 0.7566901363691374, RMSE (Test): 0.7737147808074951\n",
      "Epoch [2021/5000], Train Loss: 0.011893145468396446, R2 Score (Train): 0.9917947582314208, RMSE (Train): 0.15215021897719241\n",
      "Test Loss: 0.5916622877120972, R2 Score (Test): 0.7570962056549553, RMSE (Test): 0.7730688452720642\n",
      "Epoch [2026/5000], Train Loss: 0.00983964690628151, R2 Score (Train): 0.9936808478520115, RMSE (Train): 0.13352306616586174\n",
      "Test Loss: 0.5596166700124741, R2 Score (Test): 0.7554023281405141, RMSE (Test): 0.7757596969604492\n",
      "Epoch [2031/5000], Train Loss: 0.009724001594198247, R2 Score (Train): 0.9914815386338481, RMSE (Train): 0.1550270452770629\n",
      "Test Loss: 0.6009242832660675, R2 Score (Test): 0.7523041347261232, RMSE (Test): 0.7806572914123535\n",
      "Epoch [2036/5000], Train Loss: 0.010363935143686831, R2 Score (Train): 0.9941930481774971, RMSE (Train): 0.1279973570239234\n",
      "Test Loss: 0.6613459289073944, R2 Score (Test): 0.7473410445508084, RMSE (Test): 0.7884395718574524\n",
      "Epoch [2041/5000], Train Loss: 0.011924399024186036, R2 Score (Train): 0.9899602628635177, RMSE (Train): 0.16830152908904678\n",
      "Test Loss: 0.5929616391658783, R2 Score (Test): 0.7573592638924783, RMSE (Test): 0.7726501226425171\n",
      "Epoch [2046/5000], Train Loss: 0.010544573538936675, R2 Score (Train): 0.9935720447152442, RMSE (Train): 0.13466766010298023\n",
      "Test Loss: 0.5400983542203903, R2 Score (Test): 0.7546977996760202, RMSE (Test): 0.7768761515617371\n",
      "Epoch [2051/5000], Train Loss: 0.01231414331899335, R2 Score (Train): 0.9939918197687524, RMSE (Train): 0.1301962173552857\n",
      "Test Loss: 0.5407484173774719, R2 Score (Test): 0.7605581554871013, RMSE (Test): 0.7675400376319885\n",
      "Epoch [2056/5000], Train Loss: 0.010124399870013198, R2 Score (Train): 0.993246463567025, RMSE (Train): 0.1380360471592595\n",
      "Test Loss: 0.6268891990184784, R2 Score (Test): 0.7462627669585706, RMSE (Test): 0.7901201248168945\n",
      "Epoch [2061/5000], Train Loss: 0.009462463669478893, R2 Score (Train): 0.9924379274979371, RMSE (Train): 0.1460653854578701\n",
      "Test Loss: 0.6251674890518188, R2 Score (Test): 0.7593338930971387, RMSE (Test): 0.7694997787475586\n",
      "Epoch [2066/5000], Train Loss: 0.01144257498284181, R2 Score (Train): 0.9945756056283177, RMSE (Train): 0.12370934829107794\n",
      "Test Loss: 0.6087170541286469, R2 Score (Test): 0.7555984217440878, RMSE (Test): 0.7754486203193665\n",
      "Epoch [2071/5000], Train Loss: 0.009994213158885637, R2 Score (Train): 0.9942082097231879, RMSE (Train): 0.12783015173188989\n",
      "Test Loss: 0.5742082893848419, R2 Score (Test): 0.7510427983929506, RMSE (Test): 0.7826424241065979\n",
      "Epoch [2076/5000], Train Loss: 0.011707037221640348, R2 Score (Train): 0.9948422920377896, RMSE (Train): 0.12062998246606468\n",
      "Test Loss: 0.6278172433376312, R2 Score (Test): 0.7502543012117862, RMSE (Test): 0.783880889415741\n",
      "Epoch [2081/5000], Train Loss: 0.01497319689951837, R2 Score (Train): 0.991788931148925, RMSE (Train): 0.1522042353335527\n",
      "Test Loss: 0.5623332858085632, R2 Score (Test): 0.7568249354678604, RMSE (Test): 0.7735004425048828\n",
      "Epoch [2086/5000], Train Loss: 0.008852493522378305, R2 Score (Train): 0.9936863473810437, RMSE (Train): 0.13346495126383753\n",
      "Test Loss: 0.6224909126758575, R2 Score (Test): 0.750609308438249, RMSE (Test): 0.7833235263824463\n",
      "Epoch [2091/5000], Train Loss: 0.01178994596314927, R2 Score (Train): 0.9946386302128954, RMSE (Train): 0.1229885756751228\n",
      "Test Loss: 0.6000135540962219, R2 Score (Test): 0.7587244900215145, RMSE (Test): 0.7704734206199646\n",
      "Epoch [2096/5000], Train Loss: 0.01142037163178126, R2 Score (Train): 0.9946129858780567, RMSE (Train): 0.12328236232417267\n",
      "Test Loss: 0.6224663853645325, R2 Score (Test): 0.750635791496964, RMSE (Test): 0.7832818627357483\n",
      "Epoch [2101/5000], Train Loss: 0.01690195349510759, R2 Score (Train): 0.991989753213891, RMSE (Train): 0.15033144739065987\n",
      "Test Loss: 0.5786855518817902, R2 Score (Test): 0.753738155042009, RMSE (Test): 0.7783942222595215\n",
      "Epoch [2106/5000], Train Loss: 0.011943741391102472, R2 Score (Train): 0.9934975017445309, RMSE (Train): 0.13544625851635814\n",
      "Test Loss: 0.5689203590154648, R2 Score (Test): 0.7586186861430746, RMSE (Test): 0.7706423401832581\n",
      "Epoch [2111/5000], Train Loss: 0.012869677428777019, R2 Score (Train): 0.991511605986528, RMSE (Train): 0.15475320631188633\n",
      "Test Loss: 0.5630430281162262, R2 Score (Test): 0.75947146272504, RMSE (Test): 0.7692798376083374\n",
      "Epoch [2116/5000], Train Loss: 0.010804622899740934, R2 Score (Train): 0.9925644679760329, RMSE (Train): 0.14483813195641906\n",
      "Test Loss: 0.6386075913906097, R2 Score (Test): 0.752424853624545, RMSE (Test): 0.7804670333862305\n",
      "Epoch [2121/5000], Train Loss: 0.01043358196814855, R2 Score (Train): 0.9945228807370806, RMSE (Train): 0.12430911927874662\n",
      "Test Loss: 0.6356528103351593, R2 Score (Test): 0.7516949241941957, RMSE (Test): 0.781616747379303\n",
      "Epoch [2126/5000], Train Loss: 0.010304873304752013, R2 Score (Train): 0.9921151692052501, RMSE (Train): 0.14914993885896705\n",
      "Test Loss: 0.6400682330131531, R2 Score (Test): 0.7529101934601624, RMSE (Test): 0.7797017097473145\n",
      "Epoch [2131/5000], Train Loss: 0.009031969277809063, R2 Score (Train): 0.9925702622356706, RMSE (Train): 0.1447816872192516\n",
      "Test Loss: 0.6116062998771667, R2 Score (Test): 0.7514665728603838, RMSE (Test): 0.7819761037826538\n",
      "Epoch [2136/5000], Train Loss: 0.01031805807724595, R2 Score (Train): 0.9925220631263619, RMSE (Train): 0.14525055082195573\n",
      "Test Loss: 0.57234326004982, R2 Score (Test): 0.7565623670934464, RMSE (Test): 0.7739179730415344\n",
      "Epoch [2141/5000], Train Loss: 0.012846521140697101, R2 Score (Train): 0.9933875303318567, RMSE (Train): 0.13658680213118063\n",
      "Test Loss: 0.512164369225502, R2 Score (Test): 0.7579048568938286, RMSE (Test): 0.7717810273170471\n",
      "Epoch [2146/5000], Train Loss: 0.012819012316564718, R2 Score (Train): 0.9947077799710272, RMSE (Train): 0.12219286193097773\n",
      "Test Loss: 0.5815791636705399, R2 Score (Test): 0.7516074273087008, RMSE (Test): 0.7817544341087341\n",
      "Epoch [2151/5000], Train Loss: 0.012191730551421642, R2 Score (Train): 0.992879600301928, RMSE (Train): 0.14173564265776398\n",
      "Test Loss: 0.6325255930423737, R2 Score (Test): 0.7530381049575879, RMSE (Test): 0.7794998288154602\n",
      "Epoch [2156/5000], Train Loss: 0.008858163841068745, R2 Score (Train): 0.9929197432451742, RMSE (Train): 0.14133554378519123\n",
      "Test Loss: 0.5568154454231262, R2 Score (Test): 0.7623626052358972, RMSE (Test): 0.7646425366401672\n",
      "Epoch [2161/5000], Train Loss: 0.013273962462941805, R2 Score (Train): 0.992337229757908, RMSE (Train): 0.14703468405383532\n",
      "Test Loss: 0.5914184749126434, R2 Score (Test): 0.7533843873589656, RMSE (Test): 0.7789531946182251\n",
      "Epoch [2166/5000], Train Loss: 0.008128807879984379, R2 Score (Train): 0.9949487788615893, RMSE (Train): 0.11937821523277178\n",
      "Test Loss: 0.5724925696849823, R2 Score (Test): 0.7538655339181548, RMSE (Test): 0.778192937374115\n",
      "Epoch [2171/5000], Train Loss: 0.008925953414291143, R2 Score (Train): 0.994606708327246, RMSE (Train): 0.12335417260212386\n",
      "Test Loss: 0.6117911040782928, R2 Score (Test): 0.7585154704055594, RMSE (Test): 0.770807147026062\n",
      "Epoch [2176/5000], Train Loss: 0.010381383161681393, R2 Score (Train): 0.9928750959872706, RMSE (Train): 0.14178046605436948\n",
      "Test Loss: 0.5764963328838348, R2 Score (Test): 0.7583692383032802, RMSE (Test): 0.7710404396057129\n",
      "Epoch [2181/5000], Train Loss: 0.010442515330699583, R2 Score (Train): 0.9911268085053736, RMSE (Train): 0.15822198101895313\n",
      "Test Loss: 0.6260623335838318, R2 Score (Test): 0.7567153290317968, RMSE (Test): 0.773674726486206\n",
      "Epoch [2186/5000], Train Loss: 0.012068153630631665, R2 Score (Train): 0.9920456152270655, RMSE (Train): 0.1498063381143493\n",
      "Test Loss: 0.6081302762031555, R2 Score (Test): 0.7518516971828131, RMSE (Test): 0.7813699245452881\n",
      "Epoch [2191/5000], Train Loss: 0.010510020268460115, R2 Score (Train): 0.992092221905206, RMSE (Train): 0.1493668174392771\n",
      "Test Loss: 0.6007419228553772, R2 Score (Test): 0.7514487996776693, RMSE (Test): 0.7820039987564087\n",
      "Epoch [2196/5000], Train Loss: 0.008024677090967694, R2 Score (Train): 0.9943859698852562, RMSE (Train): 0.12585319914723014\n",
      "Test Loss: 0.6137238442897797, R2 Score (Test): 0.7546146772343811, RMSE (Test): 0.7770076990127563\n",
      "Epoch [2201/5000], Train Loss: 0.012471850728616118, R2 Score (Train): 0.9921361216485932, RMSE (Train): 0.14895163818843393\n",
      "Test Loss: 0.6004711985588074, R2 Score (Test): 0.7529393884437238, RMSE (Test): 0.7796556353569031\n",
      "Epoch [2206/5000], Train Loss: 0.010519174897732833, R2 Score (Train): 0.9912284143858698, RMSE (Train): 0.15731348202994352\n",
      "Test Loss: 0.6487407982349396, R2 Score (Test): 0.7543216666062744, RMSE (Test): 0.7774714827537537\n",
      "Epoch [2211/5000], Train Loss: 0.00975496752653271, R2 Score (Train): 0.9931705747370277, RMSE (Train): 0.13880942942060073\n",
      "Test Loss: 0.6019349098205566, R2 Score (Test): 0.7585732322224722, RMSE (Test): 0.7707149386405945\n",
      "Epoch [2216/5000], Train Loss: 0.009720235752562681, R2 Score (Train): 0.9936048302520405, RMSE (Train): 0.1343237874747223\n",
      "Test Loss: 0.5666633099317551, R2 Score (Test): 0.756814581238326, RMSE (Test): 0.7735168933868408\n",
      "Epoch [2221/5000], Train Loss: 0.012611916909615198, R2 Score (Train): 0.9903975913884463, RMSE (Train): 0.1645951302628165\n",
      "Test Loss: 0.5849097967147827, R2 Score (Test): 0.755433983795087, RMSE (Test): 0.7757094502449036\n",
      "Epoch [2226/5000], Train Loss: 0.011487096780911088, R2 Score (Train): 0.994880039063156, RMSE (Train): 0.12018775262296576\n",
      "Test Loss: 0.6085164844989777, R2 Score (Test): 0.758986051349262, RMSE (Test): 0.7700557112693787\n",
      "Epoch [2231/5000], Train Loss: 0.007848957592311004, R2 Score (Train): 0.994688293476673, RMSE (Train): 0.12241761852218337\n",
      "Test Loss: 0.579760879278183, R2 Score (Test): 0.7555536715525284, RMSE (Test): 0.7755196690559387\n",
      "Epoch [2236/5000], Train Loss: 0.010593061141359309, R2 Score (Train): 0.9939511205941369, RMSE (Train): 0.13063644509877886\n",
      "Test Loss: 0.6334551870822906, R2 Score (Test): 0.756096901442033, RMSE (Test): 0.7746574878692627\n",
      "Epoch [2241/5000], Train Loss: 0.011911522209023436, R2 Score (Train): 0.9929104335939359, RMSE (Train): 0.14142843253392393\n",
      "Test Loss: 0.608033686876297, R2 Score (Test): 0.7525520160537928, RMSE (Test): 0.7802665829658508\n",
      "Epoch [2246/5000], Train Loss: 0.012483253881024817, R2 Score (Train): 0.9934504098836661, RMSE (Train): 0.13593583263295808\n",
      "Test Loss: 0.5783453285694122, R2 Score (Test): 0.7573486022102002, RMSE (Test): 0.7726671099662781\n",
      "Epoch [2251/5000], Train Loss: 0.014256525590705374, R2 Score (Train): 0.9949500352896463, RMSE (Train): 0.1193633673907239\n",
      "Test Loss: 0.6366302073001862, R2 Score (Test): 0.7593084793860111, RMSE (Test): 0.7695404887199402\n",
      "Epoch [2256/5000], Train Loss: 0.011846328619867563, R2 Score (Train): 0.9936421582769063, RMSE (Train): 0.13393119588335517\n",
      "Test Loss: 0.5503676533699036, R2 Score (Test): 0.7618284721513238, RMSE (Test): 0.7655013203620911\n",
      "Epoch [2261/5000], Train Loss: 0.014073352639873823, R2 Score (Train): 0.9948340660795357, RMSE (Train): 0.12072613970025461\n",
      "Test Loss: 0.5353486388921738, R2 Score (Test): 0.7621568944005936, RMSE (Test): 0.7649734020233154\n",
      "Epoch [2266/5000], Train Loss: 0.01661263274339338, R2 Score (Train): 0.9924285111234178, RMSE (Train): 0.14615629826464735\n",
      "Test Loss: 0.5584650039672852, R2 Score (Test): 0.7646867631605552, RMSE (Test): 0.7608941197395325\n",
      "Epoch [2271/5000], Train Loss: 0.010655016909974316, R2 Score (Train): 0.9928481478042693, RMSE (Train): 0.14204833773497744\n",
      "Test Loss: 0.5612979084253311, R2 Score (Test): 0.7617099655513251, RMSE (Test): 0.7656917572021484\n",
      "Epoch [2276/5000], Train Loss: 0.0093653101939708, R2 Score (Train): 0.9934835022092119, RMSE (Train): 0.13559198444401524\n",
      "Test Loss: 0.5785174667835236, R2 Score (Test): 0.7520753720657107, RMSE (Test): 0.7810177803039551\n",
      "Epoch [2281/5000], Train Loss: 0.01435169375812014, R2 Score (Train): 0.9905474059719533, RMSE (Train): 0.16330609489555314\n",
      "Test Loss: 0.6378880441188812, R2 Score (Test): 0.7600386395477085, RMSE (Test): 0.7683722972869873\n",
      "Epoch [2286/5000], Train Loss: 0.011302751741216829, R2 Score (Train): 0.9951390083906626, RMSE (Train): 0.11710874487112906\n",
      "Test Loss: 0.548836961388588, R2 Score (Test): 0.7558418307763727, RMSE (Test): 0.7750623822212219\n",
      "Epoch [2291/5000], Train Loss: 0.010918971810800334, R2 Score (Train): 0.9915207541944606, RMSE (Train): 0.1546697926441783\n",
      "Test Loss: 0.58179971575737, R2 Score (Test): 0.7608662594092412, RMSE (Test): 0.7670461535453796\n",
      "Epoch [2296/5000], Train Loss: 0.02307061688043177, R2 Score (Train): 0.9951316235918912, RMSE (Train): 0.11719766667705857\n",
      "Test Loss: 0.5673766881227493, R2 Score (Test): 0.7562523322990635, RMSE (Test): 0.774410605430603\n",
      "Epoch [2301/5000], Train Loss: 0.010763513351169726, R2 Score (Train): 0.993004153711511, RMSE (Train): 0.14049052001442522\n",
      "Test Loss: 0.6585108041763306, R2 Score (Test): 0.7625498571220782, RMSE (Test): 0.7643411755561829\n",
      "Epoch [2306/5000], Train Loss: 0.011446920533974966, R2 Score (Train): 0.9906290561303347, RMSE (Train): 0.16259925782319834\n",
      "Test Loss: 0.5857881307601929, R2 Score (Test): 0.7604215784574682, RMSE (Test): 0.7677589654922485\n",
      "Epoch [2311/5000], Train Loss: 0.009408678005759915, R2 Score (Train): 0.9945510698593346, RMSE (Train): 0.12398881537820508\n",
      "Test Loss: 0.5476666986942291, R2 Score (Test): 0.7558476224093466, RMSE (Test): 0.7750532031059265\n",
      "Epoch [2316/5000], Train Loss: 0.010091532138176262, R2 Score (Train): 0.9939943051376846, RMSE (Train): 0.13016928581427645\n",
      "Test Loss: 0.6206538081169128, R2 Score (Test): 0.7535536555821599, RMSE (Test): 0.7786857485771179\n",
      "Epoch [2321/5000], Train Loss: 0.009477575464795033, R2 Score (Train): 0.9910566158950417, RMSE (Train): 0.1588465666823097\n",
      "Test Loss: 0.5794894993305206, R2 Score (Test): 0.759283117360745, RMSE (Test): 0.7695809602737427\n",
      "Epoch [2326/5000], Train Loss: 0.008896728977560997, R2 Score (Train): 0.9947229063508555, RMSE (Train): 0.12201810934467973\n",
      "Test Loss: 0.6517015397548676, R2 Score (Test): 0.7494605574768509, RMSE (Test): 0.7851254940032959\n",
      "Epoch [2331/5000], Train Loss: 0.0075677189355095225, R2 Score (Train): 0.994631142497933, RMSE (Train): 0.12307442892948152\n",
      "Test Loss: 0.5817105919122696, R2 Score (Test): 0.7484739044020936, RMSE (Test): 0.7866699695587158\n",
      "Epoch [2336/5000], Train Loss: 0.011090823371584216, R2 Score (Train): 0.9947024596552329, RMSE (Train): 0.12225426728665335\n",
      "Test Loss: 0.6413485109806061, R2 Score (Test): 0.7429788849550142, RMSE (Test): 0.7952166199684143\n",
      "Epoch [2341/5000], Train Loss: 0.010001362728265425, R2 Score (Train): 0.9950402436206616, RMSE (Train): 0.11829245988710174\n",
      "Test Loss: 0.6886275410652161, R2 Score (Test): 0.7494514122822831, RMSE (Test): 0.785139799118042\n",
      "Epoch [2346/5000], Train Loss: 0.007897496068229279, R2 Score (Train): 0.9931437547856186, RMSE (Train): 0.13908172277908595\n",
      "Test Loss: 0.6311039924621582, R2 Score (Test): 0.7494709231544572, RMSE (Test): 0.785109281539917\n",
      "Epoch [2351/5000], Train Loss: 0.009068056281345585, R2 Score (Train): 0.9944527502191017, RMSE (Train): 0.1251024317192808\n",
      "Test Loss: 0.6251003742218018, R2 Score (Test): 0.7480328512206238, RMSE (Test): 0.787359356880188\n",
      "Epoch [2356/5000], Train Loss: 0.010529837959135572, R2 Score (Train): 0.9918566770862115, RMSE (Train): 0.1515750495782931\n",
      "Test Loss: 0.5898853242397308, R2 Score (Test): 0.7525459670365644, RMSE (Test): 0.7802761197090149\n",
      "Epoch [2361/5000], Train Loss: 0.009380391682498157, R2 Score (Train): 0.9938538674416291, RMSE (Train): 0.13168243602755328\n",
      "Test Loss: 0.5876015722751617, R2 Score (Test): 0.7487641608546072, RMSE (Test): 0.7862159609794617\n",
      "Epoch [2366/5000], Train Loss: 0.011137633778465291, R2 Score (Train): 0.9944137220885166, RMSE (Train): 0.12554174457522066\n",
      "Test Loss: 0.5558527112007141, R2 Score (Test): 0.7534765404524986, RMSE (Test): 0.7788075804710388\n",
      "Epoch [2371/5000], Train Loss: 0.00834393008456876, R2 Score (Train): 0.9928130019963415, RMSE (Train): 0.14239693870115747\n",
      "Test Loss: 0.5564802139997482, R2 Score (Test): 0.7617196918644814, RMSE (Test): 0.7656762003898621\n",
      "Epoch [2376/5000], Train Loss: 0.017400510027073324, R2 Score (Train): 0.9936987750696716, RMSE (Train): 0.13333353146759508\n",
      "Test Loss: 0.6045852601528168, R2 Score (Test): 0.7484230265142666, RMSE (Test): 0.786749541759491\n",
      "Epoch [2381/5000], Train Loss: 0.011411240401988229, R2 Score (Train): 0.9924068896703852, RMSE (Train): 0.14636483444355425\n",
      "Test Loss: 0.6560328900814056, R2 Score (Test): 0.7506214289111629, RMSE (Test): 0.7833045125007629\n",
      "Epoch [2386/5000], Train Loss: 0.011316006382306417, R2 Score (Train): 0.9945521794044395, RMSE (Train): 0.1239761910479594\n",
      "Test Loss: 0.6122622191905975, R2 Score (Test): 0.7549606915445126, RMSE (Test): 0.7764597535133362\n",
      "Epoch [2391/5000], Train Loss: 0.008792080256777505, R2 Score (Train): 0.9937743772022948, RMSE (Train): 0.13253124952480638\n",
      "Test Loss: 0.6021645069122314, R2 Score (Test): 0.7592142638799674, RMSE (Test): 0.7696910500526428\n",
      "Epoch [2396/5000], Train Loss: 0.009535149321891367, R2 Score (Train): 0.9938650910002198, RMSE (Train): 0.13156214730417248\n",
      "Test Loss: 0.5037832260131836, R2 Score (Test): 0.7649016628083102, RMSE (Test): 0.7605466246604919\n",
      "Epoch [2401/5000], Train Loss: 0.009400258500439426, R2 Score (Train): 0.9922930849782736, RMSE (Train): 0.14745760501133373\n",
      "Test Loss: 0.6099683344364166, R2 Score (Test): 0.7640962981221061, RMSE (Test): 0.7618481516838074\n",
      "Epoch [2406/5000], Train Loss: 0.009876266587525606, R2 Score (Train): 0.994803039130433, RMSE (Train): 0.12108814164884356\n",
      "Test Loss: 0.6359730362892151, R2 Score (Test): 0.7494769918868114, RMSE (Test): 0.7850997447967529\n",
      "Epoch [2411/5000], Train Loss: 0.012964808576119443, R2 Score (Train): 0.9903639197211933, RMSE (Train): 0.16488346117220062\n",
      "Test Loss: 0.6024282872676849, R2 Score (Test): 0.7545918690067727, RMSE (Test): 0.7770438194274902\n",
      "Epoch [2416/5000], Train Loss: 0.007948590093292296, R2 Score (Train): 0.9954977105351177, RMSE (Train): 0.11270510747052107\n",
      "Test Loss: 0.5743035972118378, R2 Score (Test): 0.751063641713315, RMSE (Test): 0.7826096415519714\n",
      "Epoch [2421/5000], Train Loss: 0.010370836632015804, R2 Score (Train): 0.9928381327542407, RMSE (Train): 0.1421477611765401\n",
      "Test Loss: 0.6596786826848984, R2 Score (Test): 0.7562930676171261, RMSE (Test): 0.774345874786377\n",
      "Epoch [2426/5000], Train Loss: 0.009440587755913535, R2 Score (Train): 0.993671776264152, RMSE (Train): 0.1336188726755719\n",
      "Test Loss: 0.5758518278598785, R2 Score (Test): 0.7570240307032278, RMSE (Test): 0.7731837034225464\n",
      "Epoch [2431/5000], Train Loss: 0.008778530134198567, R2 Score (Train): 0.9939049769446227, RMSE (Train): 0.13113377604226065\n",
      "Test Loss: 0.6349699795246124, R2 Score (Test): 0.7569844123380642, RMSE (Test): 0.7732467651367188\n",
      "Epoch [2436/5000], Train Loss: 0.009457278880290687, R2 Score (Train): 0.9935511685340142, RMSE (Train): 0.13488616407805623\n",
      "Test Loss: 0.635757565498352, R2 Score (Test): 0.751950448886179, RMSE (Test): 0.7812144756317139\n",
      "Epoch [2441/5000], Train Loss: 0.011820024345070124, R2 Score (Train): 0.9948503790346607, RMSE (Train): 0.12053537484250848\n",
      "Test Loss: 0.6400982141494751, R2 Score (Test): 0.753300238168886, RMSE (Test): 0.7790859937667847\n",
      "Epoch [2446/5000], Train Loss: 0.011022365263973674, R2 Score (Train): 0.9925546564066796, RMSE (Train): 0.1449336611586952\n",
      "Test Loss: 0.5810347497463226, R2 Score (Test): 0.7582125189391791, RMSE (Test): 0.7712904214859009\n",
      "Epoch [2451/5000], Train Loss: 0.01006056903861463, R2 Score (Train): 0.9941768182784811, RMSE (Train): 0.12817610266729032\n",
      "Test Loss: 0.5430856496095657, R2 Score (Test): 0.7558551170942713, RMSE (Test): 0.7750413417816162\n",
      "Epoch [2456/5000], Train Loss: 0.008838753177163502, R2 Score (Train): 0.994515627689196, RMSE (Train): 0.12439139991363739\n",
      "Test Loss: 0.5656713247299194, R2 Score (Test): 0.7597984822638894, RMSE (Test): 0.7687567472457886\n",
      "Epoch [2461/5000], Train Loss: 0.008309919018453607, R2 Score (Train): 0.9942197518855993, RMSE (Train): 0.12770271512631617\n",
      "Test Loss: 0.6005761623382568, R2 Score (Test): 0.7541938963448283, RMSE (Test): 0.777673602104187\n",
      "Epoch [2466/5000], Train Loss: 0.00864531386954089, R2 Score (Train): 0.9962837022436655, RMSE (Train): 0.10239579972639769\n",
      "Test Loss: 0.6118543148040771, R2 Score (Test): 0.7538262078888199, RMSE (Test): 0.778255045413971\n",
      "Epoch [2471/5000], Train Loss: 0.009145930603456994, R2 Score (Train): 0.9957825396093488, RMSE (Train): 0.10908182540264087\n",
      "Test Loss: 0.6391682922840118, R2 Score (Test): 0.7523211080264895, RMSE (Test): 0.7806305289268494\n",
      "Epoch [2476/5000], Train Loss: 0.008294475924534103, R2 Score (Train): 0.9929684790510164, RMSE (Train): 0.1408482736115859\n",
      "Test Loss: 0.6155950129032135, R2 Score (Test): 0.755032006913398, RMSE (Test): 0.776346743106842\n",
      "Epoch [2481/5000], Train Loss: 0.018060512918358047, R2 Score (Train): 0.9877214469229225, RMSE (Train): 0.1861231896804737\n",
      "Test Loss: 0.6337164044380188, R2 Score (Test): 0.7529916047257595, RMSE (Test): 0.7795732021331787\n",
      "Epoch [2486/5000], Train Loss: 0.012414525883893171, R2 Score (Train): 0.9915939017280554, RMSE (Train): 0.15400120608545176\n",
      "Test Loss: 0.5529260486364365, R2 Score (Test): 0.7549052146151569, RMSE (Test): 0.7765476107597351\n",
      "Epoch [2491/5000], Train Loss: 0.00922976597212255, R2 Score (Train): 0.9936371979958839, RMSE (Train): 0.13398343112863914\n",
      "Test Loss: 0.6285741329193115, R2 Score (Test): 0.755781143006067, RMSE (Test): 0.7751587629318237\n",
      "Epoch [2496/5000], Train Loss: 0.010536676699606081, R2 Score (Train): 0.9931637099267461, RMSE (Train): 0.13887917621644866\n",
      "Test Loss: 0.6009358465671539, R2 Score (Test): 0.7485914774940734, RMSE (Test): 0.7864860892295837\n",
      "Epoch [2501/5000], Train Loss: 0.00973230778860549, R2 Score (Train): 0.9934555977140972, RMSE (Train): 0.13588198561818696\n",
      "Test Loss: 0.6931193172931671, R2 Score (Test): 0.7483815907678871, RMSE (Test): 0.7868143320083618\n",
      "Epoch [2506/5000], Train Loss: 0.009306282678153366, R2 Score (Train): 0.9936998924737709, RMSE (Train): 0.13332170884247793\n",
      "Test Loss: 0.5022612512111664, R2 Score (Test): 0.7596131407699072, RMSE (Test): 0.7690532803535461\n",
      "Epoch [2511/5000], Train Loss: 0.008977412129752338, R2 Score (Train): 0.9920001135788283, RMSE (Train): 0.15023419741611205\n",
      "Test Loss: 0.6195134520530701, R2 Score (Test): 0.755675409619692, RMSE (Test): 0.7753264904022217\n",
      "Epoch [2516/5000], Train Loss: 0.006930452965510388, R2 Score (Train): 0.9922078437997808, RMSE (Train): 0.14827082892925628\n",
      "Test Loss: 0.5805457830429077, R2 Score (Test): 0.7538774061297024, RMSE (Test): 0.7781741619110107\n",
      "Epoch [2521/5000], Train Loss: 0.009446943877264857, R2 Score (Train): 0.9952659299736423, RMSE (Train): 0.11556976482877895\n",
      "Test Loss: 0.5374334305524826, R2 Score (Test): 0.7569209974174655, RMSE (Test): 0.7733476161956787\n",
      "Epoch [2526/5000], Train Loss: 0.011874824839954575, R2 Score (Train): 0.9944639261056509, RMSE (Train): 0.12497634802529582\n",
      "Test Loss: 0.606915146112442, R2 Score (Test): 0.7473748277466324, RMSE (Test): 0.7883867621421814\n",
      "Epoch [2531/5000], Train Loss: 0.009311311218577126, R2 Score (Train): 0.9927396739358465, RMSE (Train): 0.14312152444030177\n",
      "Test Loss: 0.6727413535118103, R2 Score (Test): 0.7554052100169337, RMSE (Test): 0.7757551074028015\n",
      "Epoch [2536/5000], Train Loss: 0.00717835477553308, R2 Score (Train): 0.9922523348376775, RMSE (Train): 0.1478469304446839\n",
      "Test Loss: 0.5899156033992767, R2 Score (Test): 0.7550050823119002, RMSE (Test): 0.7763893604278564\n",
      "Epoch [2541/5000], Train Loss: 0.010597344914761683, R2 Score (Train): 0.993775332898386, RMSE (Train): 0.132521076690496\n",
      "Test Loss: 0.5984902381896973, R2 Score (Test): 0.7580889760300843, RMSE (Test): 0.7714874744415283\n",
      "Epoch [2546/5000], Train Loss: 0.00949743560825785, R2 Score (Train): 0.9942574788435327, RMSE (Train): 0.1272852830387957\n",
      "Test Loss: 0.5742046535015106, R2 Score (Test): 0.7577040166412163, RMSE (Test): 0.7721010446548462\n",
      "Epoch [2551/5000], Train Loss: 0.007890504513246318, R2 Score (Train): 0.992870820041144, RMSE (Train): 0.14182300379962615\n",
      "Test Loss: 0.6129899621009827, R2 Score (Test): 0.7541758480859425, RMSE (Test): 0.7777021527290344\n",
      "Epoch [2556/5000], Train Loss: 0.009795219404622912, R2 Score (Train): 0.9934832156783219, RMSE (Train): 0.1355949654060409\n",
      "Test Loss: 0.6276946663856506, R2 Score (Test): 0.7563903216490051, RMSE (Test): 0.7741912603378296\n",
      "Epoch [2561/5000], Train Loss: 0.009947997711909315, R2 Score (Train): 0.9946568292310822, RMSE (Train): 0.12277965761105844\n",
      "Test Loss: 0.5698412954807281, R2 Score (Test): 0.7551218856483946, RMSE (Test): 0.7762042880058289\n",
      "Epoch [2566/5000], Train Loss: 0.007658400107175112, R2 Score (Train): 0.9939671009349818, RMSE (Train): 0.1304637688554334\n",
      "Test Loss: 0.6179554760456085, R2 Score (Test): 0.7575623338028826, RMSE (Test): 0.7723267674446106\n",
      "Epoch [2571/5000], Train Loss: 0.010915223198632399, R2 Score (Train): 0.9939629469833884, RMSE (Train): 0.1305086765290546\n",
      "Test Loss: 0.6155792474746704, R2 Score (Test): 0.7522964235826045, RMSE (Test): 0.7806693911552429\n",
      "Epoch [2576/5000], Train Loss: 0.009902904974296689, R2 Score (Train): 0.9943562691691442, RMSE (Train): 0.12618566969597883\n",
      "Test Loss: 0.6004854142665863, R2 Score (Test): 0.7612440930865598, RMSE (Test): 0.7664399147033691\n",
      "Epoch [2581/5000], Train Loss: 0.008361307865319153, R2 Score (Train): 0.9961385062405658, RMSE (Train): 0.10437693970179344\n",
      "Test Loss: 0.6003156304359436, R2 Score (Test): 0.7549706780782354, RMSE (Test): 0.7764438986778259\n",
      "Epoch [2586/5000], Train Loss: 0.008726299740374088, R2 Score (Train): 0.9944902981698868, RMSE (Train): 0.12467831923725009\n",
      "Test Loss: 0.5197004228830338, R2 Score (Test): 0.7675228711403322, RMSE (Test): 0.756294846534729\n",
      "Epoch [2591/5000], Train Loss: 0.010087900833847621, R2 Score (Train): 0.992809667923032, RMSE (Train): 0.1424299640884102\n",
      "Test Loss: 0.6208384037017822, R2 Score (Test): 0.7581583836454984, RMSE (Test): 0.7713767886161804\n",
      "Epoch [2596/5000], Train Loss: 0.009216131137994429, R2 Score (Train): 0.9925702925718385, RMSE (Train): 0.14478139164178933\n",
      "Test Loss: 0.6244047582149506, R2 Score (Test): 0.758992487497832, RMSE (Test): 0.7700453996658325\n",
      "Epoch [2601/5000], Train Loss: 0.007358433251890044, R2 Score (Train): 0.9940101821942202, RMSE (Train): 0.1299971098308183\n",
      "Test Loss: 0.5361841320991516, R2 Score (Test): 0.7593164010978105, RMSE (Test): 0.769527792930603\n",
      "Epoch [2606/5000], Train Loss: 0.008501832548063248, R2 Score (Train): 0.993348353185746, RMSE (Train): 0.13699082504753898\n",
      "Test Loss: 0.5816010534763336, R2 Score (Test): 0.7524727849354647, RMSE (Test): 0.7803915143013\n",
      "Epoch [2611/5000], Train Loss: 0.008712667467383048, R2 Score (Train): 0.9943650940314351, RMSE (Train): 0.12608697551271703\n",
      "Test Loss: 0.618186354637146, R2 Score (Test): 0.7501479546414607, RMSE (Test): 0.7840477228164673\n",
      "Epoch [2616/5000], Train Loss: 0.009755627873043219, R2 Score (Train): 0.9913103926897081, RMSE (Train): 0.15657663885972237\n",
      "Test Loss: 0.58480504155159, R2 Score (Test): 0.7554804606389878, RMSE (Test): 0.7756357789039612\n",
      "Epoch [2621/5000], Train Loss: 0.010166367166675627, R2 Score (Train): 0.9911470541208868, RMSE (Train): 0.1580413734723381\n",
      "Test Loss: 0.6043508648872375, R2 Score (Test): 0.7630001635244458, RMSE (Test): 0.7636160850524902\n",
      "Epoch [2626/5000], Train Loss: 0.007829659851267934, R2 Score (Train): 0.9933432381985463, RMSE (Train): 0.1370434865673507\n",
      "Test Loss: 0.5826528668403625, R2 Score (Test): 0.7636191549103171, RMSE (Test): 0.7626182436943054\n",
      "Epoch [2631/5000], Train Loss: 0.011961484289107224, R2 Score (Train): 0.9955735327011662, RMSE (Train): 0.11175205568657282\n",
      "Test Loss: 0.6273722648620605, R2 Score (Test): 0.7598439294402878, RMSE (Test): 0.7686839699745178\n",
      "Epoch [2636/5000], Train Loss: 0.008794473173717657, R2 Score (Train): 0.9932974331189744, RMSE (Train): 0.13751417535762578\n",
      "Test Loss: 0.5568955987691879, R2 Score (Test): 0.7616109430338301, RMSE (Test): 0.7658509016036987\n",
      "Epoch [2641/5000], Train Loss: 0.013199806058158478, R2 Score (Train): 0.9879428336704552, RMSE (Train): 0.18443762310375542\n",
      "Test Loss: 0.6034913957118988, R2 Score (Test): 0.7621828841245665, RMSE (Test): 0.7649315595626831\n",
      "Epoch [2646/5000], Train Loss: 0.00859170756302774, R2 Score (Train): 0.9940895139277405, RMSE (Train): 0.1291333714382526\n",
      "Test Loss: 0.5700430274009705, R2 Score (Test): 0.7585282271765648, RMSE (Test): 0.7707867622375488\n",
      "Epoch [2651/5000], Train Loss: 0.011464618030004203, R2 Score (Train): 0.9927710795186002, RMSE (Train): 0.14281164261660162\n",
      "Test Loss: 0.6312199234962463, R2 Score (Test): 0.7530576216432403, RMSE (Test): 0.7794690132141113\n",
      "Epoch [2656/5000], Train Loss: 0.01098297229812791, R2 Score (Train): 0.9953154045553688, RMSE (Train): 0.1149642833876155\n",
      "Test Loss: 0.6206067800521851, R2 Score (Test): 0.7440123495096386, RMSE (Test): 0.7936162352561951\n",
      "Epoch [2661/5000], Train Loss: 0.008316884244171282, R2 Score (Train): 0.9949579975974385, RMSE (Train): 0.11926922982256223\n",
      "Test Loss: 0.5749302506446838, R2 Score (Test): 0.7555883942320719, RMSE (Test): 0.7754645347595215\n",
      "Epoch [2666/5000], Train Loss: 0.008163363439962268, R2 Score (Train): 0.9945693092014577, RMSE (Train): 0.12378112597759099\n",
      "Test Loss: 0.5902629792690277, R2 Score (Test): 0.750461686422693, RMSE (Test): 0.7835553288459778\n",
      "Epoch [2671/5000], Train Loss: 0.01763830993634959, R2 Score (Train): 0.994008838689199, RMSE (Train): 0.1300116880686749\n",
      "Test Loss: 0.6029607951641083, R2 Score (Test): 0.7521052992048836, RMSE (Test): 0.7809705138206482\n",
      "Epoch [2676/5000], Train Loss: 0.008111136422182122, R2 Score (Train): 0.9916035022794228, RMSE (Train): 0.15391323906267781\n",
      "Test Loss: 0.5871661007404327, R2 Score (Test): 0.750033890091315, RMSE (Test): 0.784226655960083\n",
      "Epoch [2681/5000], Train Loss: 0.011409348420177897, R2 Score (Train): 0.9934454811143493, RMSE (Train): 0.1359869709750672\n",
      "Test Loss: 0.5589501559734344, R2 Score (Test): 0.7569453268533541, RMSE (Test): 0.7733089327812195\n",
      "Epoch [2686/5000], Train Loss: 0.009249760827515274, R2 Score (Train): 0.9942603175521295, RMSE (Train): 0.1272538185929459\n",
      "Test Loss: 0.5729497820138931, R2 Score (Test): 0.7476352033698781, RMSE (Test): 0.7879804372787476\n",
      "Epoch [2691/5000], Train Loss: 0.010200364418172589, R2 Score (Train): 0.9951604302859198, RMSE (Train): 0.11685041679099092\n",
      "Test Loss: 0.6553036272525787, R2 Score (Test): 0.7463232087627106, RMSE (Test): 0.790026068687439\n",
      "Epoch [2696/5000], Train Loss: 0.008292387705296278, R2 Score (Train): 0.9939527778446441, RMSE (Train): 0.1306185482181181\n",
      "Test Loss: 0.5727624446153641, R2 Score (Test): 0.7516966532872881, RMSE (Test): 0.7816140651702881\n",
      "Epoch [2701/5000], Train Loss: 0.007907037623226643, R2 Score (Train): 0.9928149481799384, RMSE (Train): 0.14237765739970415\n",
      "Test Loss: 0.5501577407121658, R2 Score (Test): 0.7544901716266446, RMSE (Test): 0.7772048115730286\n",
      "Epoch [2706/5000], Train Loss: 0.010616864077746868, R2 Score (Train): 0.9951673399131155, RMSE (Train): 0.1167669712310315\n",
      "Test Loss: 0.6028691530227661, R2 Score (Test): 0.7490981481441426, RMSE (Test): 0.7856931686401367\n",
      "Epoch [2711/5000], Train Loss: 0.008941542590036988, R2 Score (Train): 0.991395203955556, RMSE (Train): 0.15581066499461996\n",
      "Test Loss: 0.6059466302394867, R2 Score (Test): 0.7554867751740284, RMSE (Test): 0.7756257653236389\n",
      "Epoch [2716/5000], Train Loss: 0.009988761623390019, R2 Score (Train): 0.9937430802350796, RMSE (Train): 0.1328639573084725\n",
      "Test Loss: 0.6311709880828857, R2 Score (Test): 0.7559651917596231, RMSE (Test): 0.7748666405677795\n",
      "Epoch [2721/5000], Train Loss: 0.009410632580208281, R2 Score (Train): 0.9948720985300732, RMSE (Train): 0.12028091593980794\n",
      "Test Loss: 0.5369113087654114, R2 Score (Test): 0.7534234105621548, RMSE (Test): 0.7788915038108826\n",
      "Epoch [2726/5000], Train Loss: 0.00878780420559148, R2 Score (Train): 0.995074167870512, RMSE (Train): 0.11788721128928185\n",
      "Test Loss: 0.5822508037090302, R2 Score (Test): 0.7572273815061222, RMSE (Test): 0.7728601098060608\n",
      "Epoch [2731/5000], Train Loss: 0.008571905937666694, R2 Score (Train): 0.9941348561206655, RMSE (Train): 0.1286370955864727\n",
      "Test Loss: 0.5598080605268478, R2 Score (Test): 0.7585666341242392, RMSE (Test): 0.770725429058075\n",
      "Epoch [2736/5000], Train Loss: 0.00812464413077881, R2 Score (Train): 0.9928352465893283, RMSE (Train): 0.1421764003946389\n",
      "Test Loss: 0.5893328487873077, R2 Score (Test): 0.7572380886350811, RMSE (Test): 0.772843062877655\n",
      "Epoch [2741/5000], Train Loss: 0.009776873320030669, R2 Score (Train): 0.9953414376232443, RMSE (Train): 0.11464440063641465\n",
      "Test Loss: 0.6104047894477844, R2 Score (Test): 0.7565852158616048, RMSE (Test): 0.7738816142082214\n",
      "Epoch [2746/5000], Train Loss: 0.016552559837388497, R2 Score (Train): 0.9905388018809542, RMSE (Train): 0.16338040153028235\n",
      "Test Loss: 0.5951652228832245, R2 Score (Test): 0.7535520324377738, RMSE (Test): 0.7786883115768433\n",
      "Epoch [2751/5000], Train Loss: 0.008849701766545573, R2 Score (Train): 0.9927070210810479, RMSE (Train): 0.14344300336567023\n",
      "Test Loss: 0.579127162694931, R2 Score (Test): 0.7478700813170851, RMSE (Test): 0.7876136302947998\n",
      "Epoch [2756/5000], Train Loss: 0.008335628546774387, R2 Score (Train): 0.9932296175429155, RMSE (Train): 0.13820809852178353\n",
      "Test Loss: 0.6286795139312744, R2 Score (Test): 0.7568571939179198, RMSE (Test): 0.7734491229057312\n",
      "Epoch [2761/5000], Train Loss: 0.007656919886358082, R2 Score (Train): 0.9940374178873584, RMSE (Train): 0.12970122476257698\n",
      "Test Loss: 0.6077894568443298, R2 Score (Test): 0.7470088004075308, RMSE (Test): 0.7889577746391296\n",
      "Epoch [2766/5000], Train Loss: 0.008047506329603493, R2 Score (Train): 0.9945645050580816, RMSE (Train): 0.12383586402500212\n",
      "Test Loss: 0.6524189114570618, R2 Score (Test): 0.7510615719053113, RMSE (Test): 0.7826129198074341\n",
      "Epoch [2771/5000], Train Loss: 0.010781360731925815, R2 Score (Train): 0.9926702995695216, RMSE (Train): 0.1438036810777391\n",
      "Test Loss: 0.623997151851654, R2 Score (Test): 0.7513155405910015, RMSE (Test): 0.782213568687439\n",
      "Epoch [2776/5000], Train Loss: 0.00980669620912522, R2 Score (Train): 0.9935412283528067, RMSE (Train): 0.13499008031824042\n",
      "Test Loss: 0.6297668218612671, R2 Score (Test): 0.7488254540052366, RMSE (Test): 0.7861200571060181\n",
      "Epoch [2781/5000], Train Loss: 0.008870600024238229, R2 Score (Train): 0.9923719900569284, RMSE (Train): 0.14670081139585256\n",
      "Test Loss: 0.6385807991027832, R2 Score (Test): 0.7496277335175429, RMSE (Test): 0.7848635315895081\n",
      "Epoch [2786/5000], Train Loss: 0.010103085854401192, R2 Score (Train): 0.9925105803776614, RMSE (Train): 0.14536202781195093\n",
      "Test Loss: 0.6064574718475342, R2 Score (Test): 0.7469577820466161, RMSE (Test): 0.78903728723526\n",
      "Epoch [2791/5000], Train Loss: 0.01082739590977629, R2 Score (Train): 0.9924297486292631, RMSE (Train): 0.146144353674393\n",
      "Test Loss: 0.645967036485672, R2 Score (Test): 0.7486759634207395, RMSE (Test): 0.7863538861274719\n",
      "Epoch [2796/5000], Train Loss: 0.007829717903708419, R2 Score (Train): 0.9933713566718874, RMSE (Train): 0.13675374121908904\n",
      "Test Loss: 0.6476772427558899, R2 Score (Test): 0.7565911678034974, RMSE (Test): 0.7738720774650574\n",
      "Epoch [2801/5000], Train Loss: 0.008939964793777714, R2 Score (Train): 0.9938319749651173, RMSE (Train): 0.13191675346180193\n",
      "Test Loss: 0.5821230113506317, R2 Score (Test): 0.7553113451404551, RMSE (Test): 0.7759039402008057\n",
      "Epoch [2806/5000], Train Loss: 0.00850935245398432, R2 Score (Train): 0.9949015352884277, RMSE (Train): 0.1199351822862987\n",
      "Test Loss: 0.5839730799198151, R2 Score (Test): 0.7558665895463225, RMSE (Test): 0.7750231027603149\n",
      "Epoch [2811/5000], Train Loss: 0.008092703802200655, R2 Score (Train): 0.994816090706161, RMSE (Train): 0.12093599652199481\n",
      "Test Loss: 0.5966999232769012, R2 Score (Test): 0.7482238016048515, RMSE (Test): 0.7870609760284424\n",
      "Epoch [2816/5000], Train Loss: 0.008786828334753713, R2 Score (Train): 0.9925179959122412, RMSE (Train): 0.14529004599252093\n",
      "Test Loss: 0.575054332613945, R2 Score (Test): 0.7476776428485064, RMSE (Test): 0.7879141569137573\n",
      "Epoch [2821/5000], Train Loss: 0.012549958618668219, R2 Score (Train): 0.9925105473221616, RMSE (Train): 0.14536234859843472\n",
      "Test Loss: 0.587577611207962, R2 Score (Test): 0.7593620007544324, RMSE (Test): 0.7694548964500427\n",
      "Epoch [2826/5000], Train Loss: 0.011255876277573407, R2 Score (Train): 0.9955998215285802, RMSE (Train): 0.11141971329502819\n",
      "Test Loss: 0.593557596206665, R2 Score (Test): 0.7539858307868107, RMSE (Test): 0.7780026793479919\n",
      "Epoch [2831/5000], Train Loss: 0.010225743482199809, R2 Score (Train): 0.9947690139805095, RMSE (Train): 0.1214838845315788\n",
      "Test Loss: 0.5541502833366394, R2 Score (Test): 0.7474028804199317, RMSE (Test): 0.788343071937561\n",
      "Epoch [2836/5000], Train Loss: 0.011816643178462982, R2 Score (Train): 0.9921310497380267, RMSE (Train): 0.14899966459351544\n",
      "Test Loss: 0.5372322350740433, R2 Score (Test): 0.7518520285988914, RMSE (Test): 0.7813694477081299\n",
      "Epoch [2841/5000], Train Loss: 0.009840102128994962, R2 Score (Train): 0.9951662423010681, RMSE (Train): 0.11678023075638219\n",
      "Test Loss: 0.6089352965354919, R2 Score (Test): 0.7568968031855519, RMSE (Test): 0.7733861804008484\n",
      "Epoch [2846/5000], Train Loss: 0.01074044476263225, R2 Score (Train): 0.9954736787024261, RMSE (Train): 0.11300549970686065\n",
      "Test Loss: 0.5420168787240982, R2 Score (Test): 0.75622964855263, RMSE (Test): 0.7744466066360474\n",
      "Epoch [2851/5000], Train Loss: 0.008670400672902664, R2 Score (Train): 0.9949538481176359, RMSE (Train): 0.11931829797426195\n",
      "Test Loss: 0.5922108590602875, R2 Score (Test): 0.7512947106450045, RMSE (Test): 0.7822463512420654\n",
      "Epoch [2856/5000], Train Loss: 0.007243214621363829, R2 Score (Train): 0.9942237036156779, RMSE (Train): 0.1276590549813026\n",
      "Test Loss: 0.7030071914196014, R2 Score (Test): 0.7494474584332604, RMSE (Test): 0.7851460576057434\n",
      "Epoch [2861/5000], Train Loss: 0.009065295259157816, R2 Score (Train): 0.9941924690747695, RMSE (Train): 0.1280037391821464\n",
      "Test Loss: 0.6262989044189453, R2 Score (Test): 0.7432338315715796, RMSE (Test): 0.7948221564292908\n",
      "Epoch [2866/5000], Train Loss: 0.00658752901169161, R2 Score (Train): 0.9963519719393853, RMSE (Train): 0.10145091674192863\n",
      "Test Loss: 0.6467984914779663, R2 Score (Test): 0.7470192714312454, RMSE (Test): 0.7889414429664612\n",
      "Epoch [2871/5000], Train Loss: 0.009021862448814014, R2 Score (Train): 0.995866510121839, RMSE (Train): 0.10799044478436158\n",
      "Test Loss: 0.6138178408145905, R2 Score (Test): 0.7485721277636073, RMSE (Test): 0.7865163087844849\n",
      "Epoch [2876/5000], Train Loss: 0.007746155276739349, R2 Score (Train): 0.9957653585120977, RMSE (Train): 0.109303788436189\n",
      "Test Loss: 0.6744861602783203, R2 Score (Test): 0.748572205888165, RMSE (Test): 0.7865161895751953\n",
      "Epoch [2881/5000], Train Loss: 0.006607923229845862, R2 Score (Train): 0.9938456869423509, RMSE (Train): 0.13177004151388916\n",
      "Test Loss: 0.6265063881874084, R2 Score (Test): 0.7562102484049237, RMSE (Test): 0.7744774222373962\n",
      "Epoch [2886/5000], Train Loss: 0.008119632334758839, R2 Score (Train): 0.9939494997457159, RMSE (Train): 0.13065394649688533\n",
      "Test Loss: 0.5890441536903381, R2 Score (Test): 0.7591702555554751, RMSE (Test): 0.769761323928833\n",
      "Epoch [2891/5000], Train Loss: 0.013665769326811036, R2 Score (Train): 0.9927323762198315, RMSE (Train): 0.14319343566741696\n",
      "Test Loss: 0.6054635047912598, R2 Score (Test): 0.7546439922803767, RMSE (Test): 0.7769613265991211\n",
      "Epoch [2896/5000], Train Loss: 0.007156980029928188, R2 Score (Train): 0.9957365586207195, RMSE (Train): 0.10967484736394545\n",
      "Test Loss: 0.61650151014328, R2 Score (Test): 0.7480962799414033, RMSE (Test): 0.7872602939605713\n",
      "Epoch [2901/5000], Train Loss: 0.007581729597101609, R2 Score (Train): 0.9937002003484564, RMSE (Train): 0.1333184512091377\n",
      "Test Loss: 0.5468699485063553, R2 Score (Test): 0.7559167384991152, RMSE (Test): 0.774943470954895\n",
      "Epoch [2906/5000], Train Loss: 0.008023733622394502, R2 Score (Train): 0.994321732159008, RMSE (Train): 0.1265711796411712\n",
      "Test Loss: 0.5778771340847015, R2 Score (Test): 0.7539544384263772, RMSE (Test): 0.7780523896217346\n",
      "Epoch [2911/5000], Train Loss: 0.00909901522876074, R2 Score (Train): 0.9954215924216242, RMSE (Train): 0.11365384073981111\n",
      "Test Loss: 0.5912952423095703, R2 Score (Test): 0.7510479825049585, RMSE (Test): 0.7826342582702637\n",
      "Epoch [2916/5000], Train Loss: 0.00663147441809997, R2 Score (Train): 0.9930991146683504, RMSE (Train): 0.1395337596521103\n",
      "Test Loss: 0.5301232635974884, R2 Score (Test): 0.7585846762139518, RMSE (Test): 0.7706966400146484\n",
      "Epoch [2921/5000], Train Loss: 0.009590632321002582, R2 Score (Train): 0.9934351829396173, RMSE (Train): 0.13609375745136773\n",
      "Test Loss: 0.5698940604925156, R2 Score (Test): 0.7566875764738542, RMSE (Test): 0.7737188339233398\n",
      "Epoch [2926/5000], Train Loss: 0.011343549587763846, R2 Score (Train): 0.9941496416860488, RMSE (Train): 0.12847485128177868\n",
      "Test Loss: 0.6626837849617004, R2 Score (Test): 0.7492117444875452, RMSE (Test): 0.785515308380127\n",
      "Epoch [2931/5000], Train Loss: 0.010657842775496343, R2 Score (Train): 0.9913115445330435, RMSE (Train): 0.15656626107575067\n",
      "Test Loss: 0.6564207673072815, R2 Score (Test): 0.7585381002035141, RMSE (Test): 0.7707709670066833\n",
      "Epoch [2936/5000], Train Loss: 0.007810573442839086, R2 Score (Train): 0.9936467791138672, RMSE (Train): 0.1338825168868956\n",
      "Test Loss: 0.668808788061142, R2 Score (Test): 0.7532594590542822, RMSE (Test): 0.7791503667831421\n",
      "Epoch [2941/5000], Train Loss: 0.006399672556047638, R2 Score (Train): 0.9933661954165377, RMSE (Train): 0.13680697108859446\n",
      "Test Loss: 0.6371647119522095, R2 Score (Test): 0.7518896317951621, RMSE (Test): 0.7813102602958679\n",
      "Epoch [2946/5000], Train Loss: 0.0075904669938609, R2 Score (Train): 0.9949864084174121, RMSE (Train): 0.11893272427395551\n",
      "Test Loss: 0.6028762757778168, R2 Score (Test): 0.7503990843061399, RMSE (Test): 0.7836535573005676\n",
      "Epoch [2951/5000], Train Loss: 0.012453003922322145, R2 Score (Train): 0.9892680137072543, RMSE (Train): 0.17400709046260704\n",
      "Test Loss: 0.601885586977005, R2 Score (Test): 0.7549862782297199, RMSE (Test): 0.7764192223548889\n",
      "Epoch [2956/5000], Train Loss: 0.007647856953553855, R2 Score (Train): 0.9948477088873023, RMSE (Train): 0.12056662039486893\n",
      "Test Loss: 0.6500466167926788, R2 Score (Test): 0.7398093543306865, RMSE (Test): 0.8001047968864441\n",
      "Epoch [2961/5000], Train Loss: 0.008455309881052623, R2 Score (Train): 0.9949813030204786, RMSE (Train): 0.11899326413396713\n",
      "Test Loss: 0.5490751415491104, R2 Score (Test): 0.752523538260768, RMSE (Test): 0.7803115248680115\n",
      "Epoch [2966/5000], Train Loss: 0.008279344959494969, R2 Score (Train): 0.9925575262588996, RMSE (Train): 0.14490572569317836\n",
      "Test Loss: 0.638263463973999, R2 Score (Test): 0.7542913966307501, RMSE (Test): 0.7775194048881531\n",
      "Epoch [2971/5000], Train Loss: 0.008502964822885891, R2 Score (Train): 0.9939052285611133, RMSE (Train): 0.13113106926324097\n",
      "Test Loss: 0.6141410171985626, R2 Score (Test): 0.7501590745753628, RMSE (Test): 0.7840302586555481\n",
      "Epoch [2976/5000], Train Loss: 0.0075571082610016065, R2 Score (Train): 0.9955590697996947, RMSE (Train): 0.11193447439493502\n",
      "Test Loss: 0.5992273390293121, R2 Score (Test): 0.7524006316709818, RMSE (Test): 0.7805051803588867\n",
      "Epoch [2981/5000], Train Loss: 0.008314199881472936, R2 Score (Train): 0.9937483577943008, RMSE (Train): 0.13280791173393056\n",
      "Test Loss: 0.5972000956535339, R2 Score (Test): 0.7525498574010665, RMSE (Test): 0.780269980430603\n",
      "Epoch [2986/5000], Train Loss: 0.008177457105678817, R2 Score (Train): 0.9966178962304995, RMSE (Train): 0.0976833072218669\n",
      "Test Loss: 0.6174627542495728, R2 Score (Test): 0.7558396660628629, RMSE (Test): 0.7750658392906189\n",
      "Epoch [2991/5000], Train Loss: 0.007096371186586718, R2 Score (Train): 0.9936443145581829, RMSE (Train): 0.13390848237199016\n",
      "Test Loss: 0.6066382825374603, R2 Score (Test): 0.7528411646324836, RMSE (Test): 0.7798105478286743\n",
      "Epoch [2996/5000], Train Loss: 0.009241012468313178, R2 Score (Train): 0.9910716963755442, RMSE (Train): 0.15871258533915342\n",
      "Test Loss: 0.5990497767925262, R2 Score (Test): 0.7587137226814847, RMSE (Test): 0.7704905867576599\n",
      "Epoch [3001/5000], Train Loss: 0.007499576197005808, R2 Score (Train): 0.9929415158342013, RMSE (Train): 0.14111806508516014\n",
      "Test Loss: 0.59432253241539, R2 Score (Test): 0.7499535488599096, RMSE (Test): 0.784352719783783\n",
      "Epoch [3006/5000], Train Loss: 0.009152922585296134, R2 Score (Train): 0.9939509476352011, RMSE (Train): 0.13063831276529086\n",
      "Test Loss: 0.6734672486782074, R2 Score (Test): 0.7494959184078529, RMSE (Test): 0.7850701212882996\n",
      "Epoch [3011/5000], Train Loss: 0.006562837748788297, R2 Score (Train): 0.9940565433406109, RMSE (Train): 0.12949304422630906\n",
      "Test Loss: 0.5626712590456009, R2 Score (Test): 0.7561325111110906, RMSE (Test): 0.7746008634567261\n",
      "Epoch [3016/5000], Train Loss: 0.008603561262134463, R2 Score (Train): 0.9937424595552535, RMSE (Train): 0.13287054712710472\n",
      "Test Loss: 0.6503704488277435, R2 Score (Test): 0.7486909884447916, RMSE (Test): 0.7863304018974304\n",
      "Epoch [3021/5000], Train Loss: 0.006926469776468973, R2 Score (Train): 0.9945232456855367, RMSE (Train): 0.12430497776066794\n",
      "Test Loss: 0.5799134820699692, R2 Score (Test): 0.744621305153439, RMSE (Test): 0.7926717400550842\n",
      "Epoch [3026/5000], Train Loss: 0.009241186334596327, R2 Score (Train): 0.9942096704294813, RMSE (Train): 0.12781403114804127\n",
      "Test Loss: 0.5636201947927475, R2 Score (Test): 0.7519314470500963, RMSE (Test): 0.7812443971633911\n",
      "Epoch [3031/5000], Train Loss: 0.008357969694770873, R2 Score (Train): 0.9939772539852111, RMSE (Train): 0.130353940813922\n",
      "Test Loss: 0.6319523751735687, R2 Score (Test): 0.7567351227401732, RMSE (Test): 0.7736432552337646\n",
      "Epoch [3036/5000], Train Loss: 0.008329731683867673, R2 Score (Train): 0.9933234361948672, RMSE (Train): 0.13724716821851982\n",
      "Test Loss: 0.5759934186935425, R2 Score (Test): 0.7570692748257306, RMSE (Test): 0.7731117010116577\n",
      "Epoch [3041/5000], Train Loss: 0.01010702042064319, R2 Score (Train): 0.9944037160863832, RMSE (Train): 0.12565412789928784\n",
      "Test Loss: 0.5613515675067902, R2 Score (Test): 0.7523988468857805, RMSE (Test): 0.7805079817771912\n",
      "Epoch [3046/5000], Train Loss: 0.008444775477983057, R2 Score (Train): 0.9942785961744064, RMSE (Train): 0.127051030431189\n",
      "Test Loss: 0.5921985656023026, R2 Score (Test): 0.7450863575183129, RMSE (Test): 0.7919496893882751\n",
      "Epoch [3051/5000], Train Loss: 0.00876521683918933, R2 Score (Train): 0.9926747751906725, RMSE (Train): 0.14375977006921642\n",
      "Test Loss: 0.6574282348155975, R2 Score (Test): 0.7538523506089737, RMSE (Test): 0.7782136797904968\n",
      "Epoch [3056/5000], Train Loss: 0.006751795706804842, R2 Score (Train): 0.9951673089891588, RMSE (Train): 0.11676734482351613\n",
      "Test Loss: 0.6607479453086853, R2 Score (Test): 0.7488355358886722, RMSE (Test): 0.7861042618751526\n",
      "Epoch [3061/5000], Train Loss: 0.007677913080745687, R2 Score (Train): 0.993523523194563, RMSE (Train): 0.13517497500017114\n",
      "Test Loss: 0.6616722345352173, R2 Score (Test): 0.7444561343187241, RMSE (Test): 0.7929280400276184\n",
      "Epoch [3066/5000], Train Loss: 0.010621016457056006, R2 Score (Train): 0.9897924881354904, RMSE (Train): 0.16970195182182904\n",
      "Test Loss: 0.641258955001831, R2 Score (Test): 0.7569572820828454, RMSE (Test): 0.7732899188995361\n",
      "Epoch [3071/5000], Train Loss: 0.009796608122996986, R2 Score (Train): 0.9947528005901927, RMSE (Train): 0.12167200793227474\n",
      "Test Loss: 0.5894450396299362, R2 Score (Test): 0.7458388848546627, RMSE (Test): 0.7907798290252686\n",
      "Epoch [3076/5000], Train Loss: 0.009604183297293881, R2 Score (Train): 0.9947541635379081, RMSE (Train): 0.12165620489694613\n",
      "Test Loss: 0.6230038404464722, R2 Score (Test): 0.7440996540062081, RMSE (Test): 0.7934809327125549\n",
      "Epoch [3081/5000], Train Loss: 0.005982170308319231, R2 Score (Train): 0.9947483062648677, RMSE (Train): 0.12172410396490972\n",
      "Test Loss: 0.6081034243106842, R2 Score (Test): 0.749890870381013, RMSE (Test): 0.7844510078430176\n",
      "Epoch [3086/5000], Train Loss: 0.0064420663596441346, R2 Score (Train): 0.9944533474947875, RMSE (Train): 0.12509569661133932\n",
      "Test Loss: 0.5453697592020035, R2 Score (Test): 0.7545737463083712, RMSE (Test): 0.777072548866272\n",
      "Epoch [3091/5000], Train Loss: 0.008042939123697579, R2 Score (Train): 0.9944869332992572, RMSE (Train): 0.1247163850340947\n",
      "Test Loss: 0.6721326261758804, R2 Score (Test): 0.7525354663192074, RMSE (Test): 0.7802926301956177\n",
      "Epoch [3096/5000], Train Loss: 0.0115826918821161, R2 Score (Train): 0.992860699310753, RMSE (Train): 0.14192363552307088\n",
      "Test Loss: 0.5690774321556091, R2 Score (Test): 0.7578842558160989, RMSE (Test): 0.7718138694763184\n",
      "Epoch [3101/5000], Train Loss: 0.010714880501230558, R2 Score (Train): 0.9914439714477915, RMSE (Train): 0.15536851087213163\n",
      "Test Loss: 0.5816508531570435, R2 Score (Test): 0.759543679729281, RMSE (Test): 0.7691643238067627\n",
      "Epoch [3106/5000], Train Loss: 0.007236115906077127, R2 Score (Train): 0.9943537817612144, RMSE (Train): 0.12621347405698172\n",
      "Test Loss: 0.6378357112407684, R2 Score (Test): 0.7493597037191616, RMSE (Test): 0.7852835059165955\n",
      "Epoch [3111/5000], Train Loss: 0.008119747896368304, R2 Score (Train): 0.9922360382953369, RMSE (Train): 0.14800234036847748\n",
      "Test Loss: 0.6259964108467102, R2 Score (Test): 0.7577445055953197, RMSE (Test): 0.7720365524291992\n",
      "Epoch [3116/5000], Train Loss: 0.007308175166447957, R2 Score (Train): 0.9946344416296207, RMSE (Train): 0.12303660885840051\n",
      "Test Loss: 0.6933231055736542, R2 Score (Test): 0.7548009497321864, RMSE (Test): 0.7767127752304077\n",
      "Epoch [3121/5000], Train Loss: 0.008166041574440897, R2 Score (Train): 0.9908520391124731, RMSE (Train): 0.16065307410845958\n",
      "Test Loss: 0.6207951307296753, R2 Score (Test): 0.7550908858168656, RMSE (Test): 0.7762534618377686\n",
      "Epoch [3126/5000], Train Loss: 0.008114368072710931, R2 Score (Train): 0.9945676795870124, RMSE (Train): 0.12379969639416011\n",
      "Test Loss: 0.6094463467597961, R2 Score (Test): 0.7563998245172322, RMSE (Test): 0.774176299571991\n",
      "Epoch [3131/5000], Train Loss: 0.008037410870504877, R2 Score (Train): 0.9955687955972169, RMSE (Train): 0.11181183694230762\n",
      "Test Loss: 0.650969922542572, R2 Score (Test): 0.7571772503735551, RMSE (Test): 0.7729398608207703\n",
      "Epoch [3136/5000], Train Loss: 0.006865569448564202, R2 Score (Train): 0.9932416878505699, RMSE (Train): 0.13808484415092837\n",
      "Test Loss: 0.5758887529373169, R2 Score (Test): 0.7580780860943379, RMSE (Test): 0.7715047597885132\n",
      "Epoch [3141/5000], Train Loss: 0.00913200449819366, R2 Score (Train): 0.9934449010345564, RMSE (Train): 0.1359929883159352\n",
      "Test Loss: 0.5710080564022064, R2 Score (Test): 0.759037295181788, RMSE (Test): 0.7699737548828125\n",
      "Epoch [3146/5000], Train Loss: 0.007514727534726262, R2 Score (Train): 0.9922997119423204, RMSE (Train): 0.1473941940096249\n",
      "Test Loss: 0.6096254885196686, R2 Score (Test): 0.765223450872081, RMSE (Test): 0.7600259184837341\n",
      "Epoch [3151/5000], Train Loss: 0.008447691410159072, R2 Score (Train): 0.9962066986410678, RMSE (Train): 0.10345120745039492\n",
      "Test Loss: 0.5959902107715607, R2 Score (Test): 0.7543249060956676, RMSE (Test): 0.777466356754303\n",
      "Epoch [3156/5000], Train Loss: 0.008845953503623605, R2 Score (Train): 0.9951183446023592, RMSE (Train): 0.11735739208495333\n",
      "Test Loss: 0.5941720902919769, R2 Score (Test): 0.7514542226031867, RMSE (Test): 0.7819954752922058\n",
      "Epoch [3161/5000], Train Loss: 0.00792670133523643, R2 Score (Train): 0.9937747888030418, RMSE (Train): 0.1325268683677129\n",
      "Test Loss: 0.5674483776092529, R2 Score (Test): 0.7654004553949516, RMSE (Test): 0.7597392797470093\n",
      "Epoch [3166/5000], Train Loss: 0.009133272843124965, R2 Score (Train): 0.9927095151198789, RMSE (Train): 0.1434184740878869\n",
      "Test Loss: 0.5781719386577606, R2 Score (Test): 0.7586199136962009, RMSE (Test): 0.7706403732299805\n",
      "Epoch [3171/5000], Train Loss: 0.011366670136339962, R2 Score (Train): 0.9941755813148387, RMSE (Train): 0.12818971556549014\n",
      "Test Loss: 0.6109075248241425, R2 Score (Test): 0.7598152361134618, RMSE (Test): 0.7687299251556396\n",
      "Epoch [3176/5000], Train Loss: 0.008375321437294284, R2 Score (Train): 0.992773816134452, RMSE (Train): 0.1427846083173057\n",
      "Test Loss: 0.5830377042293549, R2 Score (Test): 0.7637647452343639, RMSE (Test): 0.7623833417892456\n",
      "Epoch [3181/5000], Train Loss: 0.013140615851928791, R2 Score (Train): 0.9919422767648046, RMSE (Train): 0.15077629380021856\n",
      "Test Loss: 0.5900838673114777, R2 Score (Test): 0.7534762902096133, RMSE (Test): 0.7788079977035522\n",
      "Epoch [3186/5000], Train Loss: 0.009240850418185195, R2 Score (Train): 0.9944331044828342, RMSE (Train): 0.1253237627655476\n",
      "Test Loss: 0.6730924546718597, R2 Score (Test): 0.7523868416721521, RMSE (Test): 0.780526876449585\n",
      "Epoch [3191/5000], Train Loss: 0.008856638373496631, R2 Score (Train): 0.9952966733933025, RMSE (Train): 0.11519389407195936\n",
      "Test Loss: 0.5723582208156586, R2 Score (Test): 0.7598763087860946, RMSE (Test): 0.768632173538208\n",
      "Epoch [3196/5000], Train Loss: 0.007580543635413051, R2 Score (Train): 0.9942147958769125, RMSE (Train): 0.12775744981496878\n",
      "Test Loss: 0.5739252716302872, R2 Score (Test): 0.7549674845739569, RMSE (Test): 0.7764489650726318\n",
      "Epoch [3201/5000], Train Loss: 0.00735226667408521, R2 Score (Train): 0.9953460358996413, RMSE (Train): 0.11458780626788957\n",
      "Test Loss: 0.5603128969669342, R2 Score (Test): 0.7633723311031175, RMSE (Test): 0.7630162239074707\n",
      "Epoch [3206/5000], Train Loss: 0.00831341720186174, R2 Score (Train): 0.993790909680295, RMSE (Train): 0.13235516059369185\n",
      "Test Loss: 0.6178449094295502, R2 Score (Test): 0.758744233058562, RMSE (Test): 0.7704418897628784\n",
      "Epoch [3211/5000], Train Loss: 0.00926431167560319, R2 Score (Train): 0.9911607908758451, RMSE (Train): 0.15791871269603358\n",
      "Test Loss: 0.6065094769001007, R2 Score (Test): 0.7677737890334907, RMSE (Test): 0.7558865547180176\n",
      "Epoch [3216/5000], Train Loss: 0.006749033326438318, R2 Score (Train): 0.9937956730107408, RMSE (Train): 0.13230438243671466\n",
      "Test Loss: 0.5670380592346191, R2 Score (Test): 0.7621064994282294, RMSE (Test): 0.7650544047355652\n",
      "Epoch [3221/5000], Train Loss: 0.006229132103423278, R2 Score (Train): 0.9938995328434982, RMSE (Train): 0.13119232760034355\n",
      "Test Loss: 0.5967080891132355, R2 Score (Test): 0.7634241041433589, RMSE (Test): 0.7629328370094299\n",
      "Epoch [3226/5000], Train Loss: 0.0074343706558768945, R2 Score (Train): 0.9936874354284578, RMSE (Train): 0.1334534505953037\n",
      "Test Loss: 0.5531006157398224, R2 Score (Test): 0.7633921447782542, RMSE (Test): 0.7629843354225159\n",
      "Epoch [3231/5000], Train Loss: 0.008849698312891027, R2 Score (Train): 0.9940457624058254, RMSE (Train): 0.12961043580802237\n",
      "Test Loss: 0.6643148511648178, R2 Score (Test): 0.7606644067799979, RMSE (Test): 0.7673697471618652\n",
      "Epoch [3236/5000], Train Loss: 0.007392750044042866, R2 Score (Train): 0.9935275489516154, RMSE (Train): 0.13513295630567532\n",
      "Test Loss: 0.635315328836441, R2 Score (Test): 0.7604907376494144, RMSE (Test): 0.7676481604576111\n",
      "Epoch [3241/5000], Train Loss: 0.00768605712801218, R2 Score (Train): 0.9915281228579015, RMSE (Train): 0.1546025721985833\n",
      "Test Loss: 0.5950424671173096, R2 Score (Test): 0.7606429215986437, RMSE (Test): 0.7674041986465454\n",
      "Epoch [3246/5000], Train Loss: 0.007845763582736254, R2 Score (Train): 0.9950256804652846, RMSE (Train): 0.11846600155031416\n",
      "Test Loss: 0.5532586723566055, R2 Score (Test): 0.7616164199288369, RMSE (Test): 0.7658420205116272\n",
      "Epoch [3251/5000], Train Loss: 0.007884803519118577, R2 Score (Train): 0.9952392155828413, RMSE (Train): 0.11589538660562174\n",
      "Test Loss: 0.550327405333519, R2 Score (Test): 0.7589657071167282, RMSE (Test): 0.770088255405426\n",
      "Epoch [3256/5000], Train Loss: 0.007333177141845226, R2 Score (Train): 0.9939582602809849, RMSE (Train): 0.13055932513706206\n",
      "Test Loss: 0.5767699480056763, R2 Score (Test): 0.7599004159888866, RMSE (Test): 0.7685936093330383\n",
      "Epoch [3261/5000], Train Loss: 0.007877400183739761, R2 Score (Train): 0.9970346034090591, RMSE (Train): 0.09146781035610503\n",
      "Test Loss: 0.5736128091812134, R2 Score (Test): 0.7566200202315787, RMSE (Test): 0.7738263010978699\n",
      "Epoch [3266/5000], Train Loss: 0.007105737070863445, R2 Score (Train): 0.9950983193404057, RMSE (Train): 0.1175978542982944\n",
      "Test Loss: 0.5957916080951691, R2 Score (Test): 0.7630552892556519, RMSE (Test): 0.7635272741317749\n",
      "Epoch [3271/5000], Train Loss: 0.008531276059026519, R2 Score (Train): 0.9931498412226725, RMSE (Train): 0.13901997614701647\n",
      "Test Loss: 0.5650085955858231, R2 Score (Test): 0.7607614818092885, RMSE (Test): 0.7672141194343567\n",
      "Epoch [3276/5000], Train Loss: 0.008926244142154852, R2 Score (Train): 0.9947438236904697, RMSE (Train): 0.12177604158706264\n",
      "Test Loss: 0.630963146686554, R2 Score (Test): 0.7645909564760905, RMSE (Test): 0.7610489726066589\n",
      "Epoch [3281/5000], Train Loss: 0.011240671893271307, R2 Score (Train): 0.9942741648754604, RMSE (Train): 0.12710022221556025\n",
      "Test Loss: 0.5776092112064362, R2 Score (Test): 0.760338151378059, RMSE (Test): 0.767892599105835\n",
      "Epoch [3286/5000], Train Loss: 0.006141646415926516, R2 Score (Train): 0.9942027459929816, RMSE (Train): 0.12789043230044955\n",
      "Test Loss: 0.6945611536502838, R2 Score (Test): 0.7562900738104231, RMSE (Test): 0.7743505835533142\n",
      "Epoch [3291/5000], Train Loss: 0.009400597870505104, R2 Score (Train): 0.9929202417416613, RMSE (Train): 0.14133056822323875\n",
      "Test Loss: 0.5644932687282562, R2 Score (Test): 0.7546403193101809, RMSE (Test): 0.7769671082496643\n",
      "Epoch [3296/5000], Train Loss: 0.009001570171676576, R2 Score (Train): 0.9935664345831363, RMSE (Train): 0.1347264142912824\n",
      "Test Loss: 0.6784011721611023, R2 Score (Test): 0.7479732235194776, RMSE (Test): 0.7874525785446167\n",
      "Epoch [3301/5000], Train Loss: 0.009677531973769268, R2 Score (Train): 0.9923545275236444, RMSE (Train): 0.1468686338918635\n",
      "Test Loss: 0.5581887066364288, R2 Score (Test): 0.7545167411184043, RMSE (Test): 0.7771627306938171\n",
      "Epoch [3306/5000], Train Loss: 0.0074410571639115615, R2 Score (Train): 0.9942730952976598, RMSE (Train): 0.1271120927309742\n",
      "Test Loss: 0.6590338051319122, R2 Score (Test): 0.7574700029057206, RMSE (Test): 0.7724738717079163\n",
      "Epoch [3311/5000], Train Loss: 0.010269049981919428, R2 Score (Train): 0.9950634379642123, RMSE (Train): 0.11801553789543841\n",
      "Test Loss: 0.595527857542038, R2 Score (Test): 0.7601237543950797, RMSE (Test): 0.7682360410690308\n",
      "Epoch [3316/5000], Train Loss: 0.007918195255721608, R2 Score (Train): 0.9937947698718067, RMSE (Train): 0.1323140115948047\n",
      "Test Loss: 0.5547628402709961, R2 Score (Test): 0.7612848375634179, RMSE (Test): 0.7663744688034058\n",
      "Epoch [3321/5000], Train Loss: 0.006624210897522668, R2 Score (Train): 0.9946403884284706, RMSE (Train): 0.12296840749335636\n",
      "Test Loss: 0.5856609344482422, R2 Score (Test): 0.7582717885273688, RMSE (Test): 0.7711958885192871\n",
      "Epoch [3326/5000], Train Loss: 0.008516758408707878, R2 Score (Train): 0.9941246121413769, RMSE (Train): 0.1287493844645029\n",
      "Test Loss: 0.5557887554168701, R2 Score (Test): 0.7627944550158463, RMSE (Test): 0.7639474272727966\n",
      "Epoch [3331/5000], Train Loss: 0.006588821648620069, R2 Score (Train): 0.9937034519002087, RMSE (Train): 0.13328404155984475\n",
      "Test Loss: 0.6026398837566376, R2 Score (Test): 0.7544500408106591, RMSE (Test): 0.7772683501243591\n",
      "Epoch [3336/5000], Train Loss: 0.007710309854398171, R2 Score (Train): 0.9970307446226466, RMSE (Train): 0.0915273032409717\n",
      "Test Loss: 0.6195577383041382, R2 Score (Test): 0.7521457224002392, RMSE (Test): 0.7809069156646729\n",
      "Epoch [3341/5000], Train Loss: 0.007054097174356381, R2 Score (Train): 0.9951630553025621, RMSE (Train): 0.11681872224872307\n",
      "Test Loss: 0.5841879546642303, R2 Score (Test): 0.7527756299000418, RMSE (Test): 0.7799139618873596\n",
      "Epoch [3346/5000], Train Loss: 0.008233648375608027, R2 Score (Train): 0.9943068120161871, RMSE (Train): 0.12673735894390262\n",
      "Test Loss: 0.574812650680542, R2 Score (Test): 0.7516444458526207, RMSE (Test): 0.7816961407661438\n",
      "Epoch [3351/5000], Train Loss: 0.008810617728158832, R2 Score (Train): 0.9949500199922507, RMSE (Train): 0.11936354817884638\n",
      "Test Loss: 0.6904104650020599, R2 Score (Test): 0.7545202346317057, RMSE (Test): 0.7771573066711426\n",
      "Epoch [3356/5000], Train Loss: 0.008191892489170035, R2 Score (Train): 0.9958867005304012, RMSE (Train): 0.10772637733245664\n",
      "Test Loss: 0.5646564364433289, R2 Score (Test): 0.7508435142789781, RMSE (Test): 0.7829556465148926\n",
      "Epoch [3361/5000], Train Loss: 0.007926125079393387, R2 Score (Train): 0.9941836043877926, RMSE (Train): 0.1281013951752416\n",
      "Test Loss: 0.5840045362710953, R2 Score (Test): 0.7461598427584544, RMSE (Test): 0.7902804017066956\n",
      "Epoch [3366/5000], Train Loss: 0.0083272639894858, R2 Score (Train): 0.9958785016400007, RMSE (Train): 0.10783368742192108\n",
      "Test Loss: 0.62318354845047, R2 Score (Test): 0.7481854825677288, RMSE (Test): 0.7871208786964417\n",
      "Epoch [3371/5000], Train Loss: 0.006951428717002273, R2 Score (Train): 0.9931896416023246, RMSE (Train): 0.13861552506870836\n",
      "Test Loss: 0.5708968341350555, R2 Score (Test): 0.7579598831145209, RMSE (Test): 0.771693229675293\n",
      "Epoch [3376/5000], Train Loss: 0.009630667317348221, R2 Score (Train): 0.9917350324773276, RMSE (Train): 0.15270296380773096\n",
      "Test Loss: 0.6809812784194946, R2 Score (Test): 0.7493694511508112, RMSE (Test): 0.785268247127533\n",
      "Epoch [3381/5000], Train Loss: 0.007802219828590751, R2 Score (Train): 0.9942975083309468, RMSE (Train): 0.1268408724030934\n",
      "Test Loss: 0.6492156386375427, R2 Score (Test): 0.7511443932609425, RMSE (Test): 0.7824826836585999\n",
      "Epoch [3386/5000], Train Loss: 0.012230332242324948, R2 Score (Train): 0.9943103416899524, RMSE (Train): 0.126698065415423\n",
      "Test Loss: 0.5427779108285904, R2 Score (Test): 0.7531136644855789, RMSE (Test): 0.7793805599212646\n",
      "Epoch [3391/5000], Train Loss: 0.006642633331163476, R2 Score (Train): 0.9940277082504737, RMSE (Train): 0.12980678637147244\n",
      "Test Loss: 0.6513063907623291, R2 Score (Test): 0.7526608168210349, RMSE (Test): 0.7800949811935425\n",
      "Epoch [3396/5000], Train Loss: 0.0071421407628804445, R2 Score (Train): 0.993884998674311, RMSE (Train): 0.13134851540381656\n",
      "Test Loss: 0.6745863407850266, R2 Score (Test): 0.7516754786803881, RMSE (Test): 0.7816473245620728\n",
      "Epoch [3401/5000], Train Loss: 0.008738259707267085, R2 Score (Train): 0.9946780356425764, RMSE (Train): 0.12253576642223131\n",
      "Test Loss: 0.6383054852485657, R2 Score (Test): 0.7558519011696886, RMSE (Test): 0.7750464081764221\n",
      "Epoch [3406/5000], Train Loss: 0.0055092920471603675, R2 Score (Train): 0.9934914364151083, RMSE (Train): 0.13550941383454723\n",
      "Test Loss: 0.6311011612415314, R2 Score (Test): 0.7453688305960222, RMSE (Test): 0.7915107011795044\n",
      "Epoch [3411/5000], Train Loss: 0.00740433728788048, R2 Score (Train): 0.9913270419467779, RMSE (Train): 0.15642656679596123\n",
      "Test Loss: 0.6850294172763824, R2 Score (Test): 0.7502770109454253, RMSE (Test): 0.7838451862335205\n",
      "Epoch [3416/5000], Train Loss: 0.008663688669912517, R2 Score (Train): 0.9938152834843937, RMSE (Train): 0.13209512484482766\n",
      "Test Loss: 0.6126392781734467, R2 Score (Test): 0.7596126598822674, RMSE (Test): 0.7690539956092834\n",
      "Epoch [3421/5000], Train Loss: 0.006989760169138511, R2 Score (Train): 0.9931532541869459, RMSE (Train): 0.13898533977280816\n",
      "Test Loss: 0.5884175598621368, R2 Score (Test): 0.7539509454149813, RMSE (Test): 0.778057873249054\n",
      "Epoch [3426/5000], Train Loss: 0.005887474825916191, R2 Score (Train): 0.9948998561875265, RMSE (Train): 0.11995493006392985\n",
      "Test Loss: 0.661698579788208, R2 Score (Test): 0.7491653867171277, RMSE (Test): 0.7855879068374634\n",
      "Epoch [3431/5000], Train Loss: 0.007694375080366929, R2 Score (Train): 0.9934572444924449, RMSE (Train): 0.13586488844257882\n",
      "Test Loss: 0.584607869386673, R2 Score (Test): 0.7533908973620088, RMSE (Test): 0.778942883014679\n",
      "Epoch [3436/5000], Train Loss: 0.006427804531995207, R2 Score (Train): 0.9931334143004897, RMSE (Train): 0.13918656373737887\n",
      "Test Loss: 0.6237283051013947, R2 Score (Test): 0.7528892236608645, RMSE (Test): 0.77973473072052\n",
      "Epoch [3441/5000], Train Loss: 0.008218239760026336, R2 Score (Train): 0.9907025399907294, RMSE (Train): 0.16196047812715175\n",
      "Test Loss: 0.5874325335025787, R2 Score (Test): 0.7542741295428455, RMSE (Test): 0.7775467038154602\n",
      "Epoch [3446/5000], Train Loss: 0.012529347091913223, R2 Score (Train): 0.9912148794443247, RMSE (Train): 0.15743480604916346\n",
      "Test Loss: 0.6431538164615631, R2 Score (Test): 0.7545797515536775, RMSE (Test): 0.7770630121231079\n",
      "Epoch [3451/5000], Train Loss: 0.007546961889602244, R2 Score (Train): 0.9938372235531605, RMSE (Train): 0.13186061522909495\n",
      "Test Loss: 0.6378933191299438, R2 Score (Test): 0.7469414772985753, RMSE (Test): 0.7890627384185791\n",
      "Epoch [3456/5000], Train Loss: 0.005547009583096951, R2 Score (Train): 0.9959152845200361, RMSE (Train): 0.10735142069525971\n",
      "Test Loss: 0.6410247087478638, R2 Score (Test): 0.7497352062008544, RMSE (Test): 0.784695029258728\n",
      "Epoch [3461/5000], Train Loss: 0.008024100951539973, R2 Score (Train): 0.9932461840997596, RMSE (Train): 0.13803890315610806\n",
      "Test Loss: 0.6792330592870712, R2 Score (Test): 0.7540706937909858, RMSE (Test): 0.7778685688972473\n",
      "Epoch [3466/5000], Train Loss: 0.007629255026889344, R2 Score (Train): 0.9932679847499392, RMSE (Train): 0.1378159351131544\n",
      "Test Loss: 0.5946133136749268, R2 Score (Test): 0.7510282510990514, RMSE (Test): 0.7826653122901917\n",
      "Epoch [3471/5000], Train Loss: 0.008544898397910098, R2 Score (Train): 0.9925481275866579, RMSE (Train): 0.144997193380356\n",
      "Test Loss: 0.6378658711910248, R2 Score (Test): 0.7470736610193214, RMSE (Test): 0.7888566255569458\n",
      "Epoch [3476/5000], Train Loss: 0.00663858112723877, R2 Score (Train): 0.9942048261894832, RMSE (Train): 0.12786748513451648\n",
      "Test Loss: 0.6138591468334198, R2 Score (Test): 0.7511429253200813, RMSE (Test): 0.7824850082397461\n",
      "Epoch [3481/5000], Train Loss: 0.008924742850164572, R2 Score (Train): 0.9936361219689462, RMSE (Train): 0.13399475976076525\n",
      "Test Loss: 0.6145120859146118, R2 Score (Test): 0.7467499505275977, RMSE (Test): 0.789361298084259\n",
      "Epoch [3486/5000], Train Loss: 0.008004382097472748, R2 Score (Train): 0.9932841688915148, RMSE (Train): 0.13765017681810135\n",
      "Test Loss: 0.6331239342689514, R2 Score (Test): 0.7430303004779686, RMSE (Test): 0.7951370477676392\n",
      "Epoch [3491/5000], Train Loss: 0.007353905161532263, R2 Score (Train): 0.9957507156196104, RMSE (Train): 0.10949260517645766\n",
      "Test Loss: 0.6314773261547089, R2 Score (Test): 0.7519195883074969, RMSE (Test): 0.7812630534172058\n",
      "Epoch [3496/5000], Train Loss: 0.008488817800146839, R2 Score (Train): 0.9924072091508679, RMSE (Train): 0.1463617552574361\n",
      "Test Loss: 0.6198671758174896, R2 Score (Test): 0.7449502725892818, RMSE (Test): 0.7921609878540039\n",
      "Epoch [3501/5000], Train Loss: 0.006447339081205428, R2 Score (Train): 0.9942467331895798, RMSE (Train): 0.12740431824223086\n",
      "Test Loss: 0.572116881608963, R2 Score (Test): 0.7519134185217565, RMSE (Test): 0.7812727093696594\n",
      "Epoch [3506/5000], Train Loss: 0.007145803130697459, R2 Score (Train): 0.9936829314154098, RMSE (Train): 0.13350105160574144\n",
      "Test Loss: 0.5918630361557007, R2 Score (Test): 0.7542973916896312, RMSE (Test): 0.777509868144989\n",
      "Epoch [3511/5000], Train Loss: 0.007849695781866709, R2 Score (Train): 0.9941745440910087, RMSE (Train): 0.1282011291945539\n",
      "Test Loss: 0.629510909318924, R2 Score (Test): 0.7472528715400719, RMSE (Test): 0.7885770797729492\n",
      "Epoch [3516/5000], Train Loss: 0.007815939645903805, R2 Score (Train): 0.9940832990339251, RMSE (Train): 0.12920124566395572\n",
      "Test Loss: 0.5900965631008148, R2 Score (Test): 0.7522916634798311, RMSE (Test): 0.7806769013404846\n",
      "Epoch [3521/5000], Train Loss: 0.006503292223593841, R2 Score (Train): 0.9924055470471418, RMSE (Train): 0.14637777407791053\n",
      "Test Loss: 0.5754354000091553, R2 Score (Test): 0.7578858731985464, RMSE (Test): 0.7718112468719482\n",
      "Epoch [3526/5000], Train Loss: 0.006932814101067682, R2 Score (Train): 0.9917909071846928, RMSE (Train): 0.15218591986722382\n",
      "Test Loss: 0.6013058423995972, R2 Score (Test): 0.7521401466949791, RMSE (Test): 0.7809157371520996\n",
      "Epoch [3531/5000], Train Loss: 0.008054343440259496, R2 Score (Train): 0.9942084038627623, RMSE (Train): 0.12782800929422963\n",
      "Test Loss: 0.5907599925994873, R2 Score (Test): 0.750441097216923, RMSE (Test): 0.783587634563446\n",
      "Epoch [3536/5000], Train Loss: 0.006679229127864043, R2 Score (Train): 0.9944169633608752, RMSE (Train): 0.12550531835053141\n",
      "Test Loss: 0.5952894389629364, R2 Score (Test): 0.7491712974445, RMSE (Test): 0.7855786681175232\n",
      "Epoch [3541/5000], Train Loss: 0.006047537938381235, R2 Score (Train): 0.9948541594874143, RMSE (Train): 0.12049112285246535\n",
      "Test Loss: 0.6241201460361481, R2 Score (Test): 0.7493577800898064, RMSE (Test): 0.7852866053581238\n",
      "Epoch [3546/5000], Train Loss: 0.007489157724194229, R2 Score (Train): 0.9943378011575233, RMSE (Train): 0.1263919601050258\n",
      "Test Loss: 0.696599468588829, R2 Score (Test): 0.7546632841013424, RMSE (Test): 0.7769307494163513\n",
      "Epoch [3551/5000], Train Loss: 0.007170205897030731, R2 Score (Train): 0.9953773115518195, RMSE (Train): 0.11420212971291302\n",
      "Test Loss: 0.5892151296138763, R2 Score (Test): 0.7491192593066336, RMSE (Test): 0.7856601476669312\n",
      "Epoch [3556/5000], Train Loss: 0.007512115116696805, R2 Score (Train): 0.9933115988051077, RMSE (Train): 0.13736878232968103\n",
      "Test Loss: 0.6361837685108185, R2 Score (Test): 0.7511722140930097, RMSE (Test): 0.7824389934539795\n",
      "Epoch [3561/5000], Train Loss: 0.007608306865828733, R2 Score (Train): 0.9937822682735896, RMSE (Train): 0.13244723020825724\n",
      "Test Loss: 0.5724880546331406, R2 Score (Test): 0.7544069559090124, RMSE (Test): 0.7773365378379822\n",
      "Epoch [3566/5000], Train Loss: 0.007248938704530398, R2 Score (Train): 0.9935856820317952, RMSE (Train): 0.1345247312454265\n",
      "Test Loss: 0.5634072124958038, R2 Score (Test): 0.7508809261056688, RMSE (Test): 0.7828968167304993\n",
      "Epoch [3571/5000], Train Loss: 0.00893417140468955, R2 Score (Train): 0.9929323605689603, RMSE (Train): 0.1412095546044484\n",
      "Test Loss: 0.6342836916446686, R2 Score (Test): 0.7515232793521718, RMSE (Test): 0.7818868160247803\n",
      "Epoch [3576/5000], Train Loss: 0.008076475739168624, R2 Score (Train): 0.993834423827861, RMSE (Train): 0.1318905637096736\n",
      "Test Loss: 0.5907779633998871, R2 Score (Test): 0.7604124937572624, RMSE (Test): 0.7677735090255737\n",
      "Epoch [3581/5000], Train Loss: 0.007160809317914148, R2 Score (Train): 0.9948722281867409, RMSE (Train): 0.12027939530593572\n",
      "Test Loss: 0.5658206790685654, R2 Score (Test): 0.7526747929049438, RMSE (Test): 0.7800729870796204\n",
      "Epoch [3586/5000], Train Loss: 0.01065421443975841, R2 Score (Train): 0.9947686725113071, RMSE (Train): 0.12148784958979608\n",
      "Test Loss: 0.5937097072601318, R2 Score (Test): 0.754813226461915, RMSE (Test): 0.7766933441162109\n",
      "Epoch [3591/5000], Train Loss: 0.008287976961582899, R2 Score (Train): 0.9949150307113054, RMSE (Train): 0.11977634540204622\n",
      "Test Loss: 0.6495918333530426, R2 Score (Test): 0.7545577683337248, RMSE (Test): 0.7770978212356567\n",
      "Epoch [3596/5000], Train Loss: 0.008594938864310583, R2 Score (Train): 0.9944631999920004, RMSE (Train): 0.12498454373028639\n",
      "Test Loss: 0.6491384208202362, R2 Score (Test): 0.7478076548156902, RMSE (Test): 0.7877111434936523\n",
      "Epoch [3601/5000], Train Loss: 0.005717635077113907, R2 Score (Train): 0.9923212575730472, RMSE (Train): 0.1471878429360802\n",
      "Test Loss: 0.5349945724010468, R2 Score (Test): 0.7533021106360895, RMSE (Test): 0.7790830731391907\n",
      "Epoch [3606/5000], Train Loss: 0.00768595770932734, R2 Score (Train): 0.9942567547259823, RMSE (Train): 0.12729330796368826\n",
      "Test Loss: 0.6289876699447632, R2 Score (Test): 0.7493033107736091, RMSE (Test): 0.7853718400001526\n",
      "Epoch [3611/5000], Train Loss: 0.006249279831536114, R2 Score (Train): 0.9955795234402431, RMSE (Train): 0.11167640799381653\n",
      "Test Loss: 0.6367855668067932, R2 Score (Test): 0.7553876472758184, RMSE (Test): 0.7757829427719116\n",
      "Epoch [3616/5000], Train Loss: 0.007998357041894147, R2 Score (Train): 0.9933656289358916, RMSE (Train): 0.13681281214450802\n",
      "Test Loss: 0.6810100972652435, R2 Score (Test): 0.742272007550174, RMSE (Test): 0.7963094115257263\n",
      "Epoch [3621/5000], Train Loss: 0.007788513806493332, R2 Score (Train): 0.9951614321513242, RMSE (Train): 0.11683832124775444\n",
      "Test Loss: 0.6717755198478699, R2 Score (Test): 0.7469688853950911, RMSE (Test): 0.7890200018882751\n",
      "Epoch [3626/5000], Train Loss: 0.009757048450410366, R2 Score (Train): 0.9935752462603786, RMSE (Train): 0.13463411924349689\n",
      "Test Loss: 0.5241770297288895, R2 Score (Test): 0.7560455567008184, RMSE (Test): 0.7747389674186707\n",
      "Epoch [3631/5000], Train Loss: 0.00953081512125209, R2 Score (Train): 0.9959187151608208, RMSE (Train): 0.10730633046506134\n",
      "Test Loss: 0.6300677359104156, R2 Score (Test): 0.7513415431670343, RMSE (Test): 0.7821727395057678\n",
      "Epoch [3636/5000], Train Loss: 0.0074544915308554964, R2 Score (Train): 0.992813599858273, RMSE (Train): 0.14239101581935493\n",
      "Test Loss: 0.6137641370296478, R2 Score (Test): 0.7562422933844415, RMSE (Test): 0.7744265198707581\n",
      "Epoch [3641/5000], Train Loss: 0.006715160561725497, R2 Score (Train): 0.9921572970844497, RMSE (Train): 0.14875095845434883\n",
      "Test Loss: 0.5355432480573654, R2 Score (Test): 0.763109546031468, RMSE (Test): 0.7634398937225342\n",
      "Epoch [3646/5000], Train Loss: 0.008920656788783768, R2 Score (Train): 0.9939738422156628, RMSE (Train): 0.13039085708423234\n",
      "Test Loss: 0.5520762503147125, R2 Score (Test): 0.7554982741827607, RMSE (Test): 0.7756075263023376\n",
      "Epoch [3651/5000], Train Loss: 0.0055248956972112255, R2 Score (Train): 0.9950548304277919, RMSE (Train): 0.11811838078630675\n",
      "Test Loss: 0.5622002184391022, R2 Score (Test): 0.7596797966329762, RMSE (Test): 0.768946647644043\n",
      "Epoch [3656/5000], Train Loss: 0.008299102812695006, R2 Score (Train): 0.9954230368079074, RMSE (Train): 0.11363591168969367\n",
      "Test Loss: 0.6535763442516327, R2 Score (Test): 0.764919963683749, RMSE (Test): 0.7605170011520386\n",
      "Epoch [3661/5000], Train Loss: 0.006192556698806584, R2 Score (Train): 0.9946798687603442, RMSE (Train): 0.12251466126055349\n",
      "Test Loss: 0.6527058184146881, R2 Score (Test): 0.7600405360226492, RMSE (Test): 0.7683692574501038\n",
      "Epoch [3666/5000], Train Loss: 0.00801330921240151, R2 Score (Train): 0.9934551192181182, RMSE (Train): 0.13588695305390272\n",
      "Test Loss: 0.5514997839927673, R2 Score (Test): 0.7542918528064264, RMSE (Test): 0.777518630027771\n",
      "Epoch [3671/5000], Train Loss: 0.007104060845449567, R2 Score (Train): 0.9918496520140325, RMSE (Train): 0.1516404157760638\n",
      "Test Loss: 0.5691853165626526, R2 Score (Test): 0.7605509716526897, RMSE (Test): 0.7675515413284302\n",
      "Epoch [3676/5000], Train Loss: 0.006177632157535602, R2 Score (Train): 0.9947698323416758, RMSE (Train): 0.12147438139185521\n",
      "Test Loss: 0.5944592356681824, R2 Score (Test): 0.7527861523734227, RMSE (Test): 0.7798973321914673\n",
      "Epoch [3681/5000], Train Loss: 0.007078623088697593, R2 Score (Train): 0.9924460367337523, RMSE (Train): 0.1459870473851425\n",
      "Test Loss: 0.5803439021110535, R2 Score (Test): 0.7546560186057362, RMSE (Test): 0.7769423127174377\n",
      "Epoch [3686/5000], Train Loss: 0.006391205883119255, R2 Score (Train): 0.9942844174513125, RMSE (Train): 0.12698637956270617\n",
      "Test Loss: 0.616072952747345, R2 Score (Test): 0.7547449544526574, RMSE (Test): 0.7768014669418335\n",
      "Epoch [3691/5000], Train Loss: 0.0066952713920424385, R2 Score (Train): 0.993936490820827, RMSE (Train): 0.13079432784471556\n",
      "Test Loss: 0.6340674459934235, R2 Score (Test): 0.7555685616690571, RMSE (Test): 0.7754960656166077\n",
      "Epoch [3696/5000], Train Loss: 0.006110924722937246, R2 Score (Train): 0.9945248199709835, RMSE (Train): 0.12428711082972252\n",
      "Test Loss: 0.598764032125473, R2 Score (Test): 0.7505283162238016, RMSE (Test): 0.783450722694397\n",
      "Epoch [3701/5000], Train Loss: 0.007647331648816665, R2 Score (Train): 0.9941649059076185, RMSE (Train): 0.12830713938084864\n",
      "Test Loss: 0.5457777976989746, R2 Score (Test): 0.7525354667344808, RMSE (Test): 0.7802926301956177\n",
      "Epoch [3706/5000], Train Loss: 0.017522421316243708, R2 Score (Train): 0.991042671449183, RMSE (Train): 0.15897035453390038\n",
      "Test Loss: 0.5813879370689392, R2 Score (Test): 0.7594784623982421, RMSE (Test): 0.7692686915397644\n",
      "Epoch [3711/5000], Train Loss: 0.00754907038450862, R2 Score (Train): 0.9948008721200061, RMSE (Train): 0.12111338447096959\n",
      "Test Loss: 0.6476855576038361, R2 Score (Test): 0.7502436615271876, RMSE (Test): 0.7838975787162781\n",
      "Epoch [3716/5000], Train Loss: 0.006837601133156568, R2 Score (Train): 0.9940145443830984, RMSE (Train): 0.12994976488246587\n",
      "Test Loss: 0.6020688712596893, R2 Score (Test): 0.7588452857246237, RMSE (Test): 0.7702805399894714\n",
      "Epoch [3721/5000], Train Loss: 0.010142891745393475, R2 Score (Train): 0.9937466670997586, RMSE (Train): 0.13282586881026437\n",
      "Test Loss: 0.6165294051170349, R2 Score (Test): 0.7626802731664052, RMSE (Test): 0.7641312479972839\n",
      "Epoch [3726/5000], Train Loss: 0.007369851189044614, R2 Score (Train): 0.9935822858123069, RMSE (Train): 0.13456034025765312\n",
      "Test Loss: 0.5775324404239655, R2 Score (Test): 0.7587298310715374, RMSE (Test): 0.7704648971557617\n",
      "Epoch [3731/5000], Train Loss: 0.006468094885349274, R2 Score (Train): 0.9931323149947215, RMSE (Train): 0.13919770482521965\n",
      "Test Loss: 0.6128397583961487, R2 Score (Test): 0.7598443014383977, RMSE (Test): 0.7686833739280701\n",
      "Epoch [3736/5000], Train Loss: 0.008537750380734602, R2 Score (Train): 0.9940230379439322, RMSE (Train): 0.1298575306281573\n",
      "Test Loss: 0.5682788640260696, R2 Score (Test): 0.7565296806817412, RMSE (Test): 0.7739698886871338\n",
      "Epoch [3741/5000], Train Loss: 0.0074523390891651315, R2 Score (Train): 0.9949428692536835, RMSE (Train): 0.11944802728460732\n",
      "Test Loss: 0.6446489989757538, R2 Score (Test): 0.7547378979275213, RMSE (Test): 0.7768126130104065\n",
      "Epoch [3746/5000], Train Loss: 0.008658693598893782, R2 Score (Train): 0.9930779062224872, RMSE (Train): 0.1397480092619689\n",
      "Test Loss: 0.5963538289070129, R2 Score (Test): 0.7639152765380529, RMSE (Test): 0.7621403932571411\n",
      "Epoch [3751/5000], Train Loss: 0.006514542600295196, R2 Score (Train): 0.9934131717339367, RMSE (Train): 0.1363217212225083\n",
      "Test Loss: 0.6295221745967865, R2 Score (Test): 0.7577578656298455, RMSE (Test): 0.7720152735710144\n",
      "Epoch [3756/5000], Train Loss: 0.006928682598906259, R2 Score (Train): 0.9933862303383678, RMSE (Train): 0.13660022776760375\n",
      "Test Loss: 0.5634117424488068, R2 Score (Test): 0.7644370882179536, RMSE (Test): 0.7612977027893066\n",
      "Epoch [3761/5000], Train Loss: 0.007624134964620073, R2 Score (Train): 0.9939304694440945, RMSE (Train): 0.13085925447992006\n",
      "Test Loss: 0.606955349445343, R2 Score (Test): 0.7580711999417773, RMSE (Test): 0.7715157866477966\n",
      "Epoch [3766/5000], Train Loss: 0.00777619182675456, R2 Score (Train): 0.9918711548518149, RMSE (Train): 0.15144024930696592\n",
      "Test Loss: 0.6020229160785675, R2 Score (Test): 0.7573212976989689, RMSE (Test): 0.7727105617523193\n",
      "Epoch [3771/5000], Train Loss: 0.0067763103094572825, R2 Score (Train): 0.9926083613857578, RMSE (Train): 0.14440999517018088\n",
      "Test Loss: 0.6125479340553284, R2 Score (Test): 0.7604886261613468, RMSE (Test): 0.7676515579223633\n",
      "Epoch [3776/5000], Train Loss: 0.008589739309779057, R2 Score (Train): 0.9891953848711155, RMSE (Train): 0.17459489510218568\n",
      "Test Loss: 0.5799952447414398, R2 Score (Test): 0.7666524553760438, RMSE (Test): 0.7577093839645386\n",
      "Epoch [3781/5000], Train Loss: 0.006086286545420687, R2 Score (Train): 0.9941415669815782, RMSE (Train): 0.12856348164352369\n",
      "Test Loss: 0.6175588369369507, R2 Score (Test): 0.7538553824595867, RMSE (Test): 0.7782089710235596\n",
      "Epoch [3786/5000], Train Loss: 0.011181329105359813, R2 Score (Train): 0.9904523395740431, RMSE (Train): 0.16412523951327498\n",
      "Test Loss: 0.6268217861652374, R2 Score (Test): 0.7596664574038143, RMSE (Test): 0.7689679861068726\n",
      "Epoch [3791/5000], Train Loss: 0.007420383820620676, R2 Score (Train): 0.9915144848749555, RMSE (Train): 0.15472696135934189\n",
      "Test Loss: 0.600856214761734, R2 Score (Test): 0.7547718321761203, RMSE (Test): 0.7767588496208191\n",
      "Epoch [3796/5000], Train Loss: 0.008508270470580706, R2 Score (Train): 0.9950244059243306, RMSE (Train): 0.11848117750543863\n",
      "Test Loss: 0.609442949295044, R2 Score (Test): 0.7528522139936294, RMSE (Test): 0.7797930836677551\n",
      "Epoch [3801/5000], Train Loss: 0.006161592590312163, R2 Score (Train): 0.9940865219904014, RMSE (Train): 0.12916605149779772\n",
      "Test Loss: 0.5124790668487549, R2 Score (Test): 0.7580939176402067, RMSE (Test): 0.771479606628418\n",
      "Epoch [3806/5000], Train Loss: 0.008526101475581527, R2 Score (Train): 0.9943757693942448, RMSE (Train): 0.12596748262907378\n",
      "Test Loss: 0.6638878881931305, R2 Score (Test): 0.7562851178487646, RMSE (Test): 0.7743585109710693\n",
      "Epoch [3811/5000], Train Loss: 0.008852576526502768, R2 Score (Train): 0.9941020946756598, RMSE (Train): 0.12899586499615343\n",
      "Test Loss: 0.5501371473073959, R2 Score (Test): 0.7493372052097407, RMSE (Test): 0.7853187918663025\n",
      "Epoch [3816/5000], Train Loss: 0.0072002717449019355, R2 Score (Train): 0.9949449512127424, RMSE (Train): 0.11942343710536779\n",
      "Test Loss: 0.6380597949028015, R2 Score (Test): 0.7423061536638192, RMSE (Test): 0.7962566614151001\n",
      "Epoch [3821/5000], Train Loss: 0.008725025962727765, R2 Score (Train): 0.9944787665499086, RMSE (Train): 0.12480872479644312\n",
      "Test Loss: 0.6007473170757294, R2 Score (Test): 0.7496296137575272, RMSE (Test): 0.7848606109619141\n",
      "Epoch [3826/5000], Train Loss: 0.007025481783784926, R2 Score (Train): 0.9931261771304634, RMSE (Train): 0.13925989359815422\n",
      "Test Loss: 0.5693158209323883, R2 Score (Test): 0.7558869205952203, RMSE (Test): 0.7749908566474915\n",
      "Epoch [3831/5000], Train Loss: 0.00529557338450104, R2 Score (Train): 0.9948686623123627, RMSE (Train): 0.12032120943988583\n",
      "Test Loss: 0.5801497399806976, R2 Score (Test): 0.7569053443702081, RMSE (Test): 0.7733725309371948\n",
      "Epoch [3836/5000], Train Loss: 0.005832174366029601, R2 Score (Train): 0.9945717115727079, RMSE (Train): 0.12375374445990633\n",
      "Test Loss: 0.6295957863330841, R2 Score (Test): 0.754888345027411, RMSE (Test): 0.7765743136405945\n",
      "Epoch [3841/5000], Train Loss: 0.006374683095297466, R2 Score (Train): 0.9947723835473279, RMSE (Train): 0.12144475099089336\n",
      "Test Loss: 0.5778798758983612, R2 Score (Test): 0.7579028081772153, RMSE (Test): 0.7717843055725098\n",
      "Epoch [3846/5000], Train Loss: 0.0057414853751348955, R2 Score (Train): 0.9951414088167515, RMSE (Train): 0.11707982632716428\n",
      "Test Loss: 0.6166171133518219, R2 Score (Test): 0.7547170228395792, RMSE (Test): 0.7768456339836121\n",
      "Epoch [3851/5000], Train Loss: 0.006574305240064859, R2 Score (Train): 0.9934662628125173, RMSE (Train): 0.13577122029622424\n",
      "Test Loss: 0.5721256732940674, R2 Score (Test): 0.7535139944143766, RMSE (Test): 0.7787484526634216\n",
      "Epoch [3856/5000], Train Loss: 0.0070969983159254, R2 Score (Train): 0.9948653182390086, RMSE (Train): 0.12036040949274338\n",
      "Test Loss: 0.6580945253372192, R2 Score (Test): 0.7510990140176459, RMSE (Test): 0.7825540900230408\n",
      "Epoch [3861/5000], Train Loss: 0.009003314306028187, R2 Score (Train): 0.9906399171863456, RMSE (Train): 0.16250500308896385\n",
      "Test Loss: 0.6171867549419403, R2 Score (Test): 0.7505802385443118, RMSE (Test): 0.7833691835403442\n",
      "Epoch [3866/5000], Train Loss: 0.007286441163159907, R2 Score (Train): 0.9944611994139514, RMSE (Train): 0.12500712164098346\n",
      "Test Loss: 0.5800482481718063, R2 Score (Test): 0.7483297907961421, RMSE (Test): 0.7868952751159668\n",
      "Epoch [3871/5000], Train Loss: 0.008155568658063808, R2 Score (Train): 0.9944297476987684, RMSE (Train): 0.12536154157875887\n",
      "Test Loss: 0.6124306321144104, R2 Score (Test): 0.7459537316222472, RMSE (Test): 0.7906011939048767\n",
      "Epoch [3876/5000], Train Loss: 0.01415642189870899, R2 Score (Train): 0.9958339851996011, RMSE (Train): 0.10841448095751839\n",
      "Test Loss: 0.6438381671905518, R2 Score (Test): 0.747715878603731, RMSE (Test): 0.7878544330596924\n",
      "Epoch [3881/5000], Train Loss: 0.007180924275113891, R2 Score (Train): 0.993769037456833, RMSE (Train): 0.13258807367561504\n",
      "Test Loss: 0.6607403755187988, R2 Score (Test): 0.7490300240861267, RMSE (Test): 0.7857998013496399\n",
      "Epoch [3886/5000], Train Loss: 0.008876066305674613, R2 Score (Train): 0.9948656800885923, RMSE (Train): 0.12035616841866872\n",
      "Test Loss: 0.5851379334926605, R2 Score (Test): 0.7545003804505722, RMSE (Test): 0.7771887183189392\n",
      "Epoch [3891/5000], Train Loss: 0.006503653382727255, R2 Score (Train): 0.9942405691660059, RMSE (Train): 0.12747255017129128\n",
      "Test Loss: 0.5757541060447693, R2 Score (Test): 0.7599169486641769, RMSE (Test): 0.7685670852661133\n",
      "Epoch [3896/5000], Train Loss: 0.006721739657223225, R2 Score (Train): 0.9932824173554278, RMSE (Train): 0.13766812571602288\n",
      "Test Loss: 0.5525698661804199, R2 Score (Test): 0.7570648985012483, RMSE (Test): 0.773118793964386\n",
      "Epoch [3901/5000], Train Loss: 0.00515566481044516, R2 Score (Train): 0.9947717361932695, RMSE (Train): 0.12145227022259368\n",
      "Test Loss: 0.6429178714752197, R2 Score (Test): 0.7558166400449144, RMSE (Test): 0.7751023769378662\n",
      "Epoch [3906/5000], Train Loss: 0.011157860863022506, R2 Score (Train): 0.994608121525109, RMSE (Train): 0.12333801036905168\n",
      "Test Loss: 0.6457664370536804, R2 Score (Test): 0.7537679286578114, RMSE (Test): 0.7783471345901489\n",
      "Epoch [3911/5000], Train Loss: 0.005631374525061498, R2 Score (Train): 0.993975520311921, RMSE (Train): 0.13037270093464565\n",
      "Test Loss: 0.6673237234354019, R2 Score (Test): 0.7550159963209295, RMSE (Test): 0.7763720750808716\n",
      "Epoch [3916/5000], Train Loss: 0.008032630352924267, R2 Score (Train): 0.9946555140697969, RMSE (Train): 0.12279476709606367\n",
      "Test Loss: 0.5468962490558624, R2 Score (Test): 0.7515155156704396, RMSE (Test): 0.7818990349769592\n",
      "Epoch [3921/5000], Train Loss: 0.008312419444943467, R2 Score (Train): 0.9945411075645451, RMSE (Train): 0.12410210817395838\n",
      "Test Loss: 0.568997398018837, R2 Score (Test): 0.7498384416309699, RMSE (Test): 0.7845332026481628\n",
      "Epoch [3926/5000], Train Loss: 0.007976380487283071, R2 Score (Train): 0.992937823951946, RMSE (Train): 0.14115496558397234\n",
      "Test Loss: 0.5386206656694412, R2 Score (Test): 0.754503203205689, RMSE (Test): 0.7771841883659363\n",
      "Epoch [3931/5000], Train Loss: 0.0066658275900408626, R2 Score (Train): 0.9942195188669989, RMSE (Train): 0.12770528913416745\n",
      "Test Loss: 0.6303138732910156, R2 Score (Test): 0.753404230455312, RMSE (Test): 0.7789218425750732\n",
      "Epoch [3936/5000], Train Loss: 0.007357593916822225, R2 Score (Train): 0.9949215731940327, RMSE (Train): 0.11969926657904689\n",
      "Test Loss: 0.5672548860311508, R2 Score (Test): 0.7550568090201102, RMSE (Test): 0.7763074040412903\n",
      "Epoch [3941/5000], Train Loss: 0.01166392556236436, R2 Score (Train): 0.9911379993992546, RMSE (Train): 0.15812217453791658\n",
      "Test Loss: 0.6283645629882812, R2 Score (Test): 0.7590742009917012, RMSE (Test): 0.7699148058891296\n",
      "Epoch [3946/5000], Train Loss: 0.005431067994019638, R2 Score (Train): 0.9952292532029151, RMSE (Train): 0.1160165841205274\n",
      "Test Loss: 0.5952727794647217, R2 Score (Test): 0.7541475669226231, RMSE (Test): 0.7777469158172607\n",
      "Epoch [3951/5000], Train Loss: 0.004867965510735909, R2 Score (Train): 0.9967582476096065, RMSE (Train): 0.09563498862647993\n",
      "Test Loss: 0.5910308361053467, R2 Score (Test): 0.7497295163767725, RMSE (Test): 0.7847039699554443\n",
      "Epoch [3956/5000], Train Loss: 0.00894480652641505, R2 Score (Train): 0.9933494210599139, RMSE (Train): 0.13697982815915374\n",
      "Test Loss: 0.5883907973766327, R2 Score (Test): 0.7561352467679763, RMSE (Test): 0.7745965123176575\n",
      "Epoch [3961/5000], Train Loss: 0.006840048435454567, R2 Score (Train): 0.9955493910403433, RMSE (Train): 0.11205638547426355\n",
      "Test Loss: 0.5495448112487793, R2 Score (Test): 0.7586362308805662, RMSE (Test): 0.7706143856048584\n",
      "Epoch [3966/5000], Train Loss: 0.006284965861899157, R2 Score (Train): 0.9941777947270307, RMSE (Train): 0.12816535574038238\n",
      "Test Loss: 0.580907791852951, R2 Score (Test): 0.7593801622653791, RMSE (Test): 0.7694258093833923\n",
      "Epoch [3971/5000], Train Loss: 0.006880401148616026, R2 Score (Train): 0.993241621562734, RMSE (Train): 0.1380855213409452\n",
      "Test Loss: 0.566630408167839, R2 Score (Test): 0.7573975180180605, RMSE (Test): 0.7725892663002014\n",
      "Epoch [3976/5000], Train Loss: 0.007293651869986206, R2 Score (Train): 0.9946084895950118, RMSE (Train): 0.12333380053898349\n",
      "Test Loss: 0.6472612917423248, R2 Score (Test): 0.7552154519510319, RMSE (Test): 0.7760559320449829\n",
      "Epoch [3981/5000], Train Loss: 0.00731164818474402, R2 Score (Train): 0.9931715510111444, RMSE (Train): 0.13879950758301826\n",
      "Test Loss: 0.5631794780492783, R2 Score (Test): 0.7549531242899762, RMSE (Test): 0.776471734046936\n",
      "Epoch [3986/5000], Train Loss: 0.0058066157895761234, R2 Score (Train): 0.9943905973417678, RMSE (Train): 0.12580132017146853\n",
      "Test Loss: 0.6054172515869141, R2 Score (Test): 0.7534869422969932, RMSE (Test): 0.7787911891937256\n",
      "Epoch [3991/5000], Train Loss: 0.006597911492766191, R2 Score (Train): 0.9936194737358395, RMSE (Train): 0.13416991388327304\n",
      "Test Loss: 0.5962479412555695, R2 Score (Test): 0.7583722507589599, RMSE (Test): 0.7710356116294861\n",
      "Epoch [3996/5000], Train Loss: 0.006824517447967082, R2 Score (Train): 0.9951319111459643, RMSE (Train): 0.11719420544468441\n",
      "Test Loss: 0.6486807465553284, R2 Score (Test): 0.752074330207236, RMSE (Test): 0.781019389629364\n",
      "Epoch [4001/5000], Train Loss: 0.0060007475937406225, R2 Score (Train): 0.995826277415126, RMSE (Train): 0.1085147265562529\n",
      "Test Loss: 0.680017739534378, R2 Score (Test): 0.7441668637296142, RMSE (Test): 0.7933766841888428\n",
      "Epoch [4006/5000], Train Loss: 0.00667519048632433, R2 Score (Train): 0.9954029124662712, RMSE (Train): 0.11388545921469741\n",
      "Test Loss: 0.6214419305324554, R2 Score (Test): 0.7447879171304742, RMSE (Test): 0.7924131751060486\n",
      "Epoch [4011/5000], Train Loss: 0.006451421921762328, R2 Score (Train): 0.9966878098079248, RMSE (Train): 0.09666839858156953\n",
      "Test Loss: 0.5965805053710938, R2 Score (Test): 0.7520526400159242, RMSE (Test): 0.7810535430908203\n",
      "Epoch [4016/5000], Train Loss: 0.007098011730704457, R2 Score (Train): 0.9951472974825547, RMSE (Train): 0.11700885379895272\n",
      "Test Loss: 0.5929892063140869, R2 Score (Test): 0.7526960444621293, RMSE (Test): 0.7800394892692566\n",
      "Epoch [4021/5000], Train Loss: 0.005904121654263387, R2 Score (Train): 0.994318455879755, RMSE (Train): 0.12660768925486682\n",
      "Test Loss: 0.613145262002945, R2 Score (Test): 0.7509953319525442, RMSE (Test): 0.7827170491218567\n",
      "Epoch [4026/5000], Train Loss: 0.005353214044589549, R2 Score (Train): 0.993935029781334, RMSE (Train): 0.13081008474082662\n",
      "Test Loss: 0.5836691856384277, R2 Score (Test): 0.7550115517089462, RMSE (Test): 0.7763791084289551\n",
      "Epoch [4031/5000], Train Loss: 0.005710258284428467, R2 Score (Train): 0.9938025977380268, RMSE (Train): 0.13223052853207842\n",
      "Test Loss: 0.5853996574878693, R2 Score (Test): 0.7595341832740019, RMSE (Test): 0.7691795229911804\n",
      "Epoch [4036/5000], Train Loss: 0.008487841851698855, R2 Score (Train): 0.9940448198599083, RMSE (Train): 0.1296206939603849\n",
      "Test Loss: 0.5614928603172302, R2 Score (Test): 0.7564763009870396, RMSE (Test): 0.7740547060966492\n",
      "Epoch [4041/5000], Train Loss: 0.006116336987664302, R2 Score (Train): 0.9922080364468279, RMSE (Train): 0.14826899605314436\n",
      "Test Loss: 0.5635240375995636, R2 Score (Test): 0.7604820656303368, RMSE (Test): 0.767661988735199\n",
      "Epoch [4046/5000], Train Loss: 0.007832963310647756, R2 Score (Train): 0.989108116346304, RMSE (Train): 0.1752985757016667\n",
      "Test Loss: 0.6526725143194199, R2 Score (Test): 0.7588353118935532, RMSE (Test): 0.7702964544296265\n",
      "Epoch [4051/5000], Train Loss: 0.006977534755909194, R2 Score (Train): 0.9937507028898548, RMSE (Train): 0.1327830001630313\n",
      "Test Loss: 0.6272130310535431, R2 Score (Test): 0.7565687411568999, RMSE (Test): 0.7739077806472778\n",
      "Epoch [4056/5000], Train Loss: 0.005338583956472576, R2 Score (Train): 0.9948678374097962, RMSE (Train): 0.1203308803377979\n",
      "Test Loss: 0.6668115556240082, R2 Score (Test): 0.7524117960616267, RMSE (Test): 0.780487596988678\n",
      "Epoch [4061/5000], Train Loss: 0.007919637544546276, R2 Score (Train): 0.9922435621420553, RMSE (Train): 0.14793061044030606\n",
      "Test Loss: 0.5829980671405792, R2 Score (Test): 0.7546338511927084, RMSE (Test): 0.7769773602485657\n",
      "Epoch [4066/5000], Train Loss: 0.005842194213376691, R2 Score (Train): 0.9958113744067187, RMSE (Train): 0.10870828935307991\n",
      "Test Loss: 0.5958842635154724, R2 Score (Test): 0.7533553598986407, RMSE (Test): 0.7789990305900574\n",
      "Epoch [4071/5000], Train Loss: 0.00624940909134845, R2 Score (Train): 0.9923440985037005, RMSE (Train): 0.14696876988310148\n",
      "Test Loss: 0.6567019522190094, R2 Score (Test): 0.7608297891720261, RMSE (Test): 0.7671046257019043\n",
      "Epoch [4076/5000], Train Loss: 0.0068859345046803355, R2 Score (Train): 0.9933670970837992, RMSE (Train): 0.1367976733652971\n",
      "Test Loss: 0.5712756812572479, R2 Score (Test): 0.7550292338328217, RMSE (Test): 0.7763510942459106\n",
      "Epoch [4081/5000], Train Loss: 0.00954987487057224, R2 Score (Train): 0.9931178059578846, RMSE (Train): 0.13934466548656732\n",
      "Test Loss: 0.5788862407207489, R2 Score (Test): 0.7598923323646785, RMSE (Test): 0.768606424331665\n",
      "Epoch [4086/5000], Train Loss: 0.006934675194012622, R2 Score (Train): 0.9923283066327374, RMSE (Train): 0.1471202684521489\n",
      "Test Loss: 0.5890315771102905, R2 Score (Test): 0.7487290512850633, RMSE (Test): 0.7862708568572998\n",
      "Epoch [4091/5000], Train Loss: 0.004489776639578243, R2 Score (Train): 0.9950278443368772, RMSE (Train): 0.11844023188495556\n",
      "Test Loss: 0.5423206239938736, R2 Score (Test): 0.7574685851288955, RMSE (Test): 0.772476077079773\n",
      "Epoch [4096/5000], Train Loss: 0.005065465869847685, R2 Score (Train): 0.9947033403228901, RMSE (Train): 0.12224410503708777\n",
      "Test Loss: 0.6038409769535065, R2 Score (Test): 0.755996191017403, RMSE (Test): 0.7748172879219055\n",
      "Epoch [4101/5000], Train Loss: 0.007101383487073083, R2 Score (Train): 0.9923039098509514, RMSE (Train): 0.14735401163546988\n",
      "Test Loss: 0.5824091136455536, R2 Score (Test): 0.7619793429640119, RMSE (Test): 0.7652589082717896\n",
      "Epoch [4106/5000], Train Loss: 0.007213207893073559, R2 Score (Train): 0.9952501731249208, RMSE (Train): 0.1157619358933773\n",
      "Test Loss: 0.6261734664440155, R2 Score (Test): 0.7602605861887544, RMSE (Test): 0.7680168747901917\n",
      "Epoch [4111/5000], Train Loss: 0.0062571283197030425, R2 Score (Train): 0.9946466350151243, RMSE (Train): 0.1228967272282298\n",
      "Test Loss: 0.5867928564548492, R2 Score (Test): 0.7571100251003733, RMSE (Test): 0.7730469107627869\n",
      "Epoch [4116/5000], Train Loss: 0.00696247296097378, R2 Score (Train): 0.9952581838358479, RMSE (Train): 0.11566427688881908\n",
      "Test Loss: 0.5754591226577759, R2 Score (Test): 0.7562674927886404, RMSE (Test): 0.774386465549469\n",
      "Epoch [4121/5000], Train Loss: 0.005956131072404484, R2 Score (Train): 0.9943857276327788, RMSE (Train): 0.12585591448004577\n",
      "Test Loss: 0.5732332617044449, R2 Score (Test): 0.7545909409535332, RMSE (Test): 0.7770453095436096\n",
      "Epoch [4126/5000], Train Loss: 0.006121049615709732, R2 Score (Train): 0.9944925247711512, RMSE (Train): 0.12465312395766963\n",
      "Test Loss: 0.5905994176864624, R2 Score (Test): 0.7557539877808008, RMSE (Test): 0.7752017974853516\n",
      "Epoch [4131/5000], Train Loss: 0.0059477312994810445, R2 Score (Train): 0.9925942409898986, RMSE (Train): 0.1445478640168126\n",
      "Test Loss: 0.622128427028656, R2 Score (Test): 0.7595365476461982, RMSE (Test): 0.7691757082939148\n",
      "Epoch [4136/5000], Train Loss: 0.0055650857199604315, R2 Score (Train): 0.9952619745785203, RMSE (Train): 0.11561803498930308\n",
      "Test Loss: 0.567310094833374, R2 Score (Test): 0.7588509360940582, RMSE (Test): 0.7702714800834656\n",
      "Epoch [4141/5000], Train Loss: 0.007899885841955742, R2 Score (Train): 0.9951164584528752, RMSE (Train): 0.11738006187514197\n",
      "Test Loss: 0.5943092703819275, R2 Score (Test): 0.7515138415531983, RMSE (Test): 0.7819017171859741\n",
      "Epoch [4146/5000], Train Loss: 0.00853006048904111, R2 Score (Train): 0.9926705953333783, RMSE (Train): 0.14380077970674854\n",
      "Test Loss: 0.5830499529838562, R2 Score (Test): 0.7630182786459467, RMSE (Test): 0.7635869383811951\n",
      "Epoch [4151/5000], Train Loss: 0.006039432172353069, R2 Score (Train): 0.9942478626864564, RMSE (Train): 0.12739181144845707\n",
      "Test Loss: 0.5702246576547623, R2 Score (Test): 0.7578469582311864, RMSE (Test): 0.7718732357025146\n",
      "Epoch [4156/5000], Train Loss: 0.0071098446496762335, R2 Score (Train): 0.9948331766027309, RMSE (Train): 0.1207365326399166\n",
      "Test Loss: 0.5915314257144928, R2 Score (Test): 0.7533705926290655, RMSE (Test): 0.7789749503135681\n",
      "Epoch [4161/5000], Train Loss: 0.006869573766986529, R2 Score (Train): 0.9944862828609953, RMSE (Train): 0.12472374191190802\n",
      "Test Loss: 0.6131958067417145, R2 Score (Test): 0.7527475412684428, RMSE (Test): 0.779958188533783\n",
      "Epoch [4166/5000], Train Loss: 0.006023862243940433, R2 Score (Train): 0.9941067275510442, RMSE (Train): 0.128945191143264\n",
      "Test Loss: 0.5765231847763062, R2 Score (Test): 0.7640911787639715, RMSE (Test): 0.7618564367294312\n",
      "Epoch [4171/5000], Train Loss: 0.00564296367034937, R2 Score (Train): 0.9934346958269806, RMSE (Train): 0.13609880646939185\n",
      "Test Loss: 0.6279622316360474, R2 Score (Test): 0.7586305360643271, RMSE (Test): 0.7706233859062195\n",
      "Epoch [4176/5000], Train Loss: 0.005955088689612846, R2 Score (Train): 0.9929537108054639, RMSE (Train): 0.14099610726352022\n",
      "Test Loss: 0.5989863872528076, R2 Score (Test): 0.7493244681619445, RMSE (Test): 0.7853386998176575\n",
      "Epoch [4181/5000], Train Loss: 0.005792941083200276, R2 Score (Train): 0.9947823587035551, RMSE (Train): 0.12132882733529832\n",
      "Test Loss: 0.6035554707050323, R2 Score (Test): 0.7576520448662336, RMSE (Test): 0.7721838355064392\n",
      "Epoch [4186/5000], Train Loss: 0.0065680827635029955, R2 Score (Train): 0.994905311844159, RMSE (Train): 0.11989075461708804\n",
      "Test Loss: 0.5272756367921829, R2 Score (Test): 0.7571597424618484, RMSE (Test): 0.7729676961898804\n",
      "Epoch [4191/5000], Train Loss: 0.005521749805969496, R2 Score (Train): 0.9959192901586884, RMSE (Train): 0.10729877119304436\n",
      "Test Loss: 0.618334025144577, R2 Score (Test): 0.756742859385498, RMSE (Test): 0.7736309170722961\n",
      "Epoch [4196/5000], Train Loss: 0.01312984227358053, R2 Score (Train): 0.9934444279620787, RMSE (Train): 0.1359978954408761\n",
      "Test Loss: 0.5319776684045792, R2 Score (Test): 0.7600049254481296, RMSE (Test): 0.7684262990951538\n",
      "Epoch [4201/5000], Train Loss: 0.006817481111890326, R2 Score (Train): 0.9951154209941799, RMSE (Train): 0.1173925293127381\n",
      "Test Loss: 0.566984549164772, R2 Score (Test): 0.756252719107517, RMSE (Test): 0.7744099497795105\n",
      "Epoch [4206/5000], Train Loss: 0.0053177275888932245, R2 Score (Train): 0.9938466101762611, RMSE (Train): 0.1317601574592139\n",
      "Test Loss: 0.6351833641529083, R2 Score (Test): 0.7543770046881326, RMSE (Test): 0.7773839235305786\n",
      "Epoch [4211/5000], Train Loss: 0.005633194115944207, R2 Score (Train): 0.994297452935064, RMSE (Train): 0.12684148848860874\n",
      "Test Loss: 0.5739094316959381, R2 Score (Test): 0.7645202326983558, RMSE (Test): 0.7611632943153381\n",
      "Epoch [4216/5000], Train Loss: 0.005897121678572148, R2 Score (Train): 0.9943564445979476, RMSE (Train): 0.12618370851340968\n",
      "Test Loss: 0.601174384355545, R2 Score (Test): 0.7502592553192737, RMSE (Test): 0.7838730812072754\n",
      "Epoch [4221/5000], Train Loss: 0.006179339776281267, R2 Score (Train): 0.9953306800173148, RMSE (Train): 0.11477669339306727\n",
      "Test Loss: 0.6469414234161377, R2 Score (Test): 0.7560973489701365, RMSE (Test): 0.7746567130088806\n",
      "Epoch [4226/5000], Train Loss: 0.004423107214582463, R2 Score (Train): 0.9949446815653487, RMSE (Train): 0.11942662221697525\n",
      "Test Loss: 0.6029738485813141, R2 Score (Test): 0.7578325097763419, RMSE (Test): 0.7718963027000427\n",
      "Epoch [4231/5000], Train Loss: 0.006434383297649522, R2 Score (Train): 0.9957717336588283, RMSE (Train): 0.10922148038800512\n",
      "Test Loss: 0.6337125897407532, R2 Score (Test): 0.753092410342492, RMSE (Test): 0.7794140577316284\n",
      "Epoch [4236/5000], Train Loss: 0.0070837977497528, R2 Score (Train): 0.9940187251388719, RMSE (Train): 0.12990437292172802\n",
      "Test Loss: 0.5624445378780365, R2 Score (Test): 0.7542382779810047, RMSE (Test): 0.7776033878326416\n",
      "Epoch [4241/5000], Train Loss: 0.005417823539270709, R2 Score (Train): 0.9944804404291953, RMSE (Train): 0.12478980415327987\n",
      "Test Loss: 0.6003925800323486, R2 Score (Test): 0.7572397487181763, RMSE (Test): 0.7728404402732849\n",
      "Epoch [4246/5000], Train Loss: 0.00750540445248286, R2 Score (Train): 0.9945433821266495, RMSE (Train): 0.12407625060614016\n",
      "Test Loss: 0.6357134580612183, R2 Score (Test): 0.7570016801108701, RMSE (Test): 0.7732192873954773\n",
      "Epoch [4251/5000], Train Loss: 0.007290827944719543, R2 Score (Train): 0.9909144238631188, RMSE (Train): 0.16010434814432553\n",
      "Test Loss: 0.6257182955741882, R2 Score (Test): 0.7556763299840269, RMSE (Test): 0.7753251194953918\n",
      "Epoch [4256/5000], Train Loss: 0.006975281052291393, R2 Score (Train): 0.9932228387166206, RMSE (Train): 0.13827727144527868\n",
      "Test Loss: 0.5289033353328705, R2 Score (Test): 0.7643337053358372, RMSE (Test): 0.7614646553993225\n",
      "Epoch [4261/5000], Train Loss: 0.009168393172634145, R2 Score (Train): 0.9926659695947146, RMSE (Train): 0.14384615037032553\n",
      "Test Loss: 0.5546549409627914, R2 Score (Test): 0.7625639074013811, RMSE (Test): 0.764318585395813\n",
      "Epoch [4266/5000], Train Loss: 0.006689074759682019, R2 Score (Train): 0.9953264260224012, RMSE (Train): 0.1148289652779301\n",
      "Test Loss: 0.6347720324993134, R2 Score (Test): 0.7530288168451035, RMSE (Test): 0.779514491558075\n",
      "Epoch [4271/5000], Train Loss: 0.005918303135937701, R2 Score (Train): 0.9936289229741019, RMSE (Train): 0.13407052762831545\n",
      "Test Loss: 0.5841021835803986, R2 Score (Test): 0.7560732663505753, RMSE (Test): 0.7746949195861816\n",
      "Epoch [4276/5000], Train Loss: 0.007781452382914722, R2 Score (Train): 0.9926803644583799, RMSE (Train): 0.14370491406185099\n",
      "Test Loss: 0.5739464461803436, R2 Score (Test): 0.768530434415208, RMSE (Test): 0.7546541690826416\n",
      "Epoch [4281/5000], Train Loss: 0.006381209997925907, R2 Score (Train): 0.994849542783701, RMSE (Train): 0.12054516136191888\n",
      "Test Loss: 0.6438601911067963, R2 Score (Test): 0.7602554105041166, RMSE (Test): 0.7680251598358154\n",
      "Epoch [4286/5000], Train Loss: 0.006399398242744307, R2 Score (Train): 0.9937853352253332, RMSE (Train): 0.13241456078913108\n",
      "Test Loss: 0.5702394843101501, R2 Score (Test): 0.7639518301548631, RMSE (Test): 0.7620813846588135\n",
      "Epoch [4291/5000], Train Loss: 0.005937910308906187, R2 Score (Train): 0.9937445387156818, RMSE (Train): 0.13284847119026286\n",
      "Test Loss: 0.5685582160949707, R2 Score (Test): 0.7603192026250463, RMSE (Test): 0.7679229974746704\n",
      "Epoch [4296/5000], Train Loss: 0.0055140389983231826, R2 Score (Train): 0.99155288191495, RMSE (Train): 0.15437649392626562\n",
      "Test Loss: 0.5716862380504608, R2 Score (Test): 0.7713990742572182, RMSE (Test): 0.7499633431434631\n",
      "Epoch [4301/5000], Train Loss: 0.0054825647578885155, R2 Score (Train): 0.9966309879873768, RMSE (Train): 0.09749406316460747\n",
      "Test Loss: 0.5760348439216614, R2 Score (Test): 0.7663284871485894, RMSE (Test): 0.7582351565361023\n",
      "Epoch [4306/5000], Train Loss: 0.0064531463431194425, R2 Score (Train): 0.9949794972358122, RMSE (Train): 0.11901466977843954\n",
      "Test Loss: 0.5668958425521851, R2 Score (Test): 0.764418415228955, RMSE (Test): 0.7613278031349182\n",
      "Epoch [4311/5000], Train Loss: 0.00840355110509942, R2 Score (Train): 0.9912609019702436, RMSE (Train): 0.15702188845582074\n",
      "Test Loss: 0.589265376329422, R2 Score (Test): 0.7653276175445272, RMSE (Test): 0.7598572969436646\n",
      "Epoch [4316/5000], Train Loss: 0.005384298235488434, R2 Score (Train): 0.9951585681183324, RMSE (Train): 0.11687289545583825\n",
      "Test Loss: 0.5814124941825867, R2 Score (Test): 0.7536734002911423, RMSE (Test): 0.7784965634346008\n",
      "Epoch [4321/5000], Train Loss: 0.006225484345729153, R2 Score (Train): 0.9932227593006427, RMSE (Train): 0.13827808162170072\n",
      "Test Loss: 0.6227993369102478, R2 Score (Test): 0.7608683826783049, RMSE (Test): 0.7670426964759827\n",
      "Epoch [4326/5000], Train Loss: 0.010566680148864785, R2 Score (Train): 0.9947877888538487, RMSE (Train): 0.12126567568917948\n",
      "Test Loss: 0.5859420597553253, R2 Score (Test): 0.7587614990750081, RMSE (Test): 0.7704142928123474\n",
      "Epoch [4331/5000], Train Loss: 0.005898179874445002, R2 Score (Train): 0.9952837216564869, RMSE (Train): 0.11535239202640833\n",
      "Test Loss: 0.6248114705085754, R2 Score (Test): 0.760443164953605, RMSE (Test): 0.7677244544029236\n",
      "Epoch [4336/5000], Train Loss: 0.006853857834357768, R2 Score (Train): 0.99556096100552, RMSE (Train): 0.11191063775904025\n",
      "Test Loss: 0.5831497013568878, R2 Score (Test): 0.7569136841916232, RMSE (Test): 0.7733592987060547\n",
      "Epoch [4341/5000], Train Loss: 0.008355913412136337, R2 Score (Train): 0.9927132048948233, RMSE (Train): 0.1433821768543195\n",
      "Test Loss: 0.573140561580658, R2 Score (Test): 0.7619706887852802, RMSE (Test): 0.7652727961540222\n",
      "Epoch [4346/5000], Train Loss: 0.005855653803640355, R2 Score (Train): 0.9948001926303169, RMSE (Train): 0.12112129854898236\n",
      "Test Loss: 0.6333186030387878, R2 Score (Test): 0.7595207367523829, RMSE (Test): 0.7692010402679443\n",
      "Epoch [4351/5000], Train Loss: 0.004861134570091963, R2 Score (Train): 0.9952221881622026, RMSE (Train): 0.11610245732742262\n",
      "Test Loss: 0.6024428308010101, R2 Score (Test): 0.753348349993887, RMSE (Test): 0.7790100574493408\n",
      "Epoch [4356/5000], Train Loss: 0.005891167054263254, R2 Score (Train): 0.9937943909581397, RMSE (Train): 0.13231805131782468\n",
      "Test Loss: 0.5931004285812378, R2 Score (Test): 0.7568447146816653, RMSE (Test): 0.7734689712524414\n",
      "Epoch [4361/5000], Train Loss: 0.00535699234266455, R2 Score (Train): 0.9931833519771696, RMSE (Train): 0.13867951865326517\n",
      "Test Loss: 0.6197204291820526, R2 Score (Test): 0.7540002050074518, RMSE (Test): 0.7779799699783325\n",
      "Epoch [4366/5000], Train Loss: 0.005024047762465973, R2 Score (Train): 0.9939349801792121, RMSE (Train): 0.1308106196523156\n",
      "Test Loss: 0.5763745307922363, R2 Score (Test): 0.7584719193570435, RMSE (Test): 0.7708765864372253\n",
      "Epoch [4371/5000], Train Loss: 0.005507364597481986, R2 Score (Train): 0.9965360234194914, RMSE (Train): 0.09885857915003991\n",
      "Test Loss: 0.5579662173986435, R2 Score (Test): 0.7566818670126837, RMSE (Test): 0.7737279534339905\n",
      "Epoch [4376/5000], Train Loss: 0.00843293780538564, R2 Score (Train): 0.9943708147245053, RMSE (Train): 0.12602295598376437\n",
      "Test Loss: 0.6364072561264038, R2 Score (Test): 0.7517776203287603, RMSE (Test): 0.7814865708351135\n",
      "Epoch [4381/5000], Train Loss: 0.009738259871179858, R2 Score (Train): 0.9940191069304466, RMSE (Train): 0.12990022688365774\n",
      "Test Loss: 0.5889123678207397, R2 Score (Test): 0.7548735917824413, RMSE (Test): 0.7765976786613464\n",
      "Epoch [4386/5000], Train Loss: 0.005622002587188035, R2 Score (Train): 0.9961831814505326, RMSE (Train): 0.10377139327385844\n",
      "Test Loss: 0.5463494658470154, R2 Score (Test): 0.7555339325725765, RMSE (Test): 0.7755509614944458\n",
      "Epoch [4391/5000], Train Loss: 0.007073863836315771, R2 Score (Train): 0.991328131837559, RMSE (Train): 0.15641673778327206\n",
      "Test Loss: 0.583678811788559, R2 Score (Test): 0.7597490631093311, RMSE (Test): 0.7688357830047607\n",
      "Epoch [4396/5000], Train Loss: 0.0047280289193925755, R2 Score (Train): 0.9943851493782974, RMSE (Train): 0.12586239571821284\n",
      "Test Loss: 0.6117878258228302, R2 Score (Test): 0.7600772387470478, RMSE (Test): 0.7683104872703552\n",
      "Epoch [4401/5000], Train Loss: 0.006948050674206267, R2 Score (Train): 0.994569599652488, RMSE (Train): 0.12377781582446033\n",
      "Test Loss: 0.5809222757816315, R2 Score (Test): 0.7588032906893051, RMSE (Test): 0.7703476548194885\n",
      "Epoch [4406/5000], Train Loss: 0.006337345597178985, R2 Score (Train): 0.9951212966556897, RMSE (Train): 0.11732190231317358\n",
      "Test Loss: 0.6476299464702606, R2 Score (Test): 0.7519197016945722, RMSE (Test): 0.7812628746032715\n",
      "Epoch [4411/5000], Train Loss: 0.0038509260242184005, R2 Score (Train): 0.9948941350006827, RMSE (Train): 0.12002219210962178\n",
      "Test Loss: 0.6068022549152374, R2 Score (Test): 0.7520244384397515, RMSE (Test): 0.781097948551178\n",
      "Epoch [4416/5000], Train Loss: 0.005479432681264977, R2 Score (Train): 0.9936962605481199, RMSE (Train): 0.13336013237465724\n",
      "Test Loss: 0.6293883323669434, R2 Score (Test): 0.7545189772212261, RMSE (Test): 0.7771592140197754\n",
      "Epoch [4421/5000], Train Loss: 0.008856028725858778, R2 Score (Train): 0.9930992059436831, RMSE (Train): 0.13953283686961085\n",
      "Test Loss: 0.5599451661109924, R2 Score (Test): 0.7639741731460703, RMSE (Test): 0.7620453238487244\n",
      "Epoch [4426/5000], Train Loss: 0.00835831913476189, R2 Score (Train): 0.993745673362115, RMSE (Train): 0.13283642228826584\n",
      "Test Loss: 0.5967901051044464, R2 Score (Test): 0.7582908845557133, RMSE (Test): 0.7711654305458069\n",
      "Epoch [4431/5000], Train Loss: 0.007958882042051604, R2 Score (Train): 0.9938203318639762, RMSE (Train): 0.13204120139738096\n",
      "Test Loss: 0.6283166408538818, R2 Score (Test): 0.7577015057999928, RMSE (Test): 0.7721050381660461\n",
      "Epoch [4436/5000], Train Loss: 0.006958658302513261, R2 Score (Train): 0.9954628986657518, RMSE (Train): 0.11313998849866791\n",
      "Test Loss: 0.6121624410152435, R2 Score (Test): 0.7528963759250468, RMSE (Test): 0.7797234654426575\n",
      "Epoch [4441/5000], Train Loss: 0.004442284620987873, R2 Score (Train): 0.9953657075880653, RMSE (Train): 0.11434537609396338\n",
      "Test Loss: 0.5692586004734039, R2 Score (Test): 0.759977375064393, RMSE (Test): 0.7684704065322876\n",
      "Epoch [4446/5000], Train Loss: 0.005042517751765748, R2 Score (Train): 0.9963939345298631, RMSE (Train): 0.10086574354629645\n",
      "Test Loss: 0.6138292253017426, R2 Score (Test): 0.754910504019322, RMSE (Test): 0.7765392661094666\n",
      "Epoch [4451/5000], Train Loss: 0.008247869399686655, R2 Score (Train): 0.9938192106883047, RMSE (Train): 0.1320531789554974\n",
      "Test Loss: 0.623731255531311, R2 Score (Test): 0.7567296976131651, RMSE (Test): 0.7736518979072571\n",
      "Epoch [4456/5000], Train Loss: 0.005549300364994754, R2 Score (Train): 0.9945780419138722, RMSE (Train): 0.12368156406978971\n",
      "Test Loss: 0.5994686186313629, R2 Score (Test): 0.7604882283588467, RMSE (Test): 0.767652153968811\n",
      "Epoch [4461/5000], Train Loss: 0.006015932420268655, R2 Score (Train): 0.9951965937263065, RMSE (Train): 0.11641301872964968\n",
      "Test Loss: 0.5920736491680145, R2 Score (Test): 0.7549477505162419, RMSE (Test): 0.7764802575111389\n",
      "Epoch [4466/5000], Train Loss: 0.006019680372749765, R2 Score (Train): 0.9955848399991852, RMSE (Train): 0.11160923052689337\n",
      "Test Loss: 0.6051528453826904, R2 Score (Test): 0.7565470666938692, RMSE (Test): 0.7739422917366028\n",
      "Epoch [4471/5000], Train Loss: 0.004616243609537681, R2 Score (Train): 0.9958757038510944, RMSE (Train): 0.10787028148135153\n",
      "Test Loss: 0.6273615062236786, R2 Score (Test): 0.7596746943510162, RMSE (Test): 0.7689547538757324\n",
      "Epoch [4476/5000], Train Loss: 0.004167137580225244, R2 Score (Train): 0.995559543329784, RMSE (Train): 0.11192850652871161\n",
      "Test Loss: 0.5258671790361404, R2 Score (Test): 0.7613739921409921, RMSE (Test): 0.7662313580513\n",
      "Epoch [4481/5000], Train Loss: 0.004511097678914666, R2 Score (Train): 0.9946234746918093, RMSE (Train): 0.12316228506532836\n",
      "Test Loss: 0.6005521416664124, R2 Score (Test): 0.7544766611733467, RMSE (Test): 0.7772262096405029\n",
      "Epoch [4486/5000], Train Loss: 0.006686323438771069, R2 Score (Train): 0.9925291752510684, RMSE (Train): 0.14518146186121092\n",
      "Test Loss: 0.5690003931522369, R2 Score (Test): 0.7531332588019768, RMSE (Test): 0.7793496251106262\n",
      "Epoch [4491/5000], Train Loss: 0.005802069266792387, R2 Score (Train): 0.9949856519353381, RMSE (Train): 0.11894169659239115\n",
      "Test Loss: 0.5919618606567383, R2 Score (Test): 0.7621201829141175, RMSE (Test): 0.7650324106216431\n",
      "Epoch [4496/5000], Train Loss: 0.00819266983307898, R2 Score (Train): 0.9948107608983963, RMSE (Train): 0.12099815038939754\n",
      "Test Loss: 0.6408668756484985, R2 Score (Test): 0.7637818061386589, RMSE (Test): 0.7623558044433594\n",
      "Epoch [4501/5000], Train Loss: 0.005592827219516039, R2 Score (Train): 0.9932884032125057, RMSE (Train): 0.13760677586556536\n",
      "Test Loss: 0.6092014014720917, R2 Score (Test): 0.7597189357171031, RMSE (Test): 0.768884003162384\n",
      "Epoch [4506/5000], Train Loss: 0.0066354212661584215, R2 Score (Train): 0.9940171846596241, RMSE (Train): 0.12992110030109752\n",
      "Test Loss: 0.5808626413345337, R2 Score (Test): 0.7580190953686687, RMSE (Test): 0.7715988755226135\n",
      "Epoch [4511/5000], Train Loss: 0.006223253149073571, R2 Score (Train): 0.9942925217328429, RMSE (Train): 0.1268963188882492\n",
      "Test Loss: 0.600196123123169, R2 Score (Test): 0.7588192351953149, RMSE (Test): 0.7703221440315247\n",
      "Epoch [4516/5000], Train Loss: 0.006170230393763632, R2 Score (Train): 0.9951343607899101, RMSE (Train): 0.11716471541183582\n",
      "Test Loss: 0.6032209396362305, R2 Score (Test): 0.7594624636813028, RMSE (Test): 0.769294261932373\n",
      "Epoch [4521/5000], Train Loss: 0.007778867380693555, R2 Score (Train): 0.9954852647873798, RMSE (Train): 0.11286077619295613\n",
      "Test Loss: 0.6064316928386688, R2 Score (Test): 0.7528831428228717, RMSE (Test): 0.7797443866729736\n",
      "Epoch [4526/5000], Train Loss: 0.0062180252668137355, R2 Score (Train): 0.9956277094383279, RMSE (Train): 0.11106606832625812\n",
      "Test Loss: 0.6101124286651611, R2 Score (Test): 0.7589884899291501, RMSE (Test): 0.7700518369674683\n",
      "Epoch [4531/5000], Train Loss: 0.005124390784961482, R2 Score (Train): 0.9942363644387592, RMSE (Train): 0.12751907295543458\n",
      "Test Loss: 0.5557865500450134, R2 Score (Test): 0.7582702855940985, RMSE (Test): 0.7711983323097229\n",
      "Epoch [4536/5000], Train Loss: 0.005688753871557613, R2 Score (Train): 0.9943738501706555, RMSE (Train): 0.12598897349244342\n",
      "Test Loss: 0.6428289711475372, R2 Score (Test): 0.7552217103115569, RMSE (Test): 0.7760460376739502\n",
      "Epoch [4541/5000], Train Loss: 0.006023320214202006, R2 Score (Train): 0.9944007923179466, RMSE (Train): 0.12568694749945075\n",
      "Test Loss: 0.6580672860145569, R2 Score (Test): 0.7557681812462629, RMSE (Test): 0.7751793265342712\n",
      "Epoch [4546/5000], Train Loss: 0.004596551298163831, R2 Score (Train): 0.9939086868377164, RMSE (Train): 0.1310938609888455\n",
      "Test Loss: 0.5785650610923767, R2 Score (Test): 0.7560670488801503, RMSE (Test): 0.7747048735618591\n",
      "Epoch [4551/5000], Train Loss: 0.007436603948008269, R2 Score (Train): 0.9939122321639158, RMSE (Train): 0.13105570516473705\n",
      "Test Loss: 0.5318769812583923, R2 Score (Test): 0.75949038774648, RMSE (Test): 0.7692496180534363\n",
      "Epoch [4556/5000], Train Loss: 0.005632035298428188, R2 Score (Train): 0.995329529267998, RMSE (Train): 0.11479083582336838\n",
      "Test Loss: 0.5376118570566177, R2 Score (Test): 0.7583703516755137, RMSE (Test): 0.7710387110710144\n",
      "Epoch [4561/5000], Train Loss: 0.007128050065754603, R2 Score (Train): 0.9942991770137439, RMSE (Train): 0.12682231273827727\n",
      "Test Loss: 0.5492416769266129, R2 Score (Test): 0.7637704775989438, RMSE (Test): 0.7623741030693054\n",
      "Epoch [4566/5000], Train Loss: 0.005587632108169298, R2 Score (Train): 0.9958690795999818, RMSE (Train): 0.10795687481914677\n",
      "Test Loss: 0.5941578447818756, R2 Score (Test): 0.7593490365533178, RMSE (Test): 0.7694755792617798\n",
      "Epoch [4571/5000], Train Loss: 0.005666073644533753, R2 Score (Train): 0.9948111436045358, RMSE (Train): 0.1209936885031821\n",
      "Test Loss: 0.6344364285469055, R2 Score (Test): 0.7546559424036963, RMSE (Test): 0.7769423723220825\n",
      "Epoch [4576/5000], Train Loss: 0.007111227683102091, R2 Score (Train): 0.9924700270163512, RMSE (Train): 0.1457550462820102\n",
      "Test Loss: 0.5573538690805435, R2 Score (Test): 0.758495141510438, RMSE (Test): 0.770839512348175\n",
      "Epoch [4581/5000], Train Loss: 0.005951809502827625, R2 Score (Train): 0.9928003014235999, RMSE (Train): 0.14252270222546623\n",
      "Test Loss: 0.5966764986515045, R2 Score (Test): 0.75702615677476, RMSE (Test): 0.773180365562439\n",
      "Epoch [4586/5000], Train Loss: 0.005361132013301055, R2 Score (Train): 0.9948530865231154, RMSE (Train): 0.1205036840593594\n",
      "Test Loss: 0.5795498788356781, R2 Score (Test): 0.7528878450103856, RMSE (Test): 0.7797369360923767\n",
      "Epoch [4591/5000], Train Loss: 0.005989810878721376, R2 Score (Train): 0.9922149935306206, RMSE (Train): 0.14820279000801218\n",
      "Test Loss: 0.5599429458379745, R2 Score (Test): 0.7576412393288345, RMSE (Test): 0.7722011208534241\n",
      "Epoch [4596/5000], Train Loss: 0.007583556541552146, R2 Score (Train): 0.9917333050620281, RMSE (Train): 0.15271892077633414\n",
      "Test Loss: 0.5905522406101227, R2 Score (Test): 0.7558957725611328, RMSE (Test): 0.7749767899513245\n",
      "Epoch [4601/5000], Train Loss: 0.007631701145631571, R2 Score (Train): 0.9929365606048052, RMSE (Train): 0.14116759056998576\n",
      "Test Loss: 0.6126215904951096, R2 Score (Test): 0.7691280804359816, RMSE (Test): 0.7536793351173401\n",
      "Epoch [4606/5000], Train Loss: 0.006291072291787714, R2 Score (Train): 0.9946309011397472, RMSE (Train): 0.12307719531758565\n",
      "Test Loss: 0.599733978509903, R2 Score (Test): 0.7601642836545304, RMSE (Test): 0.7681711912155151\n",
      "Epoch [4611/5000], Train Loss: 0.005345939833205193, R2 Score (Train): 0.9921552710130145, RMSE (Train): 0.14877017125694583\n",
      "Test Loss: 0.5487135797739029, R2 Score (Test): 0.7618925251201336, RMSE (Test): 0.7653984427452087\n",
      "Epoch [4616/5000], Train Loss: 0.004900441912468523, R2 Score (Train): 0.9930445333049116, RMSE (Train): 0.14008448195054243\n",
      "Test Loss: 0.5655135810375214, R2 Score (Test): 0.7600123397801805, RMSE (Test): 0.7684143781661987\n",
      "Epoch [4621/5000], Train Loss: 0.007473992319622387, R2 Score (Train): 0.9950252037051548, RMSE (Train): 0.11847167855926347\n",
      "Test Loss: 0.556599572300911, R2 Score (Test): 0.754274411444241, RMSE (Test): 0.7775462865829468\n",
      "Epoch [4626/5000], Train Loss: 0.005191995839898785, R2 Score (Train): 0.9928984334272573, RMSE (Train): 0.14154807646532333\n",
      "Test Loss: 0.644593358039856, R2 Score (Test): 0.7547521025223416, RMSE (Test): 0.7767901420593262\n",
      "Epoch [4631/5000], Train Loss: 0.0059227424014049275, R2 Score (Train): 0.9938165248598338, RMSE (Train): 0.13208186733557006\n",
      "Test Loss: 0.6186241805553436, R2 Score (Test): 0.7498272821288283, RMSE (Test): 0.784550666809082\n",
      "Epoch [4636/5000], Train Loss: 0.005122445329713325, R2 Score (Train): 0.9935283937795844, RMSE (Train): 0.13512413678539176\n",
      "Test Loss: 0.6485059410333633, R2 Score (Test): 0.7634770287579351, RMSE (Test): 0.7628474235534668\n",
      "Epoch [4641/5000], Train Loss: 0.006132033925193052, R2 Score (Train): 0.9944240047611606, RMSE (Train): 0.12542614888940665\n",
      "Test Loss: 0.6008915901184082, R2 Score (Test): 0.7511503617332994, RMSE (Test): 0.7824733853340149\n",
      "Epoch [4646/5000], Train Loss: 0.01481948895767952, R2 Score (Train): 0.9900112372075723, RMSE (Train): 0.16787373017401827\n",
      "Test Loss: 0.5752647221088409, R2 Score (Test): 0.7583591532398956, RMSE (Test): 0.7710565328598022\n",
      "Epoch [4651/5000], Train Loss: 0.006308768332625429, R2 Score (Train): 0.9945353247355045, RMSE (Train): 0.12416782400571343\n",
      "Test Loss: 0.6090240776538849, R2 Score (Test): 0.744708395351289, RMSE (Test): 0.7925366163253784\n",
      "Epoch [4656/5000], Train Loss: 0.005421894020400941, R2 Score (Train): 0.9947376195698571, RMSE (Train): 0.12184788948508458\n",
      "Test Loss: 0.6625096201896667, R2 Score (Test): 0.7436929938100905, RMSE (Test): 0.7941111326217651\n",
      "Epoch [4661/5000], Train Loss: 0.006655385504321505, R2 Score (Train): 0.9945675695020164, RMSE (Train): 0.12380095077709408\n",
      "Test Loss: 0.6738850474357605, R2 Score (Test): 0.7486607436338244, RMSE (Test): 0.7863777279853821\n",
      "Epoch [4666/5000], Train Loss: 0.007088992647671451, R2 Score (Train): 0.9919193024156647, RMSE (Train): 0.15099108906435968\n",
      "Test Loss: 0.5712669342756271, R2 Score (Test): 0.7478656025162698, RMSE (Test): 0.7876207232475281\n",
      "Epoch [4671/5000], Train Loss: 0.005439107422716916, R2 Score (Train): 0.9946625724708916, RMSE (Train): 0.12271365348751502\n",
      "Test Loss: 0.5760383605957031, R2 Score (Test): 0.7556687855588637, RMSE (Test): 0.7753369808197021\n",
      "Epoch [4676/5000], Train Loss: 0.006018336804118007, R2 Score (Train): 0.9961990510801593, RMSE (Train): 0.10355543737883088\n",
      "Test Loss: 0.6128190755844116, R2 Score (Test): 0.7511847987161673, RMSE (Test): 0.7824192047119141\n",
      "Epoch [4681/5000], Train Loss: 0.005482569026450316, R2 Score (Train): 0.9945214283846638, RMSE (Train): 0.12432559953728885\n",
      "Test Loss: 0.621137410402298, R2 Score (Test): 0.7564866620369297, RMSE (Test): 0.7740382552146912\n",
      "Epoch [4686/5000], Train Loss: 0.005881798104383051, R2 Score (Train): 0.9921987540713908, RMSE (Train): 0.14835728437975967\n",
      "Test Loss: 0.6536530554294586, R2 Score (Test): 0.7550515461204336, RMSE (Test): 0.7763157486915588\n",
      "Epoch [4691/5000], Train Loss: 0.008239415377223244, R2 Score (Train): 0.9942916543999365, RMSE (Train): 0.1269059603767395\n",
      "Test Loss: 0.5594738721847534, R2 Score (Test): 0.7585351222721606, RMSE (Test): 0.7707757353782654\n",
      "Epoch [4696/5000], Train Loss: 0.0070788396600012975, R2 Score (Train): 0.9895475630955377, RMSE (Train): 0.17172584734753388\n",
      "Test Loss: 0.5436591804027557, R2 Score (Test): 0.7557237916856661, RMSE (Test): 0.775249719619751\n",
      "Epoch [4701/5000], Train Loss: 0.006690950831398368, R2 Score (Train): 0.9949824472776201, RMSE (Train): 0.1189796981969582\n",
      "Test Loss: 0.5643815845251083, R2 Score (Test): 0.7519505152555562, RMSE (Test): 0.7812143564224243\n",
      "Epoch [4706/5000], Train Loss: 0.0075077433527136845, R2 Score (Train): 0.9950018698069727, RMSE (Train): 0.1187491946566958\n",
      "Test Loss: 0.6905821412801743, R2 Score (Test): 0.748637842972016, RMSE (Test): 0.7864136099815369\n",
      "Epoch [4711/5000], Train Loss: 0.005717822796820353, R2 Score (Train): 0.9933485762730327, RMSE (Train): 0.1369885277842774\n",
      "Test Loss: 0.658601313829422, R2 Score (Test): 0.7528330945000031, RMSE (Test): 0.7798233032226562\n",
      "Epoch [4716/5000], Train Loss: 0.006737873288026701, R2 Score (Train): 0.9963216258604389, RMSE (Train): 0.10187200199216291\n",
      "Test Loss: 0.5876874923706055, R2 Score (Test): 0.7492999086159841, RMSE (Test): 0.7853772044181824\n",
      "Epoch [4721/5000], Train Loss: 0.007850578908498088, R2 Score (Train): 0.9938311745203037, RMSE (Train): 0.1319253128187284\n",
      "Test Loss: 0.62514528632164, R2 Score (Test): 0.7512876283889582, RMSE (Test): 0.7822575569152832\n",
      "Epoch [4726/5000], Train Loss: 0.005790053905608754, R2 Score (Train): 0.9951290426788366, RMSE (Train): 0.11722872804993811\n",
      "Test Loss: 0.6221844255924225, R2 Score (Test): 0.7487628520294698, RMSE (Test): 0.786217987537384\n",
      "Epoch [4731/5000], Train Loss: 0.00439431097280855, R2 Score (Train): 0.9958549157641198, RMSE (Train): 0.10814179426124323\n",
      "Test Loss: 0.553090363740921, R2 Score (Test): 0.7504754142190304, RMSE (Test): 0.7835337519645691\n",
      "Epoch [4736/5000], Train Loss: 0.004342117928899825, R2 Score (Train): 0.9955166839332394, RMSE (Train): 0.11246737769515124\n",
      "Test Loss: 0.5526523739099503, R2 Score (Test): 0.7523585594160315, RMSE (Test): 0.7805715203285217\n",
      "Epoch [4741/5000], Train Loss: 0.005478997792427738, R2 Score (Train): 0.9946453141556642, RMSE (Train): 0.12291188772012737\n",
      "Test Loss: 0.6028597950935364, R2 Score (Test): 0.7495633912163742, RMSE (Test): 0.784964382648468\n",
      "Epoch [4746/5000], Train Loss: 0.006825235109621038, R2 Score (Train): 0.9948820790951615, RMSE (Train): 0.12016380602519008\n",
      "Test Loss: 0.5705717206001282, R2 Score (Test): 0.7456952154464707, RMSE (Test): 0.791003406047821\n",
      "Epoch [4751/5000], Train Loss: 0.005229381301129858, R2 Score (Train): 0.9954557679986688, RMSE (Train): 0.11322886097436642\n",
      "Test Loss: 0.6396938860416412, R2 Score (Test): 0.7517618985179775, RMSE (Test): 0.7815113067626953\n",
      "Epoch [4756/5000], Train Loss: 0.005491122913857301, R2 Score (Train): 0.9954311098431279, RMSE (Train): 0.1135356496199455\n",
      "Test Loss: 0.5837801694869995, R2 Score (Test): 0.7504379824458515, RMSE (Test): 0.7835925221443176\n",
      "Epoch [4761/5000], Train Loss: 0.0057388773032774525, R2 Score (Train): 0.9914139215596943, RMSE (Train): 0.15564110896807418\n",
      "Test Loss: 0.6036840975284576, R2 Score (Test): 0.7538342855218305, RMSE (Test): 0.778242290019989\n",
      "Epoch [4766/5000], Train Loss: 0.009633623762056231, R2 Score (Train): 0.99193868487676, RMSE (Train): 0.15080989579986065\n",
      "Test Loss: 0.5460052937269211, R2 Score (Test): 0.7512647917556421, RMSE (Test): 0.782293438911438\n",
      "Epoch [4771/5000], Train Loss: 0.00721923434563602, R2 Score (Train): 0.9963739725533536, RMSE (Train): 0.10114453781915526\n",
      "Test Loss: 0.7233476638793945, R2 Score (Test): 0.7446539373825498, RMSE (Test): 0.7926210761070251\n",
      "Epoch [4776/5000], Train Loss: 0.005123096130167444, R2 Score (Train): 0.9935582324243122, RMSE (Train): 0.13481226835462592\n",
      "Test Loss: 0.5691571682691574, R2 Score (Test): 0.7515560679804902, RMSE (Test): 0.7818352580070496\n",
      "Epoch [4781/5000], Train Loss: 0.005303197637355576, R2 Score (Train): 0.9933827759072044, RMSE (Train): 0.13663589687218972\n",
      "Test Loss: 0.6951138377189636, R2 Score (Test): 0.7453867326877937, RMSE (Test): 0.7914829254150391\n",
      "Epoch [4786/5000], Train Loss: 0.004843057598918676, R2 Score (Train): 0.9931045361565763, RMSE (Train): 0.13947893847815412\n",
      "Test Loss: 0.7067166417837143, R2 Score (Test): 0.7452385508513708, RMSE (Test): 0.7917132377624512\n",
      "Epoch [4791/5000], Train Loss: 0.006411120586562902, R2 Score (Train): 0.9957359613822152, RMSE (Train): 0.10968252892236216\n",
      "Test Loss: 0.6346023976802826, R2 Score (Test): 0.7433046781508686, RMSE (Test): 0.794712483882904\n",
      "Epoch [4796/5000], Train Loss: 0.0053845795143085224, R2 Score (Train): 0.9921292258591407, RMSE (Train): 0.1490169312928824\n",
      "Test Loss: 0.6671234965324402, R2 Score (Test): 0.7490757564276589, RMSE (Test): 0.7857282161712646\n",
      "Epoch [4801/5000], Train Loss: 0.007864781538955867, R2 Score (Train): 0.9949016094015325, RMSE (Train): 0.11993431057279932\n",
      "Test Loss: 0.6798824369907379, R2 Score (Test): 0.7433189722908772, RMSE (Test): 0.7946903109550476\n",
      "Epoch [4806/5000], Train Loss: 0.0058931986180444556, R2 Score (Train): 0.9945205400671151, RMSE (Train): 0.12433567845412387\n",
      "Test Loss: 0.5767779350280762, R2 Score (Test): 0.7522245735848668, RMSE (Test): 0.7807826995849609\n",
      "Epoch [4811/5000], Train Loss: 0.0074290365834409995, R2 Score (Train): 0.9923889155351621, RMSE (Train): 0.14653796680825731\n",
      "Test Loss: 0.6384611129760742, R2 Score (Test): 0.744612905002026, RMSE (Test): 0.79268479347229\n",
      "Epoch [4816/5000], Train Loss: 0.005336120356029521, R2 Score (Train): 0.9945246054400683, RMSE (Train): 0.12428954574202765\n",
      "Test Loss: 0.6437541842460632, R2 Score (Test): 0.7525287577995422, RMSE (Test): 0.7803032398223877\n",
      "Epoch [4821/5000], Train Loss: 0.0054912139506389695, R2 Score (Train): 0.9953755719141024, RMSE (Train): 0.1142236163052669\n",
      "Test Loss: 0.6445626616477966, R2 Score (Test): 0.7517346056462842, RMSE (Test): 0.7815542817115784\n",
      "Epoch [4826/5000], Train Loss: 0.005415703441637258, R2 Score (Train): 0.9966354175428054, RMSE (Train): 0.09742994979862922\n",
      "Test Loss: 0.6745289266109467, R2 Score (Test): 0.7441438335109665, RMSE (Test): 0.793412446975708\n",
      "Epoch [4831/5000], Train Loss: 0.008030973239025721, R2 Score (Train): 0.9957732129203446, RMSE (Train): 0.10920237311646494\n",
      "Test Loss: 0.6389902234077454, R2 Score (Test): 0.7418784286166722, RMSE (Test): 0.796917200088501\n",
      "Epoch [4836/5000], Train Loss: 0.00701967952772975, R2 Score (Train): 0.9939054589504368, RMSE (Train): 0.13112859078770261\n",
      "Test Loss: 0.5909970998764038, R2 Score (Test): 0.7538865384908338, RMSE (Test): 0.7781596779823303\n",
      "Epoch [4841/5000], Train Loss: 0.006263701807862769, R2 Score (Train): 0.9953856119457181, RMSE (Train): 0.11409955427755351\n",
      "Test Loss: 0.5988849997520447, R2 Score (Test): 0.7503194043142035, RMSE (Test): 0.7837786674499512\n",
      "Epoch [4846/5000], Train Loss: 0.004654148108481119, R2 Score (Train): 0.9948122768339914, RMSE (Train): 0.12098047546658831\n",
      "Test Loss: 0.6554102897644043, R2 Score (Test): 0.7503734441050883, RMSE (Test): 0.7836938500404358\n",
      "Epoch [4851/5000], Train Loss: 0.006127346190623939, R2 Score (Train): 0.994610281007275, RMSE (Train): 0.1233133090605272\n",
      "Test Loss: 0.5924078822135925, R2 Score (Test): 0.7547040235869129, RMSE (Test): 0.7768661975860596\n",
      "Epoch [4856/5000], Train Loss: 0.0058330987036849065, R2 Score (Train): 0.9948820294490253, RMSE (Train): 0.12016438884527801\n",
      "Test Loss: 0.605642557144165, R2 Score (Test): 0.7503498624913473, RMSE (Test): 0.7837308645248413\n",
      "Epoch [4861/5000], Train Loss: 0.004808798820401232, R2 Score (Train): 0.9964658102789491, RMSE (Train): 0.0998554608584059\n",
      "Test Loss: 0.6040453910827637, R2 Score (Test): 0.7495061605393278, RMSE (Test): 0.785054087638855\n",
      "Epoch [4866/5000], Train Loss: 0.0074748672389735775, R2 Score (Train): 0.9945661933775366, RMSE (Train): 0.123816630198241\n",
      "Test Loss: 0.6541501879692078, R2 Score (Test): 0.7546352850213741, RMSE (Test): 0.7769750952720642\n",
      "Epoch [4871/5000], Train Loss: 0.0036770342849195004, R2 Score (Train): 0.9961920029207941, RMSE (Train): 0.1036514051489498\n",
      "Test Loss: 0.630185604095459, R2 Score (Test): 0.748553617720171, RMSE (Test): 0.7865452170372009\n",
      "Epoch [4876/5000], Train Loss: 0.006336589460261166, R2 Score (Train): 0.996072327253813, RMSE (Train): 0.10526755557552293\n",
      "Test Loss: 0.607518881559372, R2 Score (Test): 0.7502383478769147, RMSE (Test): 0.7839059233665466\n",
      "Epoch [4881/5000], Train Loss: 0.007553597815179576, R2 Score (Train): 0.9932670178780506, RMSE (Train): 0.1378258315233976\n",
      "Test Loss: 0.5672587305307388, R2 Score (Test): 0.7565476692404564, RMSE (Test): 0.7739413380622864\n",
      "Epoch [4886/5000], Train Loss: 0.008580276315721372, R2 Score (Train): 0.9946596157855435, RMSE (Train): 0.12274763759559891\n",
      "Test Loss: 0.607838362455368, R2 Score (Test): 0.7566612666136756, RMSE (Test): 0.7737606763839722\n",
      "Epoch [4891/5000], Train Loss: 0.006077300330313544, R2 Score (Train): 0.9928366898490617, RMSE (Train): 0.14216207974766776\n",
      "Test Loss: 0.645342618227005, R2 Score (Test): 0.7579927736439539, RMSE (Test): 0.7716408371925354\n",
      "Epoch [4896/5000], Train Loss: 0.006556035038859894, R2 Score (Train): 0.9928522846594967, RMSE (Train): 0.1420072491894234\n",
      "Test Loss: 0.5789553225040436, R2 Score (Test): 0.758218925026908, RMSE (Test): 0.7712801694869995\n",
      "Epoch [4901/5000], Train Loss: 0.006970493064727634, R2 Score (Train): 0.9941273315402243, RMSE (Train): 0.1287195854573115\n",
      "Test Loss: 0.5748800039291382, R2 Score (Test): 0.7604952352549887, RMSE (Test): 0.7676409482955933\n",
      "Epoch [4906/5000], Train Loss: 0.006972307669154058, R2 Score (Train): 0.9934905226783846, RMSE (Train): 0.13551892557863326\n",
      "Test Loss: 0.6064310967922211, R2 Score (Test): 0.7586181996511389, RMSE (Test): 0.7706431150436401\n",
      "Epoch [4911/5000], Train Loss: 0.006842482408198218, R2 Score (Train): 0.994567770858109, RMSE (Train): 0.12379865637988964\n",
      "Test Loss: 0.6032284498214722, R2 Score (Test): 0.7565070636870298, RMSE (Test): 0.7740058302879333\n",
      "Epoch [4916/5000], Train Loss: 0.004293839185265824, R2 Score (Train): 0.9933555501730102, RMSE (Train): 0.13691669392758604\n",
      "Test Loss: 0.6072892844676971, R2 Score (Test): 0.7633952779751374, RMSE (Test): 0.76297926902771\n",
      "Epoch [4921/5000], Train Loss: 0.007111496292054653, R2 Score (Train): 0.9937261044109896, RMSE (Train): 0.13304407368166318\n",
      "Test Loss: 0.6114901900291443, R2 Score (Test): 0.7531799398339348, RMSE (Test): 0.7792760133743286\n",
      "Epoch [4926/5000], Train Loss: 0.007141275835844378, R2 Score (Train): 0.9944955056834127, RMSE (Train): 0.12461938523864956\n",
      "Test Loss: 0.6065472364425659, R2 Score (Test): 0.753256558035005, RMSE (Test): 0.7791549563407898\n",
      "Epoch [4931/5000], Train Loss: 0.005230503874675681, R2 Score (Train): 0.9947916496485842, RMSE (Train): 0.12122075534737416\n",
      "Test Loss: 0.5503963679075241, R2 Score (Test): 0.7578770339446975, RMSE (Test): 0.77182537317276\n",
      "Epoch [4936/5000], Train Loss: 0.007936722715385258, R2 Score (Train): 0.9936770409773972, RMSE (Train): 0.13356327955422334\n",
      "Test Loss: 0.6007639765739441, R2 Score (Test): 0.756534857915664, RMSE (Test): 0.77396160364151\n",
      "Epoch [4941/5000], Train Loss: 0.007891540842441222, R2 Score (Train): 0.993054460919942, RMSE (Train): 0.13998447418211402\n",
      "Test Loss: 0.5699616372585297, R2 Score (Test): 0.7663955468606066, RMSE (Test): 0.7581263780593872\n",
      "Epoch [4946/5000], Train Loss: 0.006104535811270277, R2 Score (Train): 0.9943347423189342, RMSE (Train): 0.126426095289583\n",
      "Test Loss: 0.6292767226696014, R2 Score (Test): 0.7540889080490539, RMSE (Test): 0.777839720249176\n",
      "Epoch [4951/5000], Train Loss: 0.0061701501642043395, R2 Score (Train): 0.9941384077322062, RMSE (Train): 0.12859814188070456\n",
      "Test Loss: 0.7043774127960205, R2 Score (Test): 0.7510096957128901, RMSE (Test): 0.7826944589614868\n",
      "Epoch [4956/5000], Train Loss: 0.005985975246100376, R2 Score (Train): 0.9942113244392494, RMSE (Train): 0.12779577478174736\n",
      "Test Loss: 0.6238433420658112, R2 Score (Test): 0.7583939032941704, RMSE (Test): 0.7710011005401611\n",
      "Epoch [4961/5000], Train Loss: 0.00652615954944243, R2 Score (Train): 0.9913505315814178, RMSE (Train): 0.15621459216338615\n",
      "Test Loss: 0.5319553464651108, R2 Score (Test): 0.7625120864387793, RMSE (Test): 0.7644019722938538\n",
      "Epoch [4966/5000], Train Loss: 0.007859202237644544, R2 Score (Train): 0.993641175071622, RMSE (Train): 0.13394155134474334\n",
      "Test Loss: 0.540400505065918, R2 Score (Test): 0.7574167848189302, RMSE (Test): 0.7725585699081421\n",
      "Epoch [4971/5000], Train Loss: 0.004454614827409387, R2 Score (Train): 0.9949309608950189, RMSE (Train): 0.11958858065707206\n",
      "Test Loss: 0.5877266824245453, R2 Score (Test): 0.755384653962929, RMSE (Test): 0.7757877707481384\n",
      "Epoch [4976/5000], Train Loss: 0.005423118399145703, R2 Score (Train): 0.9953711757204391, RMSE (Train): 0.11427789651506375\n",
      "Test Loss: 0.5373021960258484, R2 Score (Test): 0.7585442201852473, RMSE (Test): 0.7707611918449402\n",
      "Epoch [4981/5000], Train Loss: 0.005115023348480463, R2 Score (Train): 0.9945259152276541, RMSE (Train): 0.12427467899233524\n",
      "Test Loss: 0.5468893051147461, R2 Score (Test): 0.7552973666010883, RMSE (Test): 0.7759261131286621\n",
      "Epoch [4986/5000], Train Loss: 0.006059059931430966, R2 Score (Train): 0.9950234024674511, RMSE (Train): 0.11849312429582223\n",
      "Test Loss: 0.6471157073974609, R2 Score (Test): 0.7526106053869092, RMSE (Test): 0.7801741361618042\n",
      "Epoch [4991/5000], Train Loss: 0.005378120307189723, R2 Score (Train): 0.9948819603614876, RMSE (Train): 0.12016519989270723\n",
      "Test Loss: 0.6258120536804199, R2 Score (Test): 0.7558340282229395, RMSE (Test): 0.7750747203826904\n",
      "Epoch [4996/5000], Train Loss: 0.005334473176238437, R2 Score (Train): 0.994714064437637, RMSE (Train): 0.12212028888358618\n",
      "Test Loss: 0.5925565659999847, R2 Score (Test): 0.7578208574386374, RMSE (Test): 0.7719148993492126\n",
      "=========================================\n",
      "average R2 on training dataset:0.9936040802440897, average R2 on testing dataset:0.7530753792860233\n",
      "average rmse on training dataset:0.13340274629099239, average rmse on testing dataset:0.7793716788291931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device', device)\n",
    "model = prediction_model(x_data.shape[1], 512, 256, 1)\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-5)\n",
    "\n",
    "model = model.to(device)\n",
    "loss = loss_func.to(device)\n",
    "epochs = 5000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 64)\n",
    "\n",
    "\n",
    "final_R2_test, final_R2_train, final_rmse_test, final_rmse_train = train(model, x_data, y_data, optimizer, loss, epochs, device)\n",
    "\n",
    "# for plot\n",
    "final_R2_train = final_R2_train[5:]\n",
    "final_R2_test = final_R2_test[5:]\n",
    "final_rmse_train = final_rmse_train[5:]\n",
    "final_rmse_test = final_rmse_test[5:]\n",
    "\n",
    "# for calculation\n",
    "print('=========================================')\n",
    "final_R2_train_sub = final_R2_train[200:]\n",
    "final_R2_test_sub = final_R2_test[200:]\n",
    "final_rmse_train_sub = final_rmse_train[200:]\n",
    "final_rmse_test_sub = final_rmse_test[200:]\n",
    "print(f'average R2 on training dataset:{np.mean(final_R2_train_sub)}, average R2 on testing dataset:{np.mean(final_R2_test_sub)}')\n",
    "print(f'average rmse on training dataset:{np.mean(final_rmse_train_sub)}, average rmse on testing dataset:{np.mean(final_rmse_test_sub)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(995)]\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, final_R2_train, label = 'r2_train')\n",
    "plt.plot(x, final_R2_test, label = 'r2_test')\n",
    "plt.legend()\n",
    "plt.title('R2 score')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('R2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, final_rmse_train, label = 'rmse_train')\n",
    "plt.plot(x, final_rmse_test, label = 'rmse_test')\n",
    "plt.legend()\n",
    "plt.title('RMSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'ANN_model_with_ALogP.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Drug_Discovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
